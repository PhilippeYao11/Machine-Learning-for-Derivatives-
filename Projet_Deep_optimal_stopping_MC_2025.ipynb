{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9c1902d5-dc2e-4a82-9b71-640a89bc76c1",
      "metadata": {
        "id": "9c1902d5-dc2e-4a82-9b71-640a89bc76c1"
      },
      "source": [
        "# Deep Optimal Stopping and Dual Pricing of Bermudan Options\n",
        "\n",
        "### Auteurs :\n",
        "- **Philippe Yao**\n",
        "- **Georges Rolland**\n",
        "\n",
        "### Encadrants :\n",
        "- **Vincent Lemaire**\n",
        "- **Gilles Pagès**\n",
        "\n",
        "### Formation :\n",
        "Master 2 Probabilités et Finance, année universitaire 2024–2025\n",
        "\n",
        "---\n",
        "\n",
        "### Présentation du projet :\n",
        "\n",
        "Ce projet porte sur la valorisation d’options Bermudéennes, un type d’option exerçable à dates discrètes, via des méthodes issues de l’apprentissage profond et de la théorie des martingales.\n",
        "\n",
        "L’approche s’inspire de deux articles clés :\n",
        "- **Becker, Cheridito, Jentzen (2019)** — *Deep Optimal Stopping*, qui propose une méthode d'entraînement de réseaux de neurones pour approximer la politique d’exercice optimale et construire une borne inférieure du prix.\n",
        "- **Rogers (2002)** — *Monte Carlo valuation of American options using a martingale representation*, qui introduit une formulation duale pour encadrer la valeur de l’option à l’aide de martingales surmajorantes.\n",
        "\n",
        "Nous implémentons ces deux stratégies pour construire des bornes inférieure et supérieure encadrant le prix de l’option. L’implémentation est enrichie par des techniques de réduction de variance (notamment la méthode de variable de contrôle), et l’ensemble est évalué par des simulations dans un cadre de Black-Scholes multidimensionnel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Librairies"
      ],
      "metadata": {
        "id": "C0II8KHEelMW"
      },
      "id": "C0II8KHEelMW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4ae2c82-cc37-4bc4-87a5-561fa717532e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4ae2c82-cc37-4bc4-87a5-561fa717532e",
        "outputId": "5652af11-b883-4bf2-8e47-23b7ab11092e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from scipy import stats\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.distributions as dist\n",
        "\n",
        "# Plotting style\n",
        "sns.set_theme()\n",
        "\n",
        "# Select device: GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c4f42d5-fd22-4b6d-b2e1-369acd118942",
      "metadata": {
        "id": "4c4f42d5-fd22-4b6d-b2e1-369acd118942"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "389d1d6c-d90e-47bb-a4b5-b590bae8b611",
      "metadata": {
        "id": "389d1d6c-d90e-47bb-a4b5-b590bae8b611"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "r = 0.05              # Risk-free rate\n",
        "sigma = 0.2           # Volatility\n",
        "dividend = 0.1        # Dividend Yield\n",
        "x0 = 100              # Initial asset price\n",
        "K = 100               # Strike price\n",
        "rho = 0.0               # Assets correlation (Brownians correlation)\n",
        "T = 3                 # Maturity (in years)\n",
        "N=9                # Number of exercise dates\n",
        "d = 5\n",
        "n_assets = 5                  # Asset numbers\n",
        "M = 8192\n",
        "dt = T / N            # Time step\n",
        "delta = 0.1\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 2. Matrice de corrélation\n",
        "rho_matrix = torch.eye(n_assets, device=device) * (1 - rho) + torch.ones((n_assets, n_assets), device=device) * rho\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "695ac814-1b9e-4a93-bbc6-afb5c54c2a6c",
      "metadata": {
        "id": "695ac814-1b9e-4a93-bbc6-afb5c54c2a6c"
      },
      "source": [
        "### Paths simulations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulation des trajectoires Black-Scholes\n",
        "\n",
        "Génère des trajectoires log-normales de $d$ actifs selon le modèle de Black-Scholes discrétisé, avec drift et diffusion constants.\n",
        "\n"
      ],
      "metadata": {
        "id": "O_erXxKfg9K2"
      },
      "id": "O_erXxKfg9K2"
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_black_scholes_paths(x0, r, sigma, delta, T, N, M, d=5, device=\"cpu\"):\n",
        "    dt = T / N\n",
        "    Z = torch.randn(N, M, d, device=device)\n",
        "\n",
        "    x0 = torch.full((M, d), x0, device=device) if not torch.is_tensor(x0) else x0\n",
        "    sigma = torch.full((d,), sigma, device=device) if not torch.is_tensor(sigma) else sigma\n",
        "    delta = torch.full((d,), delta, device=device) if not torch.is_tensor(delta) else delta\n",
        "\n",
        "    drift = (r - delta - 0.5 * sigma**2) * dt\n",
        "    diffusion = sigma * torch.sqrt(torch.tensor(dt, device=device))\n",
        "\n",
        "    increments = drift.unsqueeze(0).unsqueeze(0) + diffusion.unsqueeze(0).unsqueeze(0) * Z\n",
        "    log_paths = torch.log(x0) + torch.cumsum(increments, dim=0)\n",
        "\n",
        "    paths = torch.cat([\n",
        "        x0.unsqueeze(0),\n",
        "        torch.exp(log_paths)\n",
        "    ], dim=0)\n",
        "\n",
        "    return paths  # (N+1, M, d)\n"
      ],
      "metadata": {
        "id": "8IDEWAgbHVYU"
      },
      "id": "8IDEWAgbHVYU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Payoff de l'option max-call Bermudéenne\n",
        "\n",
        "Cette fonction calcule le payoff d’une option Bermudéenne de type max-call à une date donnée, selon la formule :\n",
        "\n",
        "$$\n",
        "\\text{Payoff}(S) = \\left( \\max_{1 \\leq i \\leq d} S_i - K \\right)^+\n",
        "$$\n"
      ],
      "metadata": {
        "id": "t9kn8P1tg7At"
      },
      "id": "t9kn8P1tg7At"
    },
    {
      "cell_type": "code",
      "source": [
        "def bermudan_max_call_payoff(S, K):\n",
        "    \"\"\"\n",
        "    Calcule le payoff d'une option Bermudéenne Max-Call.\n",
        "\n",
        "    S : tensor (M, d) — Prix des actifs à la date d'exercice (M trajectoires, d actifs)\n",
        "    K : float — Strike de l'option\n",
        "\n",
        "    Retourne :\n",
        "    payoff : tensor (M,) — Payoff pour chaque trajectoire\n",
        "    \"\"\"\n",
        "\n",
        "    # Prend le maximum des actifs pour chaque trajectoire (sur la dimension des actifs)\n",
        "    max_S = torch.max(S, dim=1).values  # shape (M,)\n",
        "\n",
        "    # Payoff = (max_S - K)+\n",
        "    payoff = torch.clamp(max_S - K, min=0.0)  # shape (M,)\n",
        "\n",
        "    return payoff\n"
      ],
      "metadata": {
        "id": "Gw_xFC7X2Zyr"
      },
      "id": "Gw_xFC7X2Zyr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_all_payoffs(S_paths, K):\n",
        "    \"\"\"\n",
        "    Calcule les payoffs à chaque pas de temps.\n",
        "    S_paths : (N+1, M, d)\n",
        "    Retourne : tensor (N+1, M)\n",
        "    \"\"\"\n",
        "    N_plus_1, M, d = S_paths.shape\n",
        "    payoffs = torch.zeros(N_plus_1, M, device=S_paths.device)\n",
        "\n",
        "    for n in range(N_plus_1):\n",
        "        payoffs[n] = bermudan_max_call_payoff(S_paths[n], K)\n",
        "\n",
        "    return payoffs\n"
      ],
      "metadata": {
        "id": "U2ETfRWbKnYz"
      },
      "id": "U2ETfRWbKnYz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cc3694b0-d0bd-4c87-9516-cc0bc0178caf",
      "metadata": {
        "id": "cc3694b0-d0bd-4c87-9516-cc0bc0178caf"
      },
      "source": [
        "### Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction utilitaire : normalisation d'un batch\n",
        "def normalize_batch(batch):\n",
        "    \"\"\"\n",
        "    Normalise un batch de données (par feature)\n",
        "    batch : tensor (batch_size, d)\n",
        "    \"\"\"\n",
        "    mean = batch.mean(dim=0, keepdim=True)\n",
        "    std = batch.std(dim=0, keepdim=True) + 1e-8  # pour éviter la division par 0\n",
        "    return (batch - mean) / std"
      ],
      "metadata": {
        "id": "arFFpUasEmSD"
      },
      "id": "arFFpUasEmSD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Réseau de neurones pour l'arrêt optimal\n",
        "\n",
        "Le réseau `StoppingNet` approxime la probabilité d'arrêt à chaque date. Il prend en entrée l’état du système $ S_n \\in \\mathbb{R}^d $ et renvoie une probabilité $ p_n \\in (0,1) $. Il est composé de 3 couches cachées et utilise une activation sigmoïde en sortie :\n",
        "\n",
        "$$\n",
        "f_\\theta(S_n) \\approx \\mathbb{P}(\\text{arrêt à } t_n \\mid S_n)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "0on4jfTAhRVI"
      },
      "id": "0on4jfTAhRVI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Réseau de neurones avec 3 couches cachées\n",
        "class StoppingNet(nn.Module):\n",
        "    def __init__(self, d, hidden_size=None):\n",
        "        \"\"\"\n",
        "        d : dimension d'entrée (dimension de l'état)\n",
        "        hidden_size : taille des couches cachées (par défaut 40 + d)\n",
        "        \"\"\"\n",
        "        super(StoppingNet, self).__init__()\n",
        "        if hidden_size is None:\n",
        "            hidden_size = 40 + d\n",
        "\n",
        "        # Réseau plus profond avec 3 couches cachées\n",
        "        self.fc1 = nn.Linear(d, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc4 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        # Initialisation Xavier\n",
        "        for layer in [self.fc1, self.fc2, self.fc3, self.fc4]:\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = torch.sigmoid(self.fc4(x))  # Sortie entre (0,1)\n",
        "        return x.squeeze(-1)  # Retourne (batch_size,)\n"
      ],
      "metadata": {
        "id": "yubsCkHxoidj"
      },
      "id": "yubsCkHxoidj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entraînement des réseaux de neurones pour l’arrêt optimal\n",
        "\n",
        "Cette fonction entraîne une suite de réseaux $\\{f_n^\\theta\\}_{n=0}^{N-1}$ pour approximer les décisions d'arrêt à chaque date $t_n$. L'apprentissage se fait en backward induction : on commence par $n = N-1$ et on remonte jusqu'à $n = 1$, en fixant $f_N \\equiv 1$ (arrêt obligatoire à maturité).\n",
        "\n",
        "L'objectif à chaque étape $n$ est de maximiser l'espérance :\n",
        "$$\n",
        "\\mathbb{E}\\left[f_n^\\theta(X_n) \\cdot g(n, X_n) + (1 - f_n^\\theta(X_n)) \\cdot g(\\tau_{n+1}, X_{\\tau_{n+1}})\\right],\n",
        "$$\n",
        "où $g(n, x)$ est le payoff à l'instant $n$ et $\\tau_{n+1}$ le temps d'arrêt futur défini par les réseaux déjà entraînés aux dates ultérieures.\n",
        "\n",
        "Le terme $f_n^\\theta(X_n)$ représente la probabilité (souple) d'arrêt donnée par le réseau. Une descente de gradient stochastique (Adam) est utilisée pour optimiser ce critère sur des mini-batchs. Après chaque entraînement, les temps d’arrêt $\\tau^\\theta$ sont mis à jour en appliquant une règle dure : on arrête dès que $f_n^\\theta(X_n) > 0.5$.\n",
        "\n",
        "Cette stratégie récursive permet d'approximer un temps d'arrêt optimal sur des trajectoires simulées, constituant la base de l'estimateur $\\hat{L}$ pour la borne inférieure.\n"
      ],
      "metadata": {
        "id": "y_RKUCXFhXYD"
      },
      "id": "y_RKUCXFhXYD"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_stopping_networks(S_paths, payoffs, N, batch_size=8192, lr=1e-3, n_epochs=1, verbose=True):\n",
        "    \"\"\"\n",
        "    Entraîne les réseaux de neurones pour l'optimal stopping.\n",
        "\n",
        "    S_paths : tensor (N+1, M, d) — trajectoires simulées\n",
        "    payoffs : tensor (N+1, M) — payoffs simulés à chaque pas de temps\n",
        "    N : nombre total de pas de temps\n",
        "    batch_size : taille des mini-batchs\n",
        "    lr : learning rate\n",
        "    n_epochs : nombre d'époques d'apprentissage pour chaque réseau\n",
        "    verbose : bool — si True, affiche la loss pendant l'entraînement\n",
        "\n",
        "    Retourne :\n",
        "    list_models : liste des modèles (de n=N-1 à n=1)\n",
        "    loss_history : liste des pertes pour chaque réseau\n",
        "    \"\"\"\n",
        "    M, d = S_paths.shape[1], S_paths.shape[2]\n",
        "    device = S_paths.device\n",
        "\n",
        "    # Initialisation des réseaux\n",
        "    models = [StoppingNet(d).to(device) for _ in range(N)]\n",
        "\n",
        "    # Stocke l'historique des pertes\n",
        "    loss_history = [[] for _ in range(N)]\n",
        "\n",
        "    # Initialisation des temps d'arrêt connus : à N, on s'arrête toujours\n",
        "    stopping_decision = torch.ones(M, device=device)\n",
        "\n",
        "    for n in reversed(range(1, N)):\n",
        "        model = models[n-1]\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Training model for time step {n}...\")\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            perm = torch.randperm(M)\n",
        "            batch_losses = []\n",
        "            for i in range(0, M, batch_size):\n",
        "                idx = perm[i:i+batch_size]\n",
        "                S_batch = S_paths[n, idx]\n",
        "                S_batch = normalize_batch(S_batch)\n",
        "\n",
        "                payoff_now = payoffs[n, idx]\n",
        "                payoff_future = payoffs[stopping_decision[idx].long(), idx]\n",
        "\n",
        "                # Forward pass\n",
        "                stop_prob = model(S_batch)\n",
        "\n",
        "                # Calcul de la loss\n",
        "                loss = -torch.mean(\n",
        "                    stop_prob * payoff_now + (1 - stop_prob) * payoff_future\n",
        "                )\n",
        "\n",
        "                # Backpropagation\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Stocke la perte pour ce batch\n",
        "                batch_losses.append(loss.item())\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"Time step {n}, Epoch {epoch+1}, Batch {i//batch_size+1}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "            # Enregistre la perte moyenne sur cette époque\n",
        "            loss_history[n-1].append(sum(batch_losses) / len(batch_losses))\n",
        "\n",
        "          # Mise à jour des décisions d'arrêt après entraînement\n",
        "        with torch.no_grad():\n",
        "            S_batch_full = normalize_batch(S_paths[n])\n",
        "            stop_prob_full = model(S_batch_full)\n",
        "            new_stop = stop_prob_full > 0.5  # décision dure\n",
        "            # On met à jour les temps d'arrêt\n",
        "            stopping_decision = torch.where(new_stop, torch.full_like(stopping_decision, n), stopping_decision)\n",
        "\n",
        "    return models, loss_history\n"
      ],
      "metadata": {
        "id": "Tr0gpIoqAACH"
      },
      "id": "Tr0gpIoqAACH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction utilitaire : lissage avec moyenne glissante\n",
        "def moving_average(values, window=5):\n",
        "    \"\"\"\n",
        "    Calcule la moyenne glissante d'une liste de valeurs.\n",
        "    values : list of floats\n",
        "    window : taille de la fenêtre\n",
        "    \"\"\"\n",
        "    if len(values) < window:\n",
        "        return values  # pas assez de points pour lisser\n",
        "    return [sum(values[i-window:i])/window for i in range(window, len(values)+1)]\n"
      ],
      "metadata": {
        "id": "p8ay6P28CZmj"
      },
      "id": "p8ay6P28CZmj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimation de la borne inférieure $\\hat{L}$\n",
        "\n",
        "Ce bloc applique la politique d’arrêt optimale apprise pour estimer la borne inférieure du prix de l’option Bermudéenne. Pour chaque trajectoire simulée, on utilise les réseaux de neurones $\\{f_n^\\theta\\}$ pour déterminer le premier temps $n$ tel que $f_n^\\theta(X_n) > 0.5$, ce qui correspond à l’exercice anticipé de l’option.\n",
        "\n",
        "On calcule ensuite le payoff associé à ce temps d’arrêt $\\tau^\\theta$, puis on actualise ce gain à la date initiale via le facteur $\\exp(-r (T - t_{\\tau^\\theta}))$. L’estimateur final est :\n",
        "$$\n",
        "\\hat{L} = \\frac{1}{M} \\sum_{i=1}^M e^{-r(T - t_{\\tau_i})} \\cdot \\left( \\max_{j=1,\\dots,d} S_j^{(i)}(t_{\\tau_i}) - K \\right)^+,\n",
        "$$\n",
        "où $M$ est le nombre de trajectoires simulées et $\\tau_i$ le temps d’arrêt sur la $i$-ème trajectoire.\n",
        "\n",
        "On retourne à la fois la moyenne des gains actualisés et leur variance empirique.\n"
      ],
      "metadata": {
        "id": "i4rSnBVWhjd8"
      },
      "id": "i4rSnBVWhjd8"
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_lower_bound(models, S_paths, K):\n",
        "    \"\"\"\n",
        "    Estime la borne inférieure du prix de l'option par Monte Carlo (avec actualisation).\n",
        "\n",
        "    Inputs :\n",
        "    - models : liste des réseaux (un par temps d’arrêt t_n)\n",
        "    - S_paths : trajectoires simulées, shape (N+1, M, d)\n",
        "    - K : strike\n",
        "    - r : taux sans risque\n",
        "    - T : maturité\n",
        "\n",
        "    Returns :\n",
        "    - lower_bound : estimation moyenne actualisée\n",
        "    - variance : variance empirique des payoffs actualisés\n",
        "    \"\"\"\n",
        "    N_plus_1, M, d = S_paths.shape\n",
        "    N = N_plus_1 - 1\n",
        "    device = S_paths.device\n",
        "\n",
        "    dt = T / N\n",
        "    times = torch.arange(N_plus_1, device=device) * dt\n",
        "\n",
        "    # Initialisation : arrêt forcé à T par défaut\n",
        "    stopping_times = torch.full((M,), N, dtype=torch.long, device=device)\n",
        "\n",
        "    for n in range(1, N):\n",
        "        model = models[n-1]\n",
        "        S_batch = normalize_batch(S_paths[n])\n",
        "        stop_prob = model(S_batch)\n",
        "        decision = stop_prob > 0.5\n",
        "\n",
        "        stopping_times = torch.where(\n",
        "            (stopping_times == N) & decision,\n",
        "            torch.full_like(stopping_times, n),\n",
        "            stopping_times\n",
        "        )\n",
        "\n",
        "    # Indices de temps arrêt + valeurs à l’arrêt\n",
        "    discount_factors = torch.exp(-r * (T-times[stopping_times]))  # shape (M,)\n",
        "    payoffs_raw = bermudan_max_call_payoff(S_paths[stopping_times, torch.arange(M)], K)\n",
        "\n",
        "    discounted_payoffs = discount_factors * payoffs_raw\n",
        "\n",
        "    lower_bound = discounted_payoffs.mean().item()\n",
        "    variance = discounted_payoffs.var(unbiased=True).item()\n",
        "\n",
        "    return lower_bound, variance\n"
      ],
      "metadata": {
        "id": "yWqTmFJOaCPM"
      },
      "id": "yWqTmFJOaCPM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simuler\n",
        "S_paths = simulate_black_scholes_paths(x0, r, sigma, dividend, T, N=N, M=M, d=d, device=device)\n",
        "\n",
        "# Calculer les payoffs\n",
        "payoffs = compute_all_payoffs(S_paths, K)\n",
        "\n",
        "# Entraîner\n",
        "models, loss_history = train_stopping_networks(S_paths, payoffs, N=9, batch_size=8192, lr=1e-3, n_epochs=500)\n",
        "\n",
        "for n, losses in enumerate(loss_history):\n",
        "    smoothed_losses = moving_average(losses, window=3)\n",
        "    plt.plot(smoothed_losses, label=f'Time step {n+1}')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Smoothed Loss')\n",
        "plt.title('Smoothed Loss Evolution for Each Time Step')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SZ-qzvx_XuFA",
        "outputId": "26d58976-b12e-4aca-8516-ada7d1f29bf3",
        "collapsed": true
      },
      "id": "SZ-qzvx_XuFA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for time step 8...\n",
            "Time step 8, Epoch 1, Batch 1, Loss: -19.669641\n",
            "Time step 8, Epoch 2, Batch 1, Loss: -19.924610\n",
            "Time step 8, Epoch 3, Batch 1, Loss: -20.176554\n",
            "Time step 8, Epoch 4, Batch 1, Loss: -20.425640\n",
            "Time step 8, Epoch 5, Batch 1, Loss: -20.671724\n",
            "Time step 8, Epoch 6, Batch 1, Loss: -20.914625\n",
            "Time step 8, Epoch 7, Batch 1, Loss: -21.154478\n",
            "Time step 8, Epoch 8, Batch 1, Loss: -21.391460\n",
            "Time step 8, Epoch 9, Batch 1, Loss: -21.625961\n",
            "Time step 8, Epoch 10, Batch 1, Loss: -21.858231\n",
            "Time step 8, Epoch 11, Batch 1, Loss: -22.088814\n",
            "Time step 8, Epoch 12, Batch 1, Loss: -22.317373\n",
            "Time step 8, Epoch 13, Batch 1, Loss: -22.543558\n",
            "Time step 8, Epoch 14, Batch 1, Loss: -22.766945\n",
            "Time step 8, Epoch 15, Batch 1, Loss: -22.987343\n",
            "Time step 8, Epoch 16, Batch 1, Loss: -23.204193\n",
            "Time step 8, Epoch 17, Batch 1, Loss: -23.416840\n",
            "Time step 8, Epoch 18, Batch 1, Loss: -23.624720\n",
            "Time step 8, Epoch 19, Batch 1, Loss: -23.826866\n",
            "Time step 8, Epoch 20, Batch 1, Loss: -24.022671\n",
            "Time step 8, Epoch 21, Batch 1, Loss: -24.211065\n",
            "Time step 8, Epoch 22, Batch 1, Loss: -24.391420\n",
            "Time step 8, Epoch 23, Batch 1, Loss: -24.563095\n",
            "Time step 8, Epoch 24, Batch 1, Loss: -24.725204\n",
            "Time step 8, Epoch 25, Batch 1, Loss: -24.877199\n",
            "Time step 8, Epoch 26, Batch 1, Loss: -25.019032\n",
            "Time step 8, Epoch 27, Batch 1, Loss: -25.150572\n",
            "Time step 8, Epoch 28, Batch 1, Loss: -25.271702\n",
            "Time step 8, Epoch 29, Batch 1, Loss: -25.382637\n",
            "Time step 8, Epoch 30, Batch 1, Loss: -25.483496\n",
            "Time step 8, Epoch 31, Batch 1, Loss: -25.574669\n",
            "Time step 8, Epoch 32, Batch 1, Loss: -25.656698\n",
            "Time step 8, Epoch 33, Batch 1, Loss: -25.730011\n",
            "Time step 8, Epoch 34, Batch 1, Loss: -25.795174\n",
            "Time step 8, Epoch 35, Batch 1, Loss: -25.852848\n",
            "Time step 8, Epoch 36, Batch 1, Loss: -25.903660\n",
            "Time step 8, Epoch 37, Batch 1, Loss: -25.948324\n",
            "Time step 8, Epoch 38, Batch 1, Loss: -25.987532\n",
            "Time step 8, Epoch 39, Batch 1, Loss: -26.021889\n",
            "Time step 8, Epoch 40, Batch 1, Loss: -26.051985\n",
            "Time step 8, Epoch 41, Batch 1, Loss: -26.078405\n",
            "Time step 8, Epoch 42, Batch 1, Loss: -26.101637\n",
            "Time step 8, Epoch 43, Batch 1, Loss: -26.122129\n",
            "Time step 8, Epoch 44, Batch 1, Loss: -26.140326\n",
            "Time step 8, Epoch 45, Batch 1, Loss: -26.156586\n",
            "Time step 8, Epoch 46, Batch 1, Loss: -26.171280\n",
            "Time step 8, Epoch 47, Batch 1, Loss: -26.184702\n",
            "Time step 8, Epoch 48, Batch 1, Loss: -26.197121\n",
            "Time step 8, Epoch 49, Batch 1, Loss: -26.208773\n",
            "Time step 8, Epoch 50, Batch 1, Loss: -26.219870\n",
            "Time step 8, Epoch 51, Batch 1, Loss: -26.230602\n",
            "Time step 8, Epoch 52, Batch 1, Loss: -26.241119\n",
            "Time step 8, Epoch 53, Batch 1, Loss: -26.251547\n",
            "Time step 8, Epoch 54, Batch 1, Loss: -26.262047\n",
            "Time step 8, Epoch 55, Batch 1, Loss: -26.272745\n",
            "Time step 8, Epoch 56, Batch 1, Loss: -26.283728\n",
            "Time step 8, Epoch 57, Batch 1, Loss: -26.295074\n",
            "Time step 8, Epoch 58, Batch 1, Loss: -26.306862\n",
            "Time step 8, Epoch 59, Batch 1, Loss: -26.319162\n",
            "Time step 8, Epoch 60, Batch 1, Loss: -26.332050\n",
            "Time step 8, Epoch 61, Batch 1, Loss: -26.345556\n",
            "Time step 8, Epoch 62, Batch 1, Loss: -26.359715\n",
            "Time step 8, Epoch 63, Batch 1, Loss: -26.374485\n",
            "Time step 8, Epoch 64, Batch 1, Loss: -26.389889\n",
            "Time step 8, Epoch 65, Batch 1, Loss: -26.405876\n",
            "Time step 8, Epoch 66, Batch 1, Loss: -26.422424\n",
            "Time step 8, Epoch 67, Batch 1, Loss: -26.439529\n",
            "Time step 8, Epoch 68, Batch 1, Loss: -26.457117\n",
            "Time step 8, Epoch 69, Batch 1, Loss: -26.475151\n",
            "Time step 8, Epoch 70, Batch 1, Loss: -26.493483\n",
            "Time step 8, Epoch 71, Batch 1, Loss: -26.512060\n",
            "Time step 8, Epoch 72, Batch 1, Loss: -26.530851\n",
            "Time step 8, Epoch 73, Batch 1, Loss: -26.549841\n",
            "Time step 8, Epoch 74, Batch 1, Loss: -26.568905\n",
            "Time step 8, Epoch 75, Batch 1, Loss: -26.587980\n",
            "Time step 8, Epoch 76, Batch 1, Loss: -26.607052\n",
            "Time step 8, Epoch 77, Batch 1, Loss: -26.626083\n",
            "Time step 8, Epoch 78, Batch 1, Loss: -26.644943\n",
            "Time step 8, Epoch 79, Batch 1, Loss: -26.663683\n",
            "Time step 8, Epoch 80, Batch 1, Loss: -26.682207\n",
            "Time step 8, Epoch 81, Batch 1, Loss: -26.700600\n",
            "Time step 8, Epoch 82, Batch 1, Loss: -26.718849\n",
            "Time step 8, Epoch 83, Batch 1, Loss: -26.737024\n",
            "Time step 8, Epoch 84, Batch 1, Loss: -26.755095\n",
            "Time step 8, Epoch 85, Batch 1, Loss: -26.773100\n",
            "Time step 8, Epoch 86, Batch 1, Loss: -26.791008\n",
            "Time step 8, Epoch 87, Batch 1, Loss: -26.808884\n",
            "Time step 8, Epoch 88, Batch 1, Loss: -26.826784\n",
            "Time step 8, Epoch 89, Batch 1, Loss: -26.844723\n",
            "Time step 8, Epoch 90, Batch 1, Loss: -26.862728\n",
            "Time step 8, Epoch 91, Batch 1, Loss: -26.880838\n",
            "Time step 8, Epoch 92, Batch 1, Loss: -26.899057\n",
            "Time step 8, Epoch 93, Batch 1, Loss: -26.917397\n",
            "Time step 8, Epoch 94, Batch 1, Loss: -26.935869\n",
            "Time step 8, Epoch 95, Batch 1, Loss: -26.954466\n",
            "Time step 8, Epoch 96, Batch 1, Loss: -26.973148\n",
            "Time step 8, Epoch 97, Batch 1, Loss: -26.991919\n",
            "Time step 8, Epoch 98, Batch 1, Loss: -27.010773\n",
            "Time step 8, Epoch 99, Batch 1, Loss: -27.029736\n",
            "Time step 8, Epoch 100, Batch 1, Loss: -27.048820\n",
            "Time step 8, Epoch 101, Batch 1, Loss: -27.067951\n",
            "Time step 8, Epoch 102, Batch 1, Loss: -27.087164\n",
            "Time step 8, Epoch 103, Batch 1, Loss: -27.106438\n",
            "Time step 8, Epoch 104, Batch 1, Loss: -27.125778\n",
            "Time step 8, Epoch 105, Batch 1, Loss: -27.145174\n",
            "Time step 8, Epoch 106, Batch 1, Loss: -27.164623\n",
            "Time step 8, Epoch 107, Batch 1, Loss: -27.184128\n",
            "Time step 8, Epoch 108, Batch 1, Loss: -27.203697\n",
            "Time step 8, Epoch 109, Batch 1, Loss: -27.223351\n",
            "Time step 8, Epoch 110, Batch 1, Loss: -27.243107\n",
            "Time step 8, Epoch 111, Batch 1, Loss: -27.262945\n",
            "Time step 8, Epoch 112, Batch 1, Loss: -27.282894\n",
            "Time step 8, Epoch 113, Batch 1, Loss: -27.302940\n",
            "Time step 8, Epoch 114, Batch 1, Loss: -27.323095\n",
            "Time step 8, Epoch 115, Batch 1, Loss: -27.343386\n",
            "Time step 8, Epoch 116, Batch 1, Loss: -27.363771\n",
            "Time step 8, Epoch 117, Batch 1, Loss: -27.384233\n",
            "Time step 8, Epoch 118, Batch 1, Loss: -27.404762\n",
            "Time step 8, Epoch 119, Batch 1, Loss: -27.425297\n",
            "Time step 8, Epoch 120, Batch 1, Loss: -27.445816\n",
            "Time step 8, Epoch 121, Batch 1, Loss: -27.466312\n",
            "Time step 8, Epoch 122, Batch 1, Loss: -27.486786\n",
            "Time step 8, Epoch 123, Batch 1, Loss: -27.507215\n",
            "Time step 8, Epoch 124, Batch 1, Loss: -27.527605\n",
            "Time step 8, Epoch 125, Batch 1, Loss: -27.547909\n",
            "Time step 8, Epoch 126, Batch 1, Loss: -27.568087\n",
            "Time step 8, Epoch 127, Batch 1, Loss: -27.588144\n",
            "Time step 8, Epoch 128, Batch 1, Loss: -27.608006\n",
            "Time step 8, Epoch 129, Batch 1, Loss: -27.627666\n",
            "Time step 8, Epoch 130, Batch 1, Loss: -27.647114\n",
            "Time step 8, Epoch 131, Batch 1, Loss: -27.666325\n",
            "Time step 8, Epoch 132, Batch 1, Loss: -27.685274\n",
            "Time step 8, Epoch 133, Batch 1, Loss: -27.703943\n",
            "Time step 8, Epoch 134, Batch 1, Loss: -27.722294\n",
            "Time step 8, Epoch 135, Batch 1, Loss: -27.740326\n",
            "Time step 8, Epoch 136, Batch 1, Loss: -27.758062\n",
            "Time step 8, Epoch 137, Batch 1, Loss: -27.775501\n",
            "Time step 8, Epoch 138, Batch 1, Loss: -27.792622\n",
            "Time step 8, Epoch 139, Batch 1, Loss: -27.809418\n",
            "Time step 8, Epoch 140, Batch 1, Loss: -27.825884\n",
            "Time step 8, Epoch 141, Batch 1, Loss: -27.842031\n",
            "Time step 8, Epoch 142, Batch 1, Loss: -27.857878\n",
            "Time step 8, Epoch 143, Batch 1, Loss: -27.873430\n",
            "Time step 8, Epoch 144, Batch 1, Loss: -27.888718\n",
            "Time step 8, Epoch 145, Batch 1, Loss: -27.903751\n",
            "Time step 8, Epoch 146, Batch 1, Loss: -27.918522\n",
            "Time step 8, Epoch 147, Batch 1, Loss: -27.933046\n",
            "Time step 8, Epoch 148, Batch 1, Loss: -27.947357\n",
            "Time step 8, Epoch 149, Batch 1, Loss: -27.961468\n",
            "Time step 8, Epoch 150, Batch 1, Loss: -27.975357\n",
            "Time step 8, Epoch 151, Batch 1, Loss: -27.989075\n",
            "Time step 8, Epoch 152, Batch 1, Loss: -28.002615\n",
            "Time step 8, Epoch 153, Batch 1, Loss: -28.015987\n",
            "Time step 8, Epoch 154, Batch 1, Loss: -28.029219\n",
            "Time step 8, Epoch 155, Batch 1, Loss: -28.042341\n",
            "Time step 8, Epoch 156, Batch 1, Loss: -28.055351\n",
            "Time step 8, Epoch 157, Batch 1, Loss: -28.068243\n",
            "Time step 8, Epoch 158, Batch 1, Loss: -28.081032\n",
            "Time step 8, Epoch 159, Batch 1, Loss: -28.093723\n",
            "Time step 8, Epoch 160, Batch 1, Loss: -28.106312\n",
            "Time step 8, Epoch 161, Batch 1, Loss: -28.118805\n",
            "Time step 8, Epoch 162, Batch 1, Loss: -28.131193\n",
            "Time step 8, Epoch 163, Batch 1, Loss: -28.143490\n",
            "Time step 8, Epoch 164, Batch 1, Loss: -28.155670\n",
            "Time step 8, Epoch 165, Batch 1, Loss: -28.167725\n",
            "Time step 8, Epoch 166, Batch 1, Loss: -28.179661\n",
            "Time step 8, Epoch 167, Batch 1, Loss: -28.191490\n",
            "Time step 8, Epoch 168, Batch 1, Loss: -28.203205\n",
            "Time step 8, Epoch 169, Batch 1, Loss: -28.214819\n",
            "Time step 8, Epoch 170, Batch 1, Loss: -28.226318\n",
            "Time step 8, Epoch 171, Batch 1, Loss: -28.237713\n",
            "Time step 8, Epoch 172, Batch 1, Loss: -28.248964\n",
            "Time step 8, Epoch 173, Batch 1, Loss: -28.260090\n",
            "Time step 8, Epoch 174, Batch 1, Loss: -28.271091\n",
            "Time step 8, Epoch 175, Batch 1, Loss: -28.281960\n",
            "Time step 8, Epoch 176, Batch 1, Loss: -28.292671\n",
            "Time step 8, Epoch 177, Batch 1, Loss: -28.303230\n",
            "Time step 8, Epoch 178, Batch 1, Loss: -28.313663\n",
            "Time step 8, Epoch 179, Batch 1, Loss: -28.323946\n",
            "Time step 8, Epoch 180, Batch 1, Loss: -28.334091\n",
            "Time step 8, Epoch 181, Batch 1, Loss: -28.344067\n",
            "Time step 8, Epoch 182, Batch 1, Loss: -28.353857\n",
            "Time step 8, Epoch 183, Batch 1, Loss: -28.363480\n",
            "Time step 8, Epoch 184, Batch 1, Loss: -28.372942\n",
            "Time step 8, Epoch 185, Batch 1, Loss: -28.382240\n",
            "Time step 8, Epoch 186, Batch 1, Loss: -28.391352\n",
            "Time step 8, Epoch 187, Batch 1, Loss: -28.400286\n",
            "Time step 8, Epoch 188, Batch 1, Loss: -28.409039\n",
            "Time step 8, Epoch 189, Batch 1, Loss: -28.417603\n",
            "Time step 8, Epoch 190, Batch 1, Loss: -28.425987\n",
            "Time step 8, Epoch 191, Batch 1, Loss: -28.434191\n",
            "Time step 8, Epoch 192, Batch 1, Loss: -28.442215\n",
            "Time step 8, Epoch 193, Batch 1, Loss: -28.450050\n",
            "Time step 8, Epoch 194, Batch 1, Loss: -28.457705\n",
            "Time step 8, Epoch 195, Batch 1, Loss: -28.465183\n",
            "Time step 8, Epoch 196, Batch 1, Loss: -28.472496\n",
            "Time step 8, Epoch 197, Batch 1, Loss: -28.479650\n",
            "Time step 8, Epoch 198, Batch 1, Loss: -28.486652\n",
            "Time step 8, Epoch 199, Batch 1, Loss: -28.493502\n",
            "Time step 8, Epoch 200, Batch 1, Loss: -28.500214\n",
            "Time step 8, Epoch 201, Batch 1, Loss: -28.506786\n",
            "Time step 8, Epoch 202, Batch 1, Loss: -28.513226\n",
            "Time step 8, Epoch 203, Batch 1, Loss: -28.519547\n",
            "Time step 8, Epoch 204, Batch 1, Loss: -28.525734\n",
            "Time step 8, Epoch 205, Batch 1, Loss: -28.531794\n",
            "Time step 8, Epoch 206, Batch 1, Loss: -28.537725\n",
            "Time step 8, Epoch 207, Batch 1, Loss: -28.543531\n",
            "Time step 8, Epoch 208, Batch 1, Loss: -28.549198\n",
            "Time step 8, Epoch 209, Batch 1, Loss: -28.554737\n",
            "Time step 8, Epoch 210, Batch 1, Loss: -28.560141\n",
            "Time step 8, Epoch 211, Batch 1, Loss: -28.565424\n",
            "Time step 8, Epoch 212, Batch 1, Loss: -28.570581\n",
            "Time step 8, Epoch 213, Batch 1, Loss: -28.575609\n",
            "Time step 8, Epoch 214, Batch 1, Loss: -28.580519\n",
            "Time step 8, Epoch 215, Batch 1, Loss: -28.585314\n",
            "Time step 8, Epoch 216, Batch 1, Loss: -28.589996\n",
            "Time step 8, Epoch 217, Batch 1, Loss: -28.594568\n",
            "Time step 8, Epoch 218, Batch 1, Loss: -28.599033\n",
            "Time step 8, Epoch 219, Batch 1, Loss: -28.603399\n",
            "Time step 8, Epoch 220, Batch 1, Loss: -28.607666\n",
            "Time step 8, Epoch 221, Batch 1, Loss: -28.611828\n",
            "Time step 8, Epoch 222, Batch 1, Loss: -28.615891\n",
            "Time step 8, Epoch 223, Batch 1, Loss: -28.619854\n",
            "Time step 8, Epoch 224, Batch 1, Loss: -28.623730\n",
            "Time step 8, Epoch 225, Batch 1, Loss: -28.627508\n",
            "Time step 8, Epoch 226, Batch 1, Loss: -28.631199\n",
            "Time step 8, Epoch 227, Batch 1, Loss: -28.634800\n",
            "Time step 8, Epoch 228, Batch 1, Loss: -28.638321\n",
            "Time step 8, Epoch 229, Batch 1, Loss: -28.641766\n",
            "Time step 8, Epoch 230, Batch 1, Loss: -28.645130\n",
            "Time step 8, Epoch 231, Batch 1, Loss: -28.648432\n",
            "Time step 8, Epoch 232, Batch 1, Loss: -28.651667\n",
            "Time step 8, Epoch 233, Batch 1, Loss: -28.654839\n",
            "Time step 8, Epoch 234, Batch 1, Loss: -28.657953\n",
            "Time step 8, Epoch 235, Batch 1, Loss: -28.660990\n",
            "Time step 8, Epoch 236, Batch 1, Loss: -28.663963\n",
            "Time step 8, Epoch 237, Batch 1, Loss: -28.666870\n",
            "Time step 8, Epoch 238, Batch 1, Loss: -28.669724\n",
            "Time step 8, Epoch 239, Batch 1, Loss: -28.672522\n",
            "Time step 8, Epoch 240, Batch 1, Loss: -28.675261\n",
            "Time step 8, Epoch 241, Batch 1, Loss: -28.677944\n",
            "Time step 8, Epoch 242, Batch 1, Loss: -28.680565\n",
            "Time step 8, Epoch 243, Batch 1, Loss: -28.683125\n",
            "Time step 8, Epoch 244, Batch 1, Loss: -28.685638\n",
            "Time step 8, Epoch 245, Batch 1, Loss: -28.688101\n",
            "Time step 8, Epoch 246, Batch 1, Loss: -28.690514\n",
            "Time step 8, Epoch 247, Batch 1, Loss: -28.692871\n",
            "Time step 8, Epoch 248, Batch 1, Loss: -28.695185\n",
            "Time step 8, Epoch 249, Batch 1, Loss: -28.697447\n",
            "Time step 8, Epoch 250, Batch 1, Loss: -28.699673\n",
            "Time step 8, Epoch 251, Batch 1, Loss: -28.701862\n",
            "Time step 8, Epoch 252, Batch 1, Loss: -28.703997\n",
            "Time step 8, Epoch 253, Batch 1, Loss: -28.706091\n",
            "Time step 8, Epoch 254, Batch 1, Loss: -28.708155\n",
            "Time step 8, Epoch 255, Batch 1, Loss: -28.710186\n",
            "Time step 8, Epoch 256, Batch 1, Loss: -28.712179\n",
            "Time step 8, Epoch 257, Batch 1, Loss: -28.714134\n",
            "Time step 8, Epoch 258, Batch 1, Loss: -28.716066\n",
            "Time step 8, Epoch 259, Batch 1, Loss: -28.717962\n",
            "Time step 8, Epoch 260, Batch 1, Loss: -28.719835\n",
            "Time step 8, Epoch 261, Batch 1, Loss: -28.721678\n",
            "Time step 8, Epoch 262, Batch 1, Loss: -28.723499\n",
            "Time step 8, Epoch 263, Batch 1, Loss: -28.725281\n",
            "Time step 8, Epoch 264, Batch 1, Loss: -28.727036\n",
            "Time step 8, Epoch 265, Batch 1, Loss: -28.728752\n",
            "Time step 8, Epoch 266, Batch 1, Loss: -28.730431\n",
            "Time step 8, Epoch 267, Batch 1, Loss: -28.732084\n",
            "Time step 8, Epoch 268, Batch 1, Loss: -28.733707\n",
            "Time step 8, Epoch 269, Batch 1, Loss: -28.735306\n",
            "Time step 8, Epoch 270, Batch 1, Loss: -28.736872\n",
            "Time step 8, Epoch 271, Batch 1, Loss: -28.738419\n",
            "Time step 8, Epoch 272, Batch 1, Loss: -28.739937\n",
            "Time step 8, Epoch 273, Batch 1, Loss: -28.741436\n",
            "Time step 8, Epoch 274, Batch 1, Loss: -28.742908\n",
            "Time step 8, Epoch 275, Batch 1, Loss: -28.744356\n",
            "Time step 8, Epoch 276, Batch 1, Loss: -28.745785\n",
            "Time step 8, Epoch 277, Batch 1, Loss: -28.747198\n",
            "Time step 8, Epoch 278, Batch 1, Loss: -28.748592\n",
            "Time step 8, Epoch 279, Batch 1, Loss: -28.749964\n",
            "Time step 8, Epoch 280, Batch 1, Loss: -28.751310\n",
            "Time step 8, Epoch 281, Batch 1, Loss: -28.752632\n",
            "Time step 8, Epoch 282, Batch 1, Loss: -28.753944\n",
            "Time step 8, Epoch 283, Batch 1, Loss: -28.755253\n",
            "Time step 8, Epoch 284, Batch 1, Loss: -28.756546\n",
            "Time step 8, Epoch 285, Batch 1, Loss: -28.757820\n",
            "Time step 8, Epoch 286, Batch 1, Loss: -28.759077\n",
            "Time step 8, Epoch 287, Batch 1, Loss: -28.760319\n",
            "Time step 8, Epoch 288, Batch 1, Loss: -28.761557\n",
            "Time step 8, Epoch 289, Batch 1, Loss: -28.762779\n",
            "Time step 8, Epoch 290, Batch 1, Loss: -28.763985\n",
            "Time step 8, Epoch 291, Batch 1, Loss: -28.765175\n",
            "Time step 8, Epoch 292, Batch 1, Loss: -28.766352\n",
            "Time step 8, Epoch 293, Batch 1, Loss: -28.767521\n",
            "Time step 8, Epoch 294, Batch 1, Loss: -28.768684\n",
            "Time step 8, Epoch 295, Batch 1, Loss: -28.769838\n",
            "Time step 8, Epoch 296, Batch 1, Loss: -28.770979\n",
            "Time step 8, Epoch 297, Batch 1, Loss: -28.772112\n",
            "Time step 8, Epoch 298, Batch 1, Loss: -28.773230\n",
            "Time step 8, Epoch 299, Batch 1, Loss: -28.774330\n",
            "Time step 8, Epoch 300, Batch 1, Loss: -28.775427\n",
            "Time step 8, Epoch 301, Batch 1, Loss: -28.776516\n",
            "Time step 8, Epoch 302, Batch 1, Loss: -28.777594\n",
            "Time step 8, Epoch 303, Batch 1, Loss: -28.778666\n",
            "Time step 8, Epoch 304, Batch 1, Loss: -28.779724\n",
            "Time step 8, Epoch 305, Batch 1, Loss: -28.780781\n",
            "Time step 8, Epoch 306, Batch 1, Loss: -28.781837\n",
            "Time step 8, Epoch 307, Batch 1, Loss: -28.782894\n",
            "Time step 8, Epoch 308, Batch 1, Loss: -28.783945\n",
            "Time step 8, Epoch 309, Batch 1, Loss: -28.784985\n",
            "Time step 8, Epoch 310, Batch 1, Loss: -28.786015\n",
            "Time step 8, Epoch 311, Batch 1, Loss: -28.787035\n",
            "Time step 8, Epoch 312, Batch 1, Loss: -28.788046\n",
            "Time step 8, Epoch 313, Batch 1, Loss: -28.789043\n",
            "Time step 8, Epoch 314, Batch 1, Loss: -28.790039\n",
            "Time step 8, Epoch 315, Batch 1, Loss: -28.791023\n",
            "Time step 8, Epoch 316, Batch 1, Loss: -28.792000\n",
            "Time step 8, Epoch 317, Batch 1, Loss: -28.792965\n",
            "Time step 8, Epoch 318, Batch 1, Loss: -28.793926\n",
            "Time step 8, Epoch 319, Batch 1, Loss: -28.794882\n",
            "Time step 8, Epoch 320, Batch 1, Loss: -28.795826\n",
            "Time step 8, Epoch 321, Batch 1, Loss: -28.796761\n",
            "Time step 8, Epoch 322, Batch 1, Loss: -28.797688\n",
            "Time step 8, Epoch 323, Batch 1, Loss: -28.798607\n",
            "Time step 8, Epoch 324, Batch 1, Loss: -28.799522\n",
            "Time step 8, Epoch 325, Batch 1, Loss: -28.800436\n",
            "Time step 8, Epoch 326, Batch 1, Loss: -28.801346\n",
            "Time step 8, Epoch 327, Batch 1, Loss: -28.802250\n",
            "Time step 8, Epoch 328, Batch 1, Loss: -28.803139\n",
            "Time step 8, Epoch 329, Batch 1, Loss: -28.804020\n",
            "Time step 8, Epoch 330, Batch 1, Loss: -28.804890\n",
            "Time step 8, Epoch 331, Batch 1, Loss: -28.805759\n",
            "Time step 8, Epoch 332, Batch 1, Loss: -28.806614\n",
            "Time step 8, Epoch 333, Batch 1, Loss: -28.807467\n",
            "Time step 8, Epoch 334, Batch 1, Loss: -28.808308\n",
            "Time step 8, Epoch 335, Batch 1, Loss: -28.809137\n",
            "Time step 8, Epoch 336, Batch 1, Loss: -28.809973\n",
            "Time step 8, Epoch 337, Batch 1, Loss: -28.810795\n",
            "Time step 8, Epoch 338, Batch 1, Loss: -28.811611\n",
            "Time step 8, Epoch 339, Batch 1, Loss: -28.812424\n",
            "Time step 8, Epoch 340, Batch 1, Loss: -28.813234\n",
            "Time step 8, Epoch 341, Batch 1, Loss: -28.814037\n",
            "Time step 8, Epoch 342, Batch 1, Loss: -28.814835\n",
            "Time step 8, Epoch 343, Batch 1, Loss: -28.815624\n",
            "Time step 8, Epoch 344, Batch 1, Loss: -28.816410\n",
            "Time step 8, Epoch 345, Batch 1, Loss: -28.817190\n",
            "Time step 8, Epoch 346, Batch 1, Loss: -28.817957\n",
            "Time step 8, Epoch 347, Batch 1, Loss: -28.818722\n",
            "Time step 8, Epoch 348, Batch 1, Loss: -28.819477\n",
            "Time step 8, Epoch 349, Batch 1, Loss: -28.820225\n",
            "Time step 8, Epoch 350, Batch 1, Loss: -28.820967\n",
            "Time step 8, Epoch 351, Batch 1, Loss: -28.821709\n",
            "Time step 8, Epoch 352, Batch 1, Loss: -28.822445\n",
            "Time step 8, Epoch 353, Batch 1, Loss: -28.823175\n",
            "Time step 8, Epoch 354, Batch 1, Loss: -28.823902\n",
            "Time step 8, Epoch 355, Batch 1, Loss: -28.824631\n",
            "Time step 8, Epoch 356, Batch 1, Loss: -28.825352\n",
            "Time step 8, Epoch 357, Batch 1, Loss: -28.826065\n",
            "Time step 8, Epoch 358, Batch 1, Loss: -28.826778\n",
            "Time step 8, Epoch 359, Batch 1, Loss: -28.827486\n",
            "Time step 8, Epoch 360, Batch 1, Loss: -28.828188\n",
            "Time step 8, Epoch 361, Batch 1, Loss: -28.828888\n",
            "Time step 8, Epoch 362, Batch 1, Loss: -28.829582\n",
            "Time step 8, Epoch 363, Batch 1, Loss: -28.830273\n",
            "Time step 8, Epoch 364, Batch 1, Loss: -28.830959\n",
            "Time step 8, Epoch 365, Batch 1, Loss: -28.831646\n",
            "Time step 8, Epoch 366, Batch 1, Loss: -28.832333\n",
            "Time step 8, Epoch 367, Batch 1, Loss: -28.833015\n",
            "Time step 8, Epoch 368, Batch 1, Loss: -28.833694\n",
            "Time step 8, Epoch 369, Batch 1, Loss: -28.834372\n",
            "Time step 8, Epoch 370, Batch 1, Loss: -28.835041\n",
            "Time step 8, Epoch 371, Batch 1, Loss: -28.835712\n",
            "Time step 8, Epoch 372, Batch 1, Loss: -28.836380\n",
            "Time step 8, Epoch 373, Batch 1, Loss: -28.837044\n",
            "Time step 8, Epoch 374, Batch 1, Loss: -28.837700\n",
            "Time step 8, Epoch 375, Batch 1, Loss: -28.838346\n",
            "Time step 8, Epoch 376, Batch 1, Loss: -28.839001\n",
            "Time step 8, Epoch 377, Batch 1, Loss: -28.839636\n",
            "Time step 8, Epoch 378, Batch 1, Loss: -28.840279\n",
            "Time step 8, Epoch 379, Batch 1, Loss: -28.840904\n",
            "Time step 8, Epoch 380, Batch 1, Loss: -28.841532\n",
            "Time step 8, Epoch 381, Batch 1, Loss: -28.842159\n",
            "Time step 8, Epoch 382, Batch 1, Loss: -28.842783\n",
            "Time step 8, Epoch 383, Batch 1, Loss: -28.843405\n",
            "Time step 8, Epoch 384, Batch 1, Loss: -28.844025\n",
            "Time step 8, Epoch 385, Batch 1, Loss: -28.844646\n",
            "Time step 8, Epoch 386, Batch 1, Loss: -28.845259\n",
            "Time step 8, Epoch 387, Batch 1, Loss: -28.845871\n",
            "Time step 8, Epoch 388, Batch 1, Loss: -28.846478\n",
            "Time step 8, Epoch 389, Batch 1, Loss: -28.847084\n",
            "Time step 8, Epoch 390, Batch 1, Loss: -28.847687\n",
            "Time step 8, Epoch 391, Batch 1, Loss: -28.848286\n",
            "Time step 8, Epoch 392, Batch 1, Loss: -28.848883\n",
            "Time step 8, Epoch 393, Batch 1, Loss: -28.849476\n",
            "Time step 8, Epoch 394, Batch 1, Loss: -28.850063\n",
            "Time step 8, Epoch 395, Batch 1, Loss: -28.850647\n",
            "Time step 8, Epoch 396, Batch 1, Loss: -28.851229\n",
            "Time step 8, Epoch 397, Batch 1, Loss: -28.851809\n",
            "Time step 8, Epoch 398, Batch 1, Loss: -28.852388\n",
            "Time step 8, Epoch 399, Batch 1, Loss: -28.852964\n",
            "Time step 8, Epoch 400, Batch 1, Loss: -28.853539\n",
            "Time step 8, Epoch 401, Batch 1, Loss: -28.854115\n",
            "Time step 8, Epoch 402, Batch 1, Loss: -28.854687\n",
            "Time step 8, Epoch 403, Batch 1, Loss: -28.855257\n",
            "Time step 8, Epoch 404, Batch 1, Loss: -28.855827\n",
            "Time step 8, Epoch 405, Batch 1, Loss: -28.856392\n",
            "Time step 8, Epoch 406, Batch 1, Loss: -28.856956\n",
            "Time step 8, Epoch 407, Batch 1, Loss: -28.857521\n",
            "Time step 8, Epoch 408, Batch 1, Loss: -28.858080\n",
            "Time step 8, Epoch 409, Batch 1, Loss: -28.858639\n",
            "Time step 8, Epoch 410, Batch 1, Loss: -28.859196\n",
            "Time step 8, Epoch 411, Batch 1, Loss: -28.859749\n",
            "Time step 8, Epoch 412, Batch 1, Loss: -28.860306\n",
            "Time step 8, Epoch 413, Batch 1, Loss: -28.860867\n",
            "Time step 8, Epoch 414, Batch 1, Loss: -28.861427\n",
            "Time step 8, Epoch 415, Batch 1, Loss: -28.861984\n",
            "Time step 8, Epoch 416, Batch 1, Loss: -28.862545\n",
            "Time step 8, Epoch 417, Batch 1, Loss: -28.863098\n",
            "Time step 8, Epoch 418, Batch 1, Loss: -28.863655\n",
            "Time step 8, Epoch 419, Batch 1, Loss: -28.864212\n",
            "Time step 8, Epoch 420, Batch 1, Loss: -28.864771\n",
            "Time step 8, Epoch 421, Batch 1, Loss: -28.865326\n",
            "Time step 8, Epoch 422, Batch 1, Loss: -28.865881\n",
            "Time step 8, Epoch 423, Batch 1, Loss: -28.866425\n",
            "Time step 8, Epoch 424, Batch 1, Loss: -28.866970\n",
            "Time step 8, Epoch 425, Batch 1, Loss: -28.867512\n",
            "Time step 8, Epoch 426, Batch 1, Loss: -28.868050\n",
            "Time step 8, Epoch 427, Batch 1, Loss: -28.868582\n",
            "Time step 8, Epoch 428, Batch 1, Loss: -28.869114\n",
            "Time step 8, Epoch 429, Batch 1, Loss: -28.869642\n",
            "Time step 8, Epoch 430, Batch 1, Loss: -28.870171\n",
            "Time step 8, Epoch 431, Batch 1, Loss: -28.870695\n",
            "Time step 8, Epoch 432, Batch 1, Loss: -28.871220\n",
            "Time step 8, Epoch 433, Batch 1, Loss: -28.871738\n",
            "Time step 8, Epoch 434, Batch 1, Loss: -28.872261\n",
            "Time step 8, Epoch 435, Batch 1, Loss: -28.872784\n",
            "Time step 8, Epoch 436, Batch 1, Loss: -28.873301\n",
            "Time step 8, Epoch 437, Batch 1, Loss: -28.873817\n",
            "Time step 8, Epoch 438, Batch 1, Loss: -28.874332\n",
            "Time step 8, Epoch 439, Batch 1, Loss: -28.874846\n",
            "Time step 8, Epoch 440, Batch 1, Loss: -28.875355\n",
            "Time step 8, Epoch 441, Batch 1, Loss: -28.875868\n",
            "Time step 8, Epoch 442, Batch 1, Loss: -28.876377\n",
            "Time step 8, Epoch 443, Batch 1, Loss: -28.876890\n",
            "Time step 8, Epoch 444, Batch 1, Loss: -28.877396\n",
            "Time step 8, Epoch 445, Batch 1, Loss: -28.877903\n",
            "Time step 8, Epoch 446, Batch 1, Loss: -28.878405\n",
            "Time step 8, Epoch 447, Batch 1, Loss: -28.878906\n",
            "Time step 8, Epoch 448, Batch 1, Loss: -28.879408\n",
            "Time step 8, Epoch 449, Batch 1, Loss: -28.879910\n",
            "Time step 8, Epoch 450, Batch 1, Loss: -28.880409\n",
            "Time step 8, Epoch 451, Batch 1, Loss: -28.880909\n",
            "Time step 8, Epoch 452, Batch 1, Loss: -28.881409\n",
            "Time step 8, Epoch 453, Batch 1, Loss: -28.881901\n",
            "Time step 8, Epoch 454, Batch 1, Loss: -28.882393\n",
            "Time step 8, Epoch 455, Batch 1, Loss: -28.882885\n",
            "Time step 8, Epoch 456, Batch 1, Loss: -28.883373\n",
            "Time step 8, Epoch 457, Batch 1, Loss: -28.883858\n",
            "Time step 8, Epoch 458, Batch 1, Loss: -28.884342\n",
            "Time step 8, Epoch 459, Batch 1, Loss: -28.884821\n",
            "Time step 8, Epoch 460, Batch 1, Loss: -28.885300\n",
            "Time step 8, Epoch 461, Batch 1, Loss: -28.885780\n",
            "Time step 8, Epoch 462, Batch 1, Loss: -28.886259\n",
            "Time step 8, Epoch 463, Batch 1, Loss: -28.886732\n",
            "Time step 8, Epoch 464, Batch 1, Loss: -28.887205\n",
            "Time step 8, Epoch 465, Batch 1, Loss: -28.887680\n",
            "Time step 8, Epoch 466, Batch 1, Loss: -28.888149\n",
            "Time step 8, Epoch 467, Batch 1, Loss: -28.888618\n",
            "Time step 8, Epoch 468, Batch 1, Loss: -28.889082\n",
            "Time step 8, Epoch 469, Batch 1, Loss: -28.889545\n",
            "Time step 8, Epoch 470, Batch 1, Loss: -28.890001\n",
            "Time step 8, Epoch 471, Batch 1, Loss: -28.890463\n",
            "Time step 8, Epoch 472, Batch 1, Loss: -28.890919\n",
            "Time step 8, Epoch 473, Batch 1, Loss: -28.891376\n",
            "Time step 8, Epoch 474, Batch 1, Loss: -28.891825\n",
            "Time step 8, Epoch 475, Batch 1, Loss: -28.892273\n",
            "Time step 8, Epoch 476, Batch 1, Loss: -28.892715\n",
            "Time step 8, Epoch 477, Batch 1, Loss: -28.893160\n",
            "Time step 8, Epoch 478, Batch 1, Loss: -28.893608\n",
            "Time step 8, Epoch 479, Batch 1, Loss: -28.894056\n",
            "Time step 8, Epoch 480, Batch 1, Loss: -28.894501\n",
            "Time step 8, Epoch 481, Batch 1, Loss: -28.894947\n",
            "Time step 8, Epoch 482, Batch 1, Loss: -28.895388\n",
            "Time step 8, Epoch 483, Batch 1, Loss: -28.895832\n",
            "Time step 8, Epoch 484, Batch 1, Loss: -28.896278\n",
            "Time step 8, Epoch 485, Batch 1, Loss: -28.896717\n",
            "Time step 8, Epoch 486, Batch 1, Loss: -28.897161\n",
            "Time step 8, Epoch 487, Batch 1, Loss: -28.897598\n",
            "Time step 8, Epoch 488, Batch 1, Loss: -28.898039\n",
            "Time step 8, Epoch 489, Batch 1, Loss: -28.898483\n",
            "Time step 8, Epoch 490, Batch 1, Loss: -28.898932\n",
            "Time step 8, Epoch 491, Batch 1, Loss: -28.899374\n",
            "Time step 8, Epoch 492, Batch 1, Loss: -28.899815\n",
            "Time step 8, Epoch 493, Batch 1, Loss: -28.900259\n",
            "Time step 8, Epoch 494, Batch 1, Loss: -28.900705\n",
            "Time step 8, Epoch 495, Batch 1, Loss: -28.901154\n",
            "Time step 8, Epoch 496, Batch 1, Loss: -28.901602\n",
            "Time step 8, Epoch 497, Batch 1, Loss: -28.902054\n",
            "Time step 8, Epoch 498, Batch 1, Loss: -28.902504\n",
            "Time step 8, Epoch 499, Batch 1, Loss: -28.902952\n",
            "Time step 8, Epoch 500, Batch 1, Loss: -28.903402\n",
            "Training model for time step 7...\n",
            "Time step 7, Epoch 1, Batch 1, Loss: -27.017544\n",
            "Time step 7, Epoch 2, Batch 1, Loss: -27.060364\n",
            "Time step 7, Epoch 3, Batch 1, Loss: -27.102190\n",
            "Time step 7, Epoch 4, Batch 1, Loss: -27.143066\n",
            "Time step 7, Epoch 5, Batch 1, Loss: -27.183092\n",
            "Time step 7, Epoch 6, Batch 1, Loss: -27.222471\n",
            "Time step 7, Epoch 7, Batch 1, Loss: -27.261400\n",
            "Time step 7, Epoch 8, Batch 1, Loss: -27.299883\n",
            "Time step 7, Epoch 9, Batch 1, Loss: -27.337973\n",
            "Time step 7, Epoch 10, Batch 1, Loss: -27.375820\n",
            "Time step 7, Epoch 11, Batch 1, Loss: -27.413300\n",
            "Time step 7, Epoch 12, Batch 1, Loss: -27.450645\n",
            "Time step 7, Epoch 13, Batch 1, Loss: -27.488073\n",
            "Time step 7, Epoch 14, Batch 1, Loss: -27.525490\n",
            "Time step 7, Epoch 15, Batch 1, Loss: -27.562939\n",
            "Time step 7, Epoch 16, Batch 1, Loss: -27.600456\n",
            "Time step 7, Epoch 17, Batch 1, Loss: -27.638111\n",
            "Time step 7, Epoch 18, Batch 1, Loss: -27.675987\n",
            "Time step 7, Epoch 19, Batch 1, Loss: -27.714283\n",
            "Time step 7, Epoch 20, Batch 1, Loss: -27.753036\n",
            "Time step 7, Epoch 21, Batch 1, Loss: -27.792356\n",
            "Time step 7, Epoch 22, Batch 1, Loss: -27.832312\n",
            "Time step 7, Epoch 23, Batch 1, Loss: -27.872925\n",
            "Time step 7, Epoch 24, Batch 1, Loss: -27.914322\n",
            "Time step 7, Epoch 25, Batch 1, Loss: -27.956438\n",
            "Time step 7, Epoch 26, Batch 1, Loss: -27.999279\n",
            "Time step 7, Epoch 27, Batch 1, Loss: -28.042753\n",
            "Time step 7, Epoch 28, Batch 1, Loss: -28.086798\n",
            "Time step 7, Epoch 29, Batch 1, Loss: -28.131227\n",
            "Time step 7, Epoch 30, Batch 1, Loss: -28.175945\n",
            "Time step 7, Epoch 31, Batch 1, Loss: -28.220823\n",
            "Time step 7, Epoch 32, Batch 1, Loss: -28.265724\n",
            "Time step 7, Epoch 33, Batch 1, Loss: -28.310347\n",
            "Time step 7, Epoch 34, Batch 1, Loss: -28.354490\n",
            "Time step 7, Epoch 35, Batch 1, Loss: -28.397892\n",
            "Time step 7, Epoch 36, Batch 1, Loss: -28.440302\n",
            "Time step 7, Epoch 37, Batch 1, Loss: -28.481560\n",
            "Time step 7, Epoch 38, Batch 1, Loss: -28.521397\n",
            "Time step 7, Epoch 39, Batch 1, Loss: -28.559580\n",
            "Time step 7, Epoch 40, Batch 1, Loss: -28.595900\n",
            "Time step 7, Epoch 41, Batch 1, Loss: -28.630175\n",
            "Time step 7, Epoch 42, Batch 1, Loss: -28.662275\n",
            "Time step 7, Epoch 43, Batch 1, Loss: -28.692093\n",
            "Time step 7, Epoch 44, Batch 1, Loss: -28.719627\n",
            "Time step 7, Epoch 45, Batch 1, Loss: -28.744865\n",
            "Time step 7, Epoch 46, Batch 1, Loss: -28.767857\n",
            "Time step 7, Epoch 47, Batch 1, Loss: -28.788645\n",
            "Time step 7, Epoch 48, Batch 1, Loss: -28.807354\n",
            "Time step 7, Epoch 49, Batch 1, Loss: -28.824087\n",
            "Time step 7, Epoch 50, Batch 1, Loss: -28.838995\n",
            "Time step 7, Epoch 51, Batch 1, Loss: -28.852209\n",
            "Time step 7, Epoch 52, Batch 1, Loss: -28.863880\n",
            "Time step 7, Epoch 53, Batch 1, Loss: -28.874136\n",
            "Time step 7, Epoch 54, Batch 1, Loss: -28.883137\n",
            "Time step 7, Epoch 55, Batch 1, Loss: -28.891020\n",
            "Time step 7, Epoch 56, Batch 1, Loss: -28.897911\n",
            "Time step 7, Epoch 57, Batch 1, Loss: -28.903934\n",
            "Time step 7, Epoch 58, Batch 1, Loss: -28.909185\n",
            "Time step 7, Epoch 59, Batch 1, Loss: -28.913767\n",
            "Time step 7, Epoch 60, Batch 1, Loss: -28.917765\n",
            "Time step 7, Epoch 61, Batch 1, Loss: -28.921261\n",
            "Time step 7, Epoch 62, Batch 1, Loss: -28.924324\n",
            "Time step 7, Epoch 63, Batch 1, Loss: -28.927013\n",
            "Time step 7, Epoch 64, Batch 1, Loss: -28.929373\n",
            "Time step 7, Epoch 65, Batch 1, Loss: -28.931454\n",
            "Time step 7, Epoch 66, Batch 1, Loss: -28.933290\n",
            "Time step 7, Epoch 67, Batch 1, Loss: -28.934912\n",
            "Time step 7, Epoch 68, Batch 1, Loss: -28.936352\n",
            "Time step 7, Epoch 69, Batch 1, Loss: -28.937630\n",
            "Time step 7, Epoch 70, Batch 1, Loss: -28.938770\n",
            "Time step 7, Epoch 71, Batch 1, Loss: -28.939787\n",
            "Time step 7, Epoch 72, Batch 1, Loss: -28.940697\n",
            "Time step 7, Epoch 73, Batch 1, Loss: -28.941513\n",
            "Time step 7, Epoch 74, Batch 1, Loss: -28.942249\n",
            "Time step 7, Epoch 75, Batch 1, Loss: -28.942913\n",
            "Time step 7, Epoch 76, Batch 1, Loss: -28.943510\n",
            "Time step 7, Epoch 77, Batch 1, Loss: -28.944052\n",
            "Time step 7, Epoch 78, Batch 1, Loss: -28.944548\n",
            "Time step 7, Epoch 79, Batch 1, Loss: -28.944998\n",
            "Time step 7, Epoch 80, Batch 1, Loss: -28.945408\n",
            "Time step 7, Epoch 81, Batch 1, Loss: -28.945784\n",
            "Time step 7, Epoch 82, Batch 1, Loss: -28.946129\n",
            "Time step 7, Epoch 83, Batch 1, Loss: -28.946447\n",
            "Time step 7, Epoch 84, Batch 1, Loss: -28.946741\n",
            "Time step 7, Epoch 85, Batch 1, Loss: -28.947014\n",
            "Time step 7, Epoch 86, Batch 1, Loss: -28.947268\n",
            "Time step 7, Epoch 87, Batch 1, Loss: -28.947504\n",
            "Time step 7, Epoch 88, Batch 1, Loss: -28.947720\n",
            "Time step 7, Epoch 89, Batch 1, Loss: -28.947922\n",
            "Time step 7, Epoch 90, Batch 1, Loss: -28.948114\n",
            "Time step 7, Epoch 91, Batch 1, Loss: -28.948290\n",
            "Time step 7, Epoch 92, Batch 1, Loss: -28.948460\n",
            "Time step 7, Epoch 93, Batch 1, Loss: -28.948620\n",
            "Time step 7, Epoch 94, Batch 1, Loss: -28.948767\n",
            "Time step 7, Epoch 95, Batch 1, Loss: -28.948906\n",
            "Time step 7, Epoch 96, Batch 1, Loss: -28.949041\n",
            "Time step 7, Epoch 97, Batch 1, Loss: -28.949165\n",
            "Time step 7, Epoch 98, Batch 1, Loss: -28.949286\n",
            "Time step 7, Epoch 99, Batch 1, Loss: -28.949402\n",
            "Time step 7, Epoch 100, Batch 1, Loss: -28.949509\n",
            "Time step 7, Epoch 101, Batch 1, Loss: -28.949612\n",
            "Time step 7, Epoch 102, Batch 1, Loss: -28.949711\n",
            "Time step 7, Epoch 103, Batch 1, Loss: -28.949806\n",
            "Time step 7, Epoch 104, Batch 1, Loss: -28.949894\n",
            "Time step 7, Epoch 105, Batch 1, Loss: -28.949982\n",
            "Time step 7, Epoch 106, Batch 1, Loss: -28.950066\n",
            "Time step 7, Epoch 107, Batch 1, Loss: -28.950144\n",
            "Time step 7, Epoch 108, Batch 1, Loss: -28.950222\n",
            "Time step 7, Epoch 109, Batch 1, Loss: -28.950296\n",
            "Time step 7, Epoch 110, Batch 1, Loss: -28.950365\n",
            "Time step 7, Epoch 111, Batch 1, Loss: -28.950434\n",
            "Time step 7, Epoch 112, Batch 1, Loss: -28.950500\n",
            "Time step 7, Epoch 113, Batch 1, Loss: -28.950565\n",
            "Time step 7, Epoch 114, Batch 1, Loss: -28.950626\n",
            "Time step 7, Epoch 115, Batch 1, Loss: -28.950684\n",
            "Time step 7, Epoch 116, Batch 1, Loss: -28.950745\n",
            "Time step 7, Epoch 117, Batch 1, Loss: -28.950800\n",
            "Time step 7, Epoch 118, Batch 1, Loss: -28.950853\n",
            "Time step 7, Epoch 119, Batch 1, Loss: -28.950907\n",
            "Time step 7, Epoch 120, Batch 1, Loss: -28.950958\n",
            "Time step 7, Epoch 121, Batch 1, Loss: -28.951008\n",
            "Time step 7, Epoch 122, Batch 1, Loss: -28.951057\n",
            "Time step 7, Epoch 123, Batch 1, Loss: -28.951107\n",
            "Time step 7, Epoch 124, Batch 1, Loss: -28.951149\n",
            "Time step 7, Epoch 125, Batch 1, Loss: -28.951195\n",
            "Time step 7, Epoch 126, Batch 1, Loss: -28.951239\n",
            "Time step 7, Epoch 127, Batch 1, Loss: -28.951279\n",
            "Time step 7, Epoch 128, Batch 1, Loss: -28.951323\n",
            "Time step 7, Epoch 129, Batch 1, Loss: -28.951363\n",
            "Time step 7, Epoch 130, Batch 1, Loss: -28.951403\n",
            "Time step 7, Epoch 131, Batch 1, Loss: -28.951443\n",
            "Time step 7, Epoch 132, Batch 1, Loss: -28.951479\n",
            "Time step 7, Epoch 133, Batch 1, Loss: -28.951511\n",
            "Time step 7, Epoch 134, Batch 1, Loss: -28.951550\n",
            "Time step 7, Epoch 135, Batch 1, Loss: -28.951582\n",
            "Time step 7, Epoch 136, Batch 1, Loss: -28.951618\n",
            "Time step 7, Epoch 137, Batch 1, Loss: -28.951651\n",
            "Time step 7, Epoch 138, Batch 1, Loss: -28.951685\n",
            "Time step 7, Epoch 139, Batch 1, Loss: -28.951714\n",
            "Time step 7, Epoch 140, Batch 1, Loss: -28.951746\n",
            "Time step 7, Epoch 141, Batch 1, Loss: -28.951777\n",
            "Time step 7, Epoch 142, Batch 1, Loss: -28.951809\n",
            "Time step 7, Epoch 143, Batch 1, Loss: -28.951836\n",
            "Time step 7, Epoch 144, Batch 1, Loss: -28.951864\n",
            "Time step 7, Epoch 145, Batch 1, Loss: -28.951891\n",
            "Time step 7, Epoch 146, Batch 1, Loss: -28.951920\n",
            "Time step 7, Epoch 147, Batch 1, Loss: -28.951946\n",
            "Time step 7, Epoch 148, Batch 1, Loss: -28.951973\n",
            "Time step 7, Epoch 149, Batch 1, Loss: -28.951998\n",
            "Time step 7, Epoch 150, Batch 1, Loss: -28.952023\n",
            "Time step 7, Epoch 151, Batch 1, Loss: -28.952047\n",
            "Time step 7, Epoch 152, Batch 1, Loss: -28.952070\n",
            "Time step 7, Epoch 153, Batch 1, Loss: -28.952095\n",
            "Time step 7, Epoch 154, Batch 1, Loss: -28.952118\n",
            "Time step 7, Epoch 155, Batch 1, Loss: -28.952141\n",
            "Time step 7, Epoch 156, Batch 1, Loss: -28.952164\n",
            "Time step 7, Epoch 157, Batch 1, Loss: -28.952183\n",
            "Time step 7, Epoch 158, Batch 1, Loss: -28.952206\n",
            "Time step 7, Epoch 159, Batch 1, Loss: -28.952229\n",
            "Time step 7, Epoch 160, Batch 1, Loss: -28.952246\n",
            "Time step 7, Epoch 161, Batch 1, Loss: -28.952271\n",
            "Time step 7, Epoch 162, Batch 1, Loss: -28.952286\n",
            "Time step 7, Epoch 163, Batch 1, Loss: -28.952307\n",
            "Time step 7, Epoch 164, Batch 1, Loss: -28.952326\n",
            "Time step 7, Epoch 165, Batch 1, Loss: -28.952343\n",
            "Time step 7, Epoch 166, Batch 1, Loss: -28.952362\n",
            "Time step 7, Epoch 167, Batch 1, Loss: -28.952379\n",
            "Time step 7, Epoch 168, Batch 1, Loss: -28.952398\n",
            "Time step 7, Epoch 169, Batch 1, Loss: -28.952414\n",
            "Time step 7, Epoch 170, Batch 1, Loss: -28.952433\n",
            "Time step 7, Epoch 171, Batch 1, Loss: -28.952446\n",
            "Time step 7, Epoch 172, Batch 1, Loss: -28.952461\n",
            "Time step 7, Epoch 173, Batch 1, Loss: -28.952477\n",
            "Time step 7, Epoch 174, Batch 1, Loss: -28.952496\n",
            "Time step 7, Epoch 175, Batch 1, Loss: -28.952509\n",
            "Time step 7, Epoch 176, Batch 1, Loss: -28.952526\n",
            "Time step 7, Epoch 177, Batch 1, Loss: -28.952543\n",
            "Time step 7, Epoch 178, Batch 1, Loss: -28.952553\n",
            "Time step 7, Epoch 179, Batch 1, Loss: -28.952570\n",
            "Time step 7, Epoch 180, Batch 1, Loss: -28.952581\n",
            "Time step 7, Epoch 181, Batch 1, Loss: -28.952595\n",
            "Time step 7, Epoch 182, Batch 1, Loss: -28.952610\n",
            "Time step 7, Epoch 183, Batch 1, Loss: -28.952625\n",
            "Time step 7, Epoch 184, Batch 1, Loss: -28.952637\n",
            "Time step 7, Epoch 185, Batch 1, Loss: -28.952646\n",
            "Time step 7, Epoch 186, Batch 1, Loss: -28.952663\n",
            "Time step 7, Epoch 187, Batch 1, Loss: -28.952675\n",
            "Time step 7, Epoch 188, Batch 1, Loss: -28.952686\n",
            "Time step 7, Epoch 189, Batch 1, Loss: -28.952702\n",
            "Time step 7, Epoch 190, Batch 1, Loss: -28.952711\n",
            "Time step 7, Epoch 191, Batch 1, Loss: -28.952721\n",
            "Time step 7, Epoch 192, Batch 1, Loss: -28.952732\n",
            "Time step 7, Epoch 193, Batch 1, Loss: -28.952745\n",
            "Time step 7, Epoch 194, Batch 1, Loss: -28.952755\n",
            "Time step 7, Epoch 195, Batch 1, Loss: -28.952766\n",
            "Time step 7, Epoch 196, Batch 1, Loss: -28.952778\n",
            "Time step 7, Epoch 197, Batch 1, Loss: -28.952787\n",
            "Time step 7, Epoch 198, Batch 1, Loss: -28.952799\n",
            "Time step 7, Epoch 199, Batch 1, Loss: -28.952805\n",
            "Time step 7, Epoch 200, Batch 1, Loss: -28.952818\n",
            "Time step 7, Epoch 201, Batch 1, Loss: -28.952827\n",
            "Time step 7, Epoch 202, Batch 1, Loss: -28.952835\n",
            "Time step 7, Epoch 203, Batch 1, Loss: -28.952848\n",
            "Time step 7, Epoch 204, Batch 1, Loss: -28.952858\n",
            "Time step 7, Epoch 205, Batch 1, Loss: -28.952864\n",
            "Time step 7, Epoch 206, Batch 1, Loss: -28.952873\n",
            "Time step 7, Epoch 207, Batch 1, Loss: -28.952883\n",
            "Time step 7, Epoch 208, Batch 1, Loss: -28.952890\n",
            "Time step 7, Epoch 209, Batch 1, Loss: -28.952902\n",
            "Time step 7, Epoch 210, Batch 1, Loss: -28.952911\n",
            "Time step 7, Epoch 211, Batch 1, Loss: -28.952917\n",
            "Time step 7, Epoch 212, Batch 1, Loss: -28.952927\n",
            "Time step 7, Epoch 213, Batch 1, Loss: -28.952934\n",
            "Time step 7, Epoch 214, Batch 1, Loss: -28.952944\n",
            "Time step 7, Epoch 215, Batch 1, Loss: -28.952951\n",
            "Time step 7, Epoch 216, Batch 1, Loss: -28.952957\n",
            "Time step 7, Epoch 217, Batch 1, Loss: -28.952965\n",
            "Time step 7, Epoch 218, Batch 1, Loss: -28.952974\n",
            "Time step 7, Epoch 219, Batch 1, Loss: -28.952980\n",
            "Time step 7, Epoch 220, Batch 1, Loss: -28.952986\n",
            "Time step 7, Epoch 221, Batch 1, Loss: -28.952995\n",
            "Time step 7, Epoch 222, Batch 1, Loss: -28.953003\n",
            "Time step 7, Epoch 223, Batch 1, Loss: -28.953011\n",
            "Time step 7, Epoch 224, Batch 1, Loss: -28.953018\n",
            "Time step 7, Epoch 225, Batch 1, Loss: -28.953024\n",
            "Time step 7, Epoch 226, Batch 1, Loss: -28.953030\n",
            "Time step 7, Epoch 227, Batch 1, Loss: -28.953037\n",
            "Time step 7, Epoch 228, Batch 1, Loss: -28.953041\n",
            "Time step 7, Epoch 229, Batch 1, Loss: -28.953049\n",
            "Time step 7, Epoch 230, Batch 1, Loss: -28.953056\n",
            "Time step 7, Epoch 231, Batch 1, Loss: -28.953064\n",
            "Time step 7, Epoch 232, Batch 1, Loss: -28.953068\n",
            "Time step 7, Epoch 233, Batch 1, Loss: -28.953074\n",
            "Time step 7, Epoch 234, Batch 1, Loss: -28.953079\n",
            "Time step 7, Epoch 235, Batch 1, Loss: -28.953087\n",
            "Time step 7, Epoch 236, Batch 1, Loss: -28.953091\n",
            "Time step 7, Epoch 237, Batch 1, Loss: -28.953096\n",
            "Time step 7, Epoch 238, Batch 1, Loss: -28.953104\n",
            "Time step 7, Epoch 239, Batch 1, Loss: -28.953110\n",
            "Time step 7, Epoch 240, Batch 1, Loss: -28.953115\n",
            "Time step 7, Epoch 241, Batch 1, Loss: -28.953121\n",
            "Time step 7, Epoch 242, Batch 1, Loss: -28.953127\n",
            "Time step 7, Epoch 243, Batch 1, Loss: -28.953131\n",
            "Time step 7, Epoch 244, Batch 1, Loss: -28.953136\n",
            "Time step 7, Epoch 245, Batch 1, Loss: -28.953142\n",
            "Time step 7, Epoch 246, Batch 1, Loss: -28.953148\n",
            "Time step 7, Epoch 247, Batch 1, Loss: -28.953152\n",
            "Time step 7, Epoch 248, Batch 1, Loss: -28.953157\n",
            "Time step 7, Epoch 249, Batch 1, Loss: -28.953163\n",
            "Time step 7, Epoch 250, Batch 1, Loss: -28.953167\n",
            "Time step 7, Epoch 251, Batch 1, Loss: -28.953167\n",
            "Time step 7, Epoch 252, Batch 1, Loss: -28.953176\n",
            "Time step 7, Epoch 253, Batch 1, Loss: -28.953180\n",
            "Time step 7, Epoch 254, Batch 1, Loss: -28.953184\n",
            "Time step 7, Epoch 255, Batch 1, Loss: -28.953192\n",
            "Time step 7, Epoch 256, Batch 1, Loss: -28.953194\n",
            "Time step 7, Epoch 257, Batch 1, Loss: -28.953201\n",
            "Time step 7, Epoch 258, Batch 1, Loss: -28.953203\n",
            "Time step 7, Epoch 259, Batch 1, Loss: -28.953209\n",
            "Time step 7, Epoch 260, Batch 1, Loss: -28.953211\n",
            "Time step 7, Epoch 261, Batch 1, Loss: -28.953217\n",
            "Time step 7, Epoch 262, Batch 1, Loss: -28.953220\n",
            "Time step 7, Epoch 263, Batch 1, Loss: -28.953224\n",
            "Time step 7, Epoch 264, Batch 1, Loss: -28.953228\n",
            "Time step 7, Epoch 265, Batch 1, Loss: -28.953234\n",
            "Time step 7, Epoch 266, Batch 1, Loss: -28.953239\n",
            "Time step 7, Epoch 267, Batch 1, Loss: -28.953241\n",
            "Time step 7, Epoch 268, Batch 1, Loss: -28.953245\n",
            "Time step 7, Epoch 269, Batch 1, Loss: -28.953249\n",
            "Time step 7, Epoch 270, Batch 1, Loss: -28.953255\n",
            "Time step 7, Epoch 271, Batch 1, Loss: -28.953257\n",
            "Time step 7, Epoch 272, Batch 1, Loss: -28.953260\n",
            "Time step 7, Epoch 273, Batch 1, Loss: -28.953264\n",
            "Time step 7, Epoch 274, Batch 1, Loss: -28.953268\n",
            "Time step 7, Epoch 275, Batch 1, Loss: -28.953272\n",
            "Time step 7, Epoch 276, Batch 1, Loss: -28.953278\n",
            "Time step 7, Epoch 277, Batch 1, Loss: -28.953278\n",
            "Time step 7, Epoch 278, Batch 1, Loss: -28.953281\n",
            "Time step 7, Epoch 279, Batch 1, Loss: -28.953285\n",
            "Time step 7, Epoch 280, Batch 1, Loss: -28.953287\n",
            "Time step 7, Epoch 281, Batch 1, Loss: -28.953293\n",
            "Time step 7, Epoch 282, Batch 1, Loss: -28.953293\n",
            "Time step 7, Epoch 283, Batch 1, Loss: -28.953299\n",
            "Time step 7, Epoch 284, Batch 1, Loss: -28.953304\n",
            "Time step 7, Epoch 285, Batch 1, Loss: -28.953308\n",
            "Time step 7, Epoch 286, Batch 1, Loss: -28.953308\n",
            "Time step 7, Epoch 287, Batch 1, Loss: -28.953312\n",
            "Time step 7, Epoch 288, Batch 1, Loss: -28.953314\n",
            "Time step 7, Epoch 289, Batch 1, Loss: -28.953318\n",
            "Time step 7, Epoch 290, Batch 1, Loss: -28.953321\n",
            "Time step 7, Epoch 291, Batch 1, Loss: -28.953323\n",
            "Time step 7, Epoch 292, Batch 1, Loss: -28.953323\n",
            "Time step 7, Epoch 293, Batch 1, Loss: -28.953327\n",
            "Time step 7, Epoch 294, Batch 1, Loss: -28.953329\n",
            "Time step 7, Epoch 295, Batch 1, Loss: -28.953333\n",
            "Time step 7, Epoch 296, Batch 1, Loss: -28.953337\n",
            "Time step 7, Epoch 297, Batch 1, Loss: -28.953342\n",
            "Time step 7, Epoch 298, Batch 1, Loss: -28.953342\n",
            "Time step 7, Epoch 299, Batch 1, Loss: -28.953346\n",
            "Time step 7, Epoch 300, Batch 1, Loss: -28.953348\n",
            "Time step 7, Epoch 301, Batch 1, Loss: -28.953350\n",
            "Time step 7, Epoch 302, Batch 1, Loss: -28.953356\n",
            "Time step 7, Epoch 303, Batch 1, Loss: -28.953358\n",
            "Time step 7, Epoch 304, Batch 1, Loss: -28.953362\n",
            "Time step 7, Epoch 305, Batch 1, Loss: -28.953363\n",
            "Time step 7, Epoch 306, Batch 1, Loss: -28.953363\n",
            "Time step 7, Epoch 307, Batch 1, Loss: -28.953369\n",
            "Time step 7, Epoch 308, Batch 1, Loss: -28.953371\n",
            "Time step 7, Epoch 309, Batch 1, Loss: -28.953371\n",
            "Time step 7, Epoch 310, Batch 1, Loss: -28.953377\n",
            "Time step 7, Epoch 311, Batch 1, Loss: -28.953379\n",
            "Time step 7, Epoch 312, Batch 1, Loss: -28.953379\n",
            "Time step 7, Epoch 313, Batch 1, Loss: -28.953381\n",
            "Time step 7, Epoch 314, Batch 1, Loss: -28.953386\n",
            "Time step 7, Epoch 315, Batch 1, Loss: -28.953386\n",
            "Time step 7, Epoch 316, Batch 1, Loss: -28.953388\n",
            "Time step 7, Epoch 317, Batch 1, Loss: -28.953392\n",
            "Time step 7, Epoch 318, Batch 1, Loss: -28.953396\n",
            "Time step 7, Epoch 319, Batch 1, Loss: -28.953396\n",
            "Time step 7, Epoch 320, Batch 1, Loss: -28.953400\n",
            "Time step 7, Epoch 321, Batch 1, Loss: -28.953400\n",
            "Time step 7, Epoch 322, Batch 1, Loss: -28.953402\n",
            "Time step 7, Epoch 323, Batch 1, Loss: -28.953403\n",
            "Time step 7, Epoch 324, Batch 1, Loss: -28.953407\n",
            "Time step 7, Epoch 325, Batch 1, Loss: -28.953409\n",
            "Time step 7, Epoch 326, Batch 1, Loss: -28.953413\n",
            "Time step 7, Epoch 327, Batch 1, Loss: -28.953413\n",
            "Time step 7, Epoch 328, Batch 1, Loss: -28.953413\n",
            "Time step 7, Epoch 329, Batch 1, Loss: -28.953415\n",
            "Time step 7, Epoch 330, Batch 1, Loss: -28.953421\n",
            "Time step 7, Epoch 331, Batch 1, Loss: -28.953423\n",
            "Time step 7, Epoch 332, Batch 1, Loss: -28.953423\n",
            "Time step 7, Epoch 333, Batch 1, Loss: -28.953426\n",
            "Time step 7, Epoch 334, Batch 1, Loss: -28.953428\n",
            "Time step 7, Epoch 335, Batch 1, Loss: -28.953430\n",
            "Time step 7, Epoch 336, Batch 1, Loss: -28.953428\n",
            "Time step 7, Epoch 337, Batch 1, Loss: -28.953434\n",
            "Time step 7, Epoch 338, Batch 1, Loss: -28.953434\n",
            "Time step 7, Epoch 339, Batch 1, Loss: -28.953438\n",
            "Time step 7, Epoch 340, Batch 1, Loss: -28.953438\n",
            "Time step 7, Epoch 341, Batch 1, Loss: -28.953440\n",
            "Time step 7, Epoch 342, Batch 1, Loss: -28.953442\n",
            "Time step 7, Epoch 343, Batch 1, Loss: -28.953442\n",
            "Time step 7, Epoch 344, Batch 1, Loss: -28.953444\n",
            "Time step 7, Epoch 345, Batch 1, Loss: -28.953445\n",
            "Time step 7, Epoch 346, Batch 1, Loss: -28.953449\n",
            "Time step 7, Epoch 347, Batch 1, Loss: -28.953451\n",
            "Time step 7, Epoch 348, Batch 1, Loss: -28.953453\n",
            "Time step 7, Epoch 349, Batch 1, Loss: -28.953451\n",
            "Time step 7, Epoch 350, Batch 1, Loss: -28.953457\n",
            "Time step 7, Epoch 351, Batch 1, Loss: -28.953455\n",
            "Time step 7, Epoch 352, Batch 1, Loss: -28.953459\n",
            "Time step 7, Epoch 353, Batch 1, Loss: -28.953463\n",
            "Time step 7, Epoch 354, Batch 1, Loss: -28.953461\n",
            "Time step 7, Epoch 355, Batch 1, Loss: -28.953463\n",
            "Time step 7, Epoch 356, Batch 1, Loss: -28.953465\n",
            "Time step 7, Epoch 357, Batch 1, Loss: -28.953466\n",
            "Time step 7, Epoch 358, Batch 1, Loss: -28.953466\n",
            "Time step 7, Epoch 359, Batch 1, Loss: -28.953466\n",
            "Time step 7, Epoch 360, Batch 1, Loss: -28.953472\n",
            "Time step 7, Epoch 361, Batch 1, Loss: -28.953476\n",
            "Time step 7, Epoch 362, Batch 1, Loss: -28.953474\n",
            "Time step 7, Epoch 363, Batch 1, Loss: -28.953476\n",
            "Time step 7, Epoch 364, Batch 1, Loss: -28.953476\n",
            "Time step 7, Epoch 365, Batch 1, Loss: -28.953478\n",
            "Time step 7, Epoch 366, Batch 1, Loss: -28.953480\n",
            "Time step 7, Epoch 367, Batch 1, Loss: -28.953482\n",
            "Time step 7, Epoch 368, Batch 1, Loss: -28.953480\n",
            "Time step 7, Epoch 369, Batch 1, Loss: -28.953484\n",
            "Time step 7, Epoch 370, Batch 1, Loss: -28.953485\n",
            "Time step 7, Epoch 371, Batch 1, Loss: -28.953485\n",
            "Time step 7, Epoch 372, Batch 1, Loss: -28.953489\n",
            "Time step 7, Epoch 373, Batch 1, Loss: -28.953489\n",
            "Time step 7, Epoch 374, Batch 1, Loss: -28.953491\n",
            "Time step 7, Epoch 375, Batch 1, Loss: -28.953495\n",
            "Time step 7, Epoch 376, Batch 1, Loss: -28.953493\n",
            "Time step 7, Epoch 377, Batch 1, Loss: -28.953495\n",
            "Time step 7, Epoch 378, Batch 1, Loss: -28.953499\n",
            "Time step 7, Epoch 379, Batch 1, Loss: -28.953499\n",
            "Time step 7, Epoch 380, Batch 1, Loss: -28.953499\n",
            "Time step 7, Epoch 381, Batch 1, Loss: -28.953499\n",
            "Time step 7, Epoch 382, Batch 1, Loss: -28.953501\n",
            "Time step 7, Epoch 383, Batch 1, Loss: -28.953501\n",
            "Time step 7, Epoch 384, Batch 1, Loss: -28.953505\n",
            "Time step 7, Epoch 385, Batch 1, Loss: -28.953505\n",
            "Time step 7, Epoch 386, Batch 1, Loss: -28.953505\n",
            "Time step 7, Epoch 387, Batch 1, Loss: -28.953506\n",
            "Time step 7, Epoch 388, Batch 1, Loss: -28.953510\n",
            "Time step 7, Epoch 389, Batch 1, Loss: -28.953510\n",
            "Time step 7, Epoch 390, Batch 1, Loss: -28.953508\n",
            "Time step 7, Epoch 391, Batch 1, Loss: -28.953510\n",
            "Time step 7, Epoch 392, Batch 1, Loss: -28.953514\n",
            "Time step 7, Epoch 393, Batch 1, Loss: -28.953512\n",
            "Time step 7, Epoch 394, Batch 1, Loss: -28.953516\n",
            "Time step 7, Epoch 395, Batch 1, Loss: -28.953518\n",
            "Time step 7, Epoch 396, Batch 1, Loss: -28.953520\n",
            "Time step 7, Epoch 397, Batch 1, Loss: -28.953518\n",
            "Time step 7, Epoch 398, Batch 1, Loss: -28.953520\n",
            "Time step 7, Epoch 399, Batch 1, Loss: -28.953524\n",
            "Time step 7, Epoch 400, Batch 1, Loss: -28.953524\n",
            "Time step 7, Epoch 401, Batch 1, Loss: -28.953524\n",
            "Time step 7, Epoch 402, Batch 1, Loss: -28.953524\n",
            "Time step 7, Epoch 403, Batch 1, Loss: -28.953526\n",
            "Time step 7, Epoch 404, Batch 1, Loss: -28.953527\n",
            "Time step 7, Epoch 405, Batch 1, Loss: -28.953527\n",
            "Time step 7, Epoch 406, Batch 1, Loss: -28.953527\n",
            "Time step 7, Epoch 407, Batch 1, Loss: -28.953529\n",
            "Time step 7, Epoch 408, Batch 1, Loss: -28.953531\n",
            "Time step 7, Epoch 409, Batch 1, Loss: -28.953531\n",
            "Time step 7, Epoch 410, Batch 1, Loss: -28.953531\n",
            "Time step 7, Epoch 411, Batch 1, Loss: -28.953533\n",
            "Time step 7, Epoch 412, Batch 1, Loss: -28.953535\n",
            "Time step 7, Epoch 413, Batch 1, Loss: -28.953537\n",
            "Time step 7, Epoch 414, Batch 1, Loss: -28.953535\n",
            "Time step 7, Epoch 415, Batch 1, Loss: -28.953537\n",
            "Time step 7, Epoch 416, Batch 1, Loss: -28.953541\n",
            "Time step 7, Epoch 417, Batch 1, Loss: -28.953539\n",
            "Time step 7, Epoch 418, Batch 1, Loss: -28.953541\n",
            "Time step 7, Epoch 419, Batch 1, Loss: -28.953541\n",
            "Time step 7, Epoch 420, Batch 1, Loss: -28.953543\n",
            "Time step 7, Epoch 421, Batch 1, Loss: -28.953543\n",
            "Time step 7, Epoch 422, Batch 1, Loss: -28.953545\n",
            "Time step 7, Epoch 423, Batch 1, Loss: -28.953545\n",
            "Time step 7, Epoch 424, Batch 1, Loss: -28.953547\n",
            "Time step 7, Epoch 425, Batch 1, Loss: -28.953548\n",
            "Time step 7, Epoch 426, Batch 1, Loss: -28.953547\n",
            "Time step 7, Epoch 427, Batch 1, Loss: -28.953548\n",
            "Time step 7, Epoch 428, Batch 1, Loss: -28.953548\n",
            "Time step 7, Epoch 429, Batch 1, Loss: -28.953550\n",
            "Time step 7, Epoch 430, Batch 1, Loss: -28.953554\n",
            "Time step 7, Epoch 431, Batch 1, Loss: -28.953552\n",
            "Time step 7, Epoch 432, Batch 1, Loss: -28.953554\n",
            "Time step 7, Epoch 433, Batch 1, Loss: -28.953554\n",
            "Time step 7, Epoch 434, Batch 1, Loss: -28.953554\n",
            "Time step 7, Epoch 435, Batch 1, Loss: -28.953556\n",
            "Time step 7, Epoch 436, Batch 1, Loss: -28.953556\n",
            "Time step 7, Epoch 437, Batch 1, Loss: -28.953556\n",
            "Time step 7, Epoch 438, Batch 1, Loss: -28.953556\n",
            "Time step 7, Epoch 439, Batch 1, Loss: -28.953556\n",
            "Time step 7, Epoch 440, Batch 1, Loss: -28.953558\n",
            "Time step 7, Epoch 441, Batch 1, Loss: -28.953562\n",
            "Time step 7, Epoch 442, Batch 1, Loss: -28.953562\n",
            "Time step 7, Epoch 443, Batch 1, Loss: -28.953564\n",
            "Time step 7, Epoch 444, Batch 1, Loss: -28.953564\n",
            "Time step 7, Epoch 445, Batch 1, Loss: -28.953562\n",
            "Time step 7, Epoch 446, Batch 1, Loss: -28.953564\n",
            "Time step 7, Epoch 447, Batch 1, Loss: -28.953568\n",
            "Time step 7, Epoch 448, Batch 1, Loss: -28.953566\n",
            "Time step 7, Epoch 449, Batch 1, Loss: -28.953564\n",
            "Time step 7, Epoch 450, Batch 1, Loss: -28.953568\n",
            "Time step 7, Epoch 451, Batch 1, Loss: -28.953564\n",
            "Time step 7, Epoch 452, Batch 1, Loss: -28.953566\n",
            "Time step 7, Epoch 453, Batch 1, Loss: -28.953569\n",
            "Time step 7, Epoch 454, Batch 1, Loss: -28.953568\n",
            "Time step 7, Epoch 455, Batch 1, Loss: -28.953571\n",
            "Time step 7, Epoch 456, Batch 1, Loss: -28.953571\n",
            "Time step 7, Epoch 457, Batch 1, Loss: -28.953571\n",
            "Time step 7, Epoch 458, Batch 1, Loss: -28.953571\n",
            "Time step 7, Epoch 459, Batch 1, Loss: -28.953573\n",
            "Time step 7, Epoch 460, Batch 1, Loss: -28.953575\n",
            "Time step 7, Epoch 461, Batch 1, Loss: -28.953575\n",
            "Time step 7, Epoch 462, Batch 1, Loss: -28.953575\n",
            "Time step 7, Epoch 463, Batch 1, Loss: -28.953573\n",
            "Time step 7, Epoch 464, Batch 1, Loss: -28.953575\n",
            "Time step 7, Epoch 465, Batch 1, Loss: -28.953579\n",
            "Time step 7, Epoch 466, Batch 1, Loss: -28.953577\n",
            "Time step 7, Epoch 467, Batch 1, Loss: -28.953579\n",
            "Time step 7, Epoch 468, Batch 1, Loss: -28.953579\n",
            "Time step 7, Epoch 469, Batch 1, Loss: -28.953579\n",
            "Time step 7, Epoch 470, Batch 1, Loss: -28.953579\n",
            "Time step 7, Epoch 471, Batch 1, Loss: -28.953583\n",
            "Time step 7, Epoch 472, Batch 1, Loss: -28.953583\n",
            "Time step 7, Epoch 473, Batch 1, Loss: -28.953583\n",
            "Time step 7, Epoch 474, Batch 1, Loss: -28.953583\n",
            "Time step 7, Epoch 475, Batch 1, Loss: -28.953585\n",
            "Time step 7, Epoch 476, Batch 1, Loss: -28.953585\n",
            "Time step 7, Epoch 477, Batch 1, Loss: -28.953587\n",
            "Time step 7, Epoch 478, Batch 1, Loss: -28.953587\n",
            "Time step 7, Epoch 479, Batch 1, Loss: -28.953585\n",
            "Time step 7, Epoch 480, Batch 1, Loss: -28.953587\n",
            "Time step 7, Epoch 481, Batch 1, Loss: -28.953587\n",
            "Time step 7, Epoch 482, Batch 1, Loss: -28.953587\n",
            "Time step 7, Epoch 483, Batch 1, Loss: -28.953588\n",
            "Time step 7, Epoch 484, Batch 1, Loss: -28.953588\n",
            "Time step 7, Epoch 485, Batch 1, Loss: -28.953590\n",
            "Time step 7, Epoch 486, Batch 1, Loss: -28.953590\n",
            "Time step 7, Epoch 487, Batch 1, Loss: -28.953590\n",
            "Time step 7, Epoch 488, Batch 1, Loss: -28.953590\n",
            "Time step 7, Epoch 489, Batch 1, Loss: -28.953590\n",
            "Time step 7, Epoch 490, Batch 1, Loss: -28.953590\n",
            "Time step 7, Epoch 491, Batch 1, Loss: -28.953594\n",
            "Time step 7, Epoch 492, Batch 1, Loss: -28.953594\n",
            "Time step 7, Epoch 493, Batch 1, Loss: -28.953594\n",
            "Time step 7, Epoch 494, Batch 1, Loss: -28.953594\n",
            "Time step 7, Epoch 495, Batch 1, Loss: -28.953594\n",
            "Time step 7, Epoch 496, Batch 1, Loss: -28.953594\n",
            "Time step 7, Epoch 497, Batch 1, Loss: -28.953598\n",
            "Time step 7, Epoch 498, Batch 1, Loss: -28.953598\n",
            "Time step 7, Epoch 499, Batch 1, Loss: -28.953598\n",
            "Time step 7, Epoch 500, Batch 1, Loss: -28.953596\n",
            "Training model for time step 6...\n",
            "Time step 6, Epoch 1, Batch 1, Loss: -26.985279\n",
            "Time step 6, Epoch 2, Batch 1, Loss: -27.035694\n",
            "Time step 6, Epoch 3, Batch 1, Loss: -27.087193\n",
            "Time step 6, Epoch 4, Batch 1, Loss: -27.139811\n",
            "Time step 6, Epoch 5, Batch 1, Loss: -27.193632\n",
            "Time step 6, Epoch 6, Batch 1, Loss: -27.248684\n",
            "Time step 6, Epoch 7, Batch 1, Loss: -27.304783\n",
            "Time step 6, Epoch 8, Batch 1, Loss: -27.362164\n",
            "Time step 6, Epoch 9, Batch 1, Loss: -27.420799\n",
            "Time step 6, Epoch 10, Batch 1, Loss: -27.480530\n",
            "Time step 6, Epoch 11, Batch 1, Loss: -27.541288\n",
            "Time step 6, Epoch 12, Batch 1, Loss: -27.603128\n",
            "Time step 6, Epoch 13, Batch 1, Loss: -27.665922\n",
            "Time step 6, Epoch 14, Batch 1, Loss: -27.729338\n",
            "Time step 6, Epoch 15, Batch 1, Loss: -27.793299\n",
            "Time step 6, Epoch 16, Batch 1, Loss: -27.857492\n",
            "Time step 6, Epoch 17, Batch 1, Loss: -27.921856\n",
            "Time step 6, Epoch 18, Batch 1, Loss: -27.986042\n",
            "Time step 6, Epoch 19, Batch 1, Loss: -28.049772\n",
            "Time step 6, Epoch 20, Batch 1, Loss: -28.112724\n",
            "Time step 6, Epoch 21, Batch 1, Loss: -28.174536\n",
            "Time step 6, Epoch 22, Batch 1, Loss: -28.234779\n",
            "Time step 6, Epoch 23, Batch 1, Loss: -28.293180\n",
            "Time step 6, Epoch 24, Batch 1, Loss: -28.349499\n",
            "Time step 6, Epoch 25, Batch 1, Loss: -28.403435\n",
            "Time step 6, Epoch 26, Batch 1, Loss: -28.454702\n",
            "Time step 6, Epoch 27, Batch 1, Loss: -28.503048\n",
            "Time step 6, Epoch 28, Batch 1, Loss: -28.548409\n",
            "Time step 6, Epoch 29, Batch 1, Loss: -28.590584\n",
            "Time step 6, Epoch 30, Batch 1, Loss: -28.629585\n",
            "Time step 6, Epoch 31, Batch 1, Loss: -28.665361\n",
            "Time step 6, Epoch 32, Batch 1, Loss: -28.697927\n",
            "Time step 6, Epoch 33, Batch 1, Loss: -28.727331\n",
            "Time step 6, Epoch 34, Batch 1, Loss: -28.753700\n",
            "Time step 6, Epoch 35, Batch 1, Loss: -28.777222\n",
            "Time step 6, Epoch 36, Batch 1, Loss: -28.798086\n",
            "Time step 6, Epoch 37, Batch 1, Loss: -28.816498\n",
            "Time step 6, Epoch 38, Batch 1, Loss: -28.832676\n",
            "Time step 6, Epoch 39, Batch 1, Loss: -28.846846\n",
            "Time step 6, Epoch 40, Batch 1, Loss: -28.859222\n",
            "Time step 6, Epoch 41, Batch 1, Loss: -28.870014\n",
            "Time step 6, Epoch 42, Batch 1, Loss: -28.879419\n",
            "Time step 6, Epoch 43, Batch 1, Loss: -28.887600\n",
            "Time step 6, Epoch 44, Batch 1, Loss: -28.894722\n",
            "Time step 6, Epoch 45, Batch 1, Loss: -28.900917\n",
            "Time step 6, Epoch 46, Batch 1, Loss: -28.906305\n",
            "Time step 6, Epoch 47, Batch 1, Loss: -28.911005\n",
            "Time step 6, Epoch 48, Batch 1, Loss: -28.915110\n",
            "Time step 6, Epoch 49, Batch 1, Loss: -28.918695\n",
            "Time step 6, Epoch 50, Batch 1, Loss: -28.921837\n",
            "Time step 6, Epoch 51, Batch 1, Loss: -28.924595\n",
            "Time step 6, Epoch 52, Batch 1, Loss: -28.927021\n",
            "Time step 6, Epoch 53, Batch 1, Loss: -28.929157\n",
            "Time step 6, Epoch 54, Batch 1, Loss: -28.931047\n",
            "Time step 6, Epoch 55, Batch 1, Loss: -28.932724\n",
            "Time step 6, Epoch 56, Batch 1, Loss: -28.934214\n",
            "Time step 6, Epoch 57, Batch 1, Loss: -28.935539\n",
            "Time step 6, Epoch 58, Batch 1, Loss: -28.936724\n",
            "Time step 6, Epoch 59, Batch 1, Loss: -28.937784\n",
            "Time step 6, Epoch 60, Batch 1, Loss: -28.938740\n",
            "Time step 6, Epoch 61, Batch 1, Loss: -28.939596\n",
            "Time step 6, Epoch 62, Batch 1, Loss: -28.940372\n",
            "Time step 6, Epoch 63, Batch 1, Loss: -28.941072\n",
            "Time step 6, Epoch 64, Batch 1, Loss: -28.941710\n",
            "Time step 6, Epoch 65, Batch 1, Loss: -28.942287\n",
            "Time step 6, Epoch 66, Batch 1, Loss: -28.942818\n",
            "Time step 6, Epoch 67, Batch 1, Loss: -28.943300\n",
            "Time step 6, Epoch 68, Batch 1, Loss: -28.943745\n",
            "Time step 6, Epoch 69, Batch 1, Loss: -28.944153\n",
            "Time step 6, Epoch 70, Batch 1, Loss: -28.944527\n",
            "Time step 6, Epoch 71, Batch 1, Loss: -28.944870\n",
            "Time step 6, Epoch 72, Batch 1, Loss: -28.945194\n",
            "Time step 6, Epoch 73, Batch 1, Loss: -28.945490\n",
            "Time step 6, Epoch 74, Batch 1, Loss: -28.945766\n",
            "Time step 6, Epoch 75, Batch 1, Loss: -28.946024\n",
            "Time step 6, Epoch 76, Batch 1, Loss: -28.946264\n",
            "Time step 6, Epoch 77, Batch 1, Loss: -28.946493\n",
            "Time step 6, Epoch 78, Batch 1, Loss: -28.946697\n",
            "Time step 6, Epoch 79, Batch 1, Loss: -28.946899\n",
            "Time step 6, Epoch 80, Batch 1, Loss: -28.947084\n",
            "Time step 6, Epoch 81, Batch 1, Loss: -28.947262\n",
            "Time step 6, Epoch 82, Batch 1, Loss: -28.947428\n",
            "Time step 6, Epoch 83, Batch 1, Loss: -28.947586\n",
            "Time step 6, Epoch 84, Batch 1, Loss: -28.947733\n",
            "Time step 6, Epoch 85, Batch 1, Loss: -28.947874\n",
            "Time step 6, Epoch 86, Batch 1, Loss: -28.948009\n",
            "Time step 6, Epoch 87, Batch 1, Loss: -28.948137\n",
            "Time step 6, Epoch 88, Batch 1, Loss: -28.948257\n",
            "Time step 6, Epoch 89, Batch 1, Loss: -28.948378\n",
            "Time step 6, Epoch 90, Batch 1, Loss: -28.948490\n",
            "Time step 6, Epoch 91, Batch 1, Loss: -28.948595\n",
            "Time step 6, Epoch 92, Batch 1, Loss: -28.948696\n",
            "Time step 6, Epoch 93, Batch 1, Loss: -28.948797\n",
            "Time step 6, Epoch 94, Batch 1, Loss: -28.948891\n",
            "Time step 6, Epoch 95, Batch 1, Loss: -28.948982\n",
            "Time step 6, Epoch 96, Batch 1, Loss: -28.949070\n",
            "Time step 6, Epoch 97, Batch 1, Loss: -28.949154\n",
            "Time step 6, Epoch 98, Batch 1, Loss: -28.949238\n",
            "Time step 6, Epoch 99, Batch 1, Loss: -28.949316\n",
            "Time step 6, Epoch 100, Batch 1, Loss: -28.949392\n",
            "Time step 6, Epoch 101, Batch 1, Loss: -28.949465\n",
            "Time step 6, Epoch 102, Batch 1, Loss: -28.949535\n",
            "Time step 6, Epoch 103, Batch 1, Loss: -28.949608\n",
            "Time step 6, Epoch 104, Batch 1, Loss: -28.949675\n",
            "Time step 6, Epoch 105, Batch 1, Loss: -28.949739\n",
            "Time step 6, Epoch 106, Batch 1, Loss: -28.949806\n",
            "Time step 6, Epoch 107, Batch 1, Loss: -28.949867\n",
            "Time step 6, Epoch 108, Batch 1, Loss: -28.949928\n",
            "Time step 6, Epoch 109, Batch 1, Loss: -28.949986\n",
            "Time step 6, Epoch 110, Batch 1, Loss: -28.950041\n",
            "Time step 6, Epoch 111, Batch 1, Loss: -28.950098\n",
            "Time step 6, Epoch 112, Batch 1, Loss: -28.950153\n",
            "Time step 6, Epoch 113, Batch 1, Loss: -28.950203\n",
            "Time step 6, Epoch 114, Batch 1, Loss: -28.950256\n",
            "Time step 6, Epoch 115, Batch 1, Loss: -28.950306\n",
            "Time step 6, Epoch 116, Batch 1, Loss: -28.950356\n",
            "Time step 6, Epoch 117, Batch 1, Loss: -28.950403\n",
            "Time step 6, Epoch 118, Batch 1, Loss: -28.950451\n",
            "Time step 6, Epoch 119, Batch 1, Loss: -28.950495\n",
            "Time step 6, Epoch 120, Batch 1, Loss: -28.950541\n",
            "Time step 6, Epoch 121, Batch 1, Loss: -28.950584\n",
            "Time step 6, Epoch 122, Batch 1, Loss: -28.950626\n",
            "Time step 6, Epoch 123, Batch 1, Loss: -28.950668\n",
            "Time step 6, Epoch 124, Batch 1, Loss: -28.950710\n",
            "Time step 6, Epoch 125, Batch 1, Loss: -28.950752\n",
            "Time step 6, Epoch 126, Batch 1, Loss: -28.950787\n",
            "Time step 6, Epoch 127, Batch 1, Loss: -28.950827\n",
            "Time step 6, Epoch 128, Batch 1, Loss: -28.950867\n",
            "Time step 6, Epoch 129, Batch 1, Loss: -28.950903\n",
            "Time step 6, Epoch 130, Batch 1, Loss: -28.950939\n",
            "Time step 6, Epoch 131, Batch 1, Loss: -28.950975\n",
            "Time step 6, Epoch 132, Batch 1, Loss: -28.951008\n",
            "Time step 6, Epoch 133, Batch 1, Loss: -28.951044\n",
            "Time step 6, Epoch 134, Batch 1, Loss: -28.951077\n",
            "Time step 6, Epoch 135, Batch 1, Loss: -28.951109\n",
            "Time step 6, Epoch 136, Batch 1, Loss: -28.951145\n",
            "Time step 6, Epoch 137, Batch 1, Loss: -28.951174\n",
            "Time step 6, Epoch 138, Batch 1, Loss: -28.951204\n",
            "Time step 6, Epoch 139, Batch 1, Loss: -28.951235\n",
            "Time step 6, Epoch 140, Batch 1, Loss: -28.951263\n",
            "Time step 6, Epoch 141, Batch 1, Loss: -28.951294\n",
            "Time step 6, Epoch 142, Batch 1, Loss: -28.951324\n",
            "Time step 6, Epoch 143, Batch 1, Loss: -28.951353\n",
            "Time step 6, Epoch 144, Batch 1, Loss: -28.951378\n",
            "Time step 6, Epoch 145, Batch 1, Loss: -28.951406\n",
            "Time step 6, Epoch 146, Batch 1, Loss: -28.951433\n",
            "Time step 6, Epoch 147, Batch 1, Loss: -28.951462\n",
            "Time step 6, Epoch 148, Batch 1, Loss: -28.951485\n",
            "Time step 6, Epoch 149, Batch 1, Loss: -28.951511\n",
            "Time step 6, Epoch 150, Batch 1, Loss: -28.951538\n",
            "Time step 6, Epoch 151, Batch 1, Loss: -28.951561\n",
            "Time step 6, Epoch 152, Batch 1, Loss: -28.951586\n",
            "Time step 6, Epoch 153, Batch 1, Loss: -28.951609\n",
            "Time step 6, Epoch 154, Batch 1, Loss: -28.951633\n",
            "Time step 6, Epoch 155, Batch 1, Loss: -28.951656\n",
            "Time step 6, Epoch 156, Batch 1, Loss: -28.951679\n",
            "Time step 6, Epoch 157, Batch 1, Loss: -28.951700\n",
            "Time step 6, Epoch 158, Batch 1, Loss: -28.951723\n",
            "Time step 6, Epoch 159, Batch 1, Loss: -28.951744\n",
            "Time step 6, Epoch 160, Batch 1, Loss: -28.951767\n",
            "Time step 6, Epoch 161, Batch 1, Loss: -28.951786\n",
            "Time step 6, Epoch 162, Batch 1, Loss: -28.951807\n",
            "Time step 6, Epoch 163, Batch 1, Loss: -28.951828\n",
            "Time step 6, Epoch 164, Batch 1, Loss: -28.951847\n",
            "Time step 6, Epoch 165, Batch 1, Loss: -28.951866\n",
            "Time step 6, Epoch 166, Batch 1, Loss: -28.951885\n",
            "Time step 6, Epoch 167, Batch 1, Loss: -28.951908\n",
            "Time step 6, Epoch 168, Batch 1, Loss: -28.951923\n",
            "Time step 6, Epoch 169, Batch 1, Loss: -28.951942\n",
            "Time step 6, Epoch 170, Batch 1, Loss: -28.951960\n",
            "Time step 6, Epoch 171, Batch 1, Loss: -28.951981\n",
            "Time step 6, Epoch 172, Batch 1, Loss: -28.951996\n",
            "Time step 6, Epoch 173, Batch 1, Loss: -28.952017\n",
            "Time step 6, Epoch 174, Batch 1, Loss: -28.952030\n",
            "Time step 6, Epoch 175, Batch 1, Loss: -28.952047\n",
            "Time step 6, Epoch 176, Batch 1, Loss: -28.952065\n",
            "Time step 6, Epoch 177, Batch 1, Loss: -28.952080\n",
            "Time step 6, Epoch 178, Batch 1, Loss: -28.952097\n",
            "Time step 6, Epoch 179, Batch 1, Loss: -28.952112\n",
            "Time step 6, Epoch 180, Batch 1, Loss: -28.952129\n",
            "Time step 6, Epoch 181, Batch 1, Loss: -28.952143\n",
            "Time step 6, Epoch 182, Batch 1, Loss: -28.952160\n",
            "Time step 6, Epoch 183, Batch 1, Loss: -28.952173\n",
            "Time step 6, Epoch 184, Batch 1, Loss: -28.952190\n",
            "Time step 6, Epoch 185, Batch 1, Loss: -28.952202\n",
            "Time step 6, Epoch 186, Batch 1, Loss: -28.952217\n",
            "Time step 6, Epoch 187, Batch 1, Loss: -28.952230\n",
            "Time step 6, Epoch 188, Batch 1, Loss: -28.952244\n",
            "Time step 6, Epoch 189, Batch 1, Loss: -28.952259\n",
            "Time step 6, Epoch 190, Batch 1, Loss: -28.952272\n",
            "Time step 6, Epoch 191, Batch 1, Loss: -28.952286\n",
            "Time step 6, Epoch 192, Batch 1, Loss: -28.952297\n",
            "Time step 6, Epoch 193, Batch 1, Loss: -28.952312\n",
            "Time step 6, Epoch 194, Batch 1, Loss: -28.952324\n",
            "Time step 6, Epoch 195, Batch 1, Loss: -28.952337\n",
            "Time step 6, Epoch 196, Batch 1, Loss: -28.952349\n",
            "Time step 6, Epoch 197, Batch 1, Loss: -28.952362\n",
            "Time step 6, Epoch 198, Batch 1, Loss: -28.952374\n",
            "Time step 6, Epoch 199, Batch 1, Loss: -28.952387\n",
            "Time step 6, Epoch 200, Batch 1, Loss: -28.952400\n",
            "Time step 6, Epoch 201, Batch 1, Loss: -28.952412\n",
            "Time step 6, Epoch 202, Batch 1, Loss: -28.952421\n",
            "Time step 6, Epoch 203, Batch 1, Loss: -28.952435\n",
            "Time step 6, Epoch 204, Batch 1, Loss: -28.952446\n",
            "Time step 6, Epoch 205, Batch 1, Loss: -28.952454\n",
            "Time step 6, Epoch 206, Batch 1, Loss: -28.952463\n",
            "Time step 6, Epoch 207, Batch 1, Loss: -28.952475\n",
            "Time step 6, Epoch 208, Batch 1, Loss: -28.952486\n",
            "Time step 6, Epoch 209, Batch 1, Loss: -28.952497\n",
            "Time step 6, Epoch 210, Batch 1, Loss: -28.952509\n",
            "Time step 6, Epoch 211, Batch 1, Loss: -28.952518\n",
            "Time step 6, Epoch 212, Batch 1, Loss: -28.952528\n",
            "Time step 6, Epoch 213, Batch 1, Loss: -28.952539\n",
            "Time step 6, Epoch 214, Batch 1, Loss: -28.952549\n",
            "Time step 6, Epoch 215, Batch 1, Loss: -28.952559\n",
            "Time step 6, Epoch 216, Batch 1, Loss: -28.952566\n",
            "Time step 6, Epoch 217, Batch 1, Loss: -28.952578\n",
            "Time step 6, Epoch 218, Batch 1, Loss: -28.952587\n",
            "Time step 6, Epoch 219, Batch 1, Loss: -28.952595\n",
            "Time step 6, Epoch 220, Batch 1, Loss: -28.952606\n",
            "Time step 6, Epoch 221, Batch 1, Loss: -28.952612\n",
            "Time step 6, Epoch 222, Batch 1, Loss: -28.952623\n",
            "Time step 6, Epoch 223, Batch 1, Loss: -28.952633\n",
            "Time step 6, Epoch 224, Batch 1, Loss: -28.952637\n",
            "Time step 6, Epoch 225, Batch 1, Loss: -28.952648\n",
            "Time step 6, Epoch 226, Batch 1, Loss: -28.952656\n",
            "Time step 6, Epoch 227, Batch 1, Loss: -28.952663\n",
            "Time step 6, Epoch 228, Batch 1, Loss: -28.952671\n",
            "Time step 6, Epoch 229, Batch 1, Loss: -28.952682\n",
            "Time step 6, Epoch 230, Batch 1, Loss: -28.952690\n",
            "Time step 6, Epoch 231, Batch 1, Loss: -28.952696\n",
            "Time step 6, Epoch 232, Batch 1, Loss: -28.952705\n",
            "Time step 6, Epoch 233, Batch 1, Loss: -28.952715\n",
            "Time step 6, Epoch 234, Batch 1, Loss: -28.952723\n",
            "Time step 6, Epoch 235, Batch 1, Loss: -28.952732\n",
            "Time step 6, Epoch 236, Batch 1, Loss: -28.952738\n",
            "Time step 6, Epoch 237, Batch 1, Loss: -28.952744\n",
            "Time step 6, Epoch 238, Batch 1, Loss: -28.952751\n",
            "Time step 6, Epoch 239, Batch 1, Loss: -28.952759\n",
            "Time step 6, Epoch 240, Batch 1, Loss: -28.952766\n",
            "Time step 6, Epoch 241, Batch 1, Loss: -28.952774\n",
            "Time step 6, Epoch 242, Batch 1, Loss: -28.952780\n",
            "Time step 6, Epoch 243, Batch 1, Loss: -28.952789\n",
            "Time step 6, Epoch 244, Batch 1, Loss: -28.952795\n",
            "Time step 6, Epoch 245, Batch 1, Loss: -28.952801\n",
            "Time step 6, Epoch 246, Batch 1, Loss: -28.952808\n",
            "Time step 6, Epoch 247, Batch 1, Loss: -28.952814\n",
            "Time step 6, Epoch 248, Batch 1, Loss: -28.952822\n",
            "Time step 6, Epoch 249, Batch 1, Loss: -28.952827\n",
            "Time step 6, Epoch 250, Batch 1, Loss: -28.952835\n",
            "Time step 6, Epoch 251, Batch 1, Loss: -28.952839\n",
            "Time step 6, Epoch 252, Batch 1, Loss: -28.952848\n",
            "Time step 6, Epoch 253, Batch 1, Loss: -28.952854\n",
            "Time step 6, Epoch 254, Batch 1, Loss: -28.952858\n",
            "Time step 6, Epoch 255, Batch 1, Loss: -28.952866\n",
            "Time step 6, Epoch 256, Batch 1, Loss: -28.952869\n",
            "Time step 6, Epoch 257, Batch 1, Loss: -28.952877\n",
            "Time step 6, Epoch 258, Batch 1, Loss: -28.952883\n",
            "Time step 6, Epoch 259, Batch 1, Loss: -28.952890\n",
            "Time step 6, Epoch 260, Batch 1, Loss: -28.952896\n",
            "Time step 6, Epoch 261, Batch 1, Loss: -28.952902\n",
            "Time step 6, Epoch 262, Batch 1, Loss: -28.952908\n",
            "Time step 6, Epoch 263, Batch 1, Loss: -28.952911\n",
            "Time step 6, Epoch 264, Batch 1, Loss: -28.952917\n",
            "Time step 6, Epoch 265, Batch 1, Loss: -28.952923\n",
            "Time step 6, Epoch 266, Batch 1, Loss: -28.952929\n",
            "Time step 6, Epoch 267, Batch 1, Loss: -28.952934\n",
            "Time step 6, Epoch 268, Batch 1, Loss: -28.952940\n",
            "Time step 6, Epoch 269, Batch 1, Loss: -28.952944\n",
            "Time step 6, Epoch 270, Batch 1, Loss: -28.952948\n",
            "Time step 6, Epoch 271, Batch 1, Loss: -28.952957\n",
            "Time step 6, Epoch 272, Batch 1, Loss: -28.952961\n",
            "Time step 6, Epoch 273, Batch 1, Loss: -28.952965\n",
            "Time step 6, Epoch 274, Batch 1, Loss: -28.952971\n",
            "Time step 6, Epoch 275, Batch 1, Loss: -28.952976\n",
            "Time step 6, Epoch 276, Batch 1, Loss: -28.952982\n",
            "Time step 6, Epoch 277, Batch 1, Loss: -28.952984\n",
            "Time step 6, Epoch 278, Batch 1, Loss: -28.952990\n",
            "Time step 6, Epoch 279, Batch 1, Loss: -28.952993\n",
            "Time step 6, Epoch 280, Batch 1, Loss: -28.953001\n",
            "Time step 6, Epoch 281, Batch 1, Loss: -28.953007\n",
            "Time step 6, Epoch 282, Batch 1, Loss: -28.953009\n",
            "Time step 6, Epoch 283, Batch 1, Loss: -28.953012\n",
            "Time step 6, Epoch 284, Batch 1, Loss: -28.953018\n",
            "Time step 6, Epoch 285, Batch 1, Loss: -28.953018\n",
            "Time step 6, Epoch 286, Batch 1, Loss: -28.953026\n",
            "Time step 6, Epoch 287, Batch 1, Loss: -28.953032\n",
            "Time step 6, Epoch 288, Batch 1, Loss: -28.953033\n",
            "Time step 6, Epoch 289, Batch 1, Loss: -28.953037\n",
            "Time step 6, Epoch 290, Batch 1, Loss: -28.953043\n",
            "Time step 6, Epoch 291, Batch 1, Loss: -28.953049\n",
            "Time step 6, Epoch 292, Batch 1, Loss: -28.953054\n",
            "Time step 6, Epoch 293, Batch 1, Loss: -28.953056\n",
            "Time step 6, Epoch 294, Batch 1, Loss: -28.953062\n",
            "Time step 6, Epoch 295, Batch 1, Loss: -28.953064\n",
            "Time step 6, Epoch 296, Batch 1, Loss: -28.953066\n",
            "Time step 6, Epoch 297, Batch 1, Loss: -28.953072\n",
            "Time step 6, Epoch 298, Batch 1, Loss: -28.953075\n",
            "Time step 6, Epoch 299, Batch 1, Loss: -28.953083\n",
            "Time step 6, Epoch 300, Batch 1, Loss: -28.953089\n",
            "Time step 6, Epoch 301, Batch 1, Loss: -28.953087\n",
            "Time step 6, Epoch 302, Batch 1, Loss: -28.953091\n",
            "Time step 6, Epoch 303, Batch 1, Loss: -28.953098\n",
            "Time step 6, Epoch 304, Batch 1, Loss: -28.953100\n",
            "Time step 6, Epoch 305, Batch 1, Loss: -28.953104\n",
            "Time step 6, Epoch 306, Batch 1, Loss: -28.953106\n",
            "Time step 6, Epoch 307, Batch 1, Loss: -28.953112\n",
            "Time step 6, Epoch 308, Batch 1, Loss: -28.953115\n",
            "Time step 6, Epoch 309, Batch 1, Loss: -28.953117\n",
            "Time step 6, Epoch 310, Batch 1, Loss: -28.953121\n",
            "Time step 6, Epoch 311, Batch 1, Loss: -28.953127\n",
            "Time step 6, Epoch 312, Batch 1, Loss: -28.953129\n",
            "Time step 6, Epoch 313, Batch 1, Loss: -28.953133\n",
            "Time step 6, Epoch 314, Batch 1, Loss: -28.953135\n",
            "Time step 6, Epoch 315, Batch 1, Loss: -28.953140\n",
            "Time step 6, Epoch 316, Batch 1, Loss: -28.953142\n",
            "Time step 6, Epoch 317, Batch 1, Loss: -28.953146\n",
            "Time step 6, Epoch 318, Batch 1, Loss: -28.953150\n",
            "Time step 6, Epoch 319, Batch 1, Loss: -28.953152\n",
            "Time step 6, Epoch 320, Batch 1, Loss: -28.953154\n",
            "Time step 6, Epoch 321, Batch 1, Loss: -28.953157\n",
            "Time step 6, Epoch 322, Batch 1, Loss: -28.953161\n",
            "Time step 6, Epoch 323, Batch 1, Loss: -28.953163\n",
            "Time step 6, Epoch 324, Batch 1, Loss: -28.953169\n",
            "Time step 6, Epoch 325, Batch 1, Loss: -28.953171\n",
            "Time step 6, Epoch 326, Batch 1, Loss: -28.953173\n",
            "Time step 6, Epoch 327, Batch 1, Loss: -28.953175\n",
            "Time step 6, Epoch 328, Batch 1, Loss: -28.953180\n",
            "Time step 6, Epoch 329, Batch 1, Loss: -28.953184\n",
            "Time step 6, Epoch 330, Batch 1, Loss: -28.953186\n",
            "Time step 6, Epoch 331, Batch 1, Loss: -28.953186\n",
            "Time step 6, Epoch 332, Batch 1, Loss: -28.953192\n",
            "Time step 6, Epoch 333, Batch 1, Loss: -28.953194\n",
            "Time step 6, Epoch 334, Batch 1, Loss: -28.953197\n",
            "Time step 6, Epoch 335, Batch 1, Loss: -28.953201\n",
            "Time step 6, Epoch 336, Batch 1, Loss: -28.953203\n",
            "Time step 6, Epoch 337, Batch 1, Loss: -28.953205\n",
            "Time step 6, Epoch 338, Batch 1, Loss: -28.953209\n",
            "Time step 6, Epoch 339, Batch 1, Loss: -28.953211\n",
            "Time step 6, Epoch 340, Batch 1, Loss: -28.953215\n",
            "Time step 6, Epoch 341, Batch 1, Loss: -28.953218\n",
            "Time step 6, Epoch 342, Batch 1, Loss: -28.953220\n",
            "Time step 6, Epoch 343, Batch 1, Loss: -28.953224\n",
            "Time step 6, Epoch 344, Batch 1, Loss: -28.953226\n",
            "Time step 6, Epoch 345, Batch 1, Loss: -28.953228\n",
            "Time step 6, Epoch 346, Batch 1, Loss: -28.953230\n",
            "Time step 6, Epoch 347, Batch 1, Loss: -28.953232\n",
            "Time step 6, Epoch 348, Batch 1, Loss: -28.953238\n",
            "Time step 6, Epoch 349, Batch 1, Loss: -28.953239\n",
            "Time step 6, Epoch 350, Batch 1, Loss: -28.953241\n",
            "Time step 6, Epoch 351, Batch 1, Loss: -28.953241\n",
            "Time step 6, Epoch 352, Batch 1, Loss: -28.953245\n",
            "Time step 6, Epoch 353, Batch 1, Loss: -28.953249\n",
            "Time step 6, Epoch 354, Batch 1, Loss: -28.953251\n",
            "Time step 6, Epoch 355, Batch 1, Loss: -28.953255\n",
            "Time step 6, Epoch 356, Batch 1, Loss: -28.953257\n",
            "Time step 6, Epoch 357, Batch 1, Loss: -28.953259\n",
            "Time step 6, Epoch 358, Batch 1, Loss: -28.953262\n",
            "Time step 6, Epoch 359, Batch 1, Loss: -28.953262\n",
            "Time step 6, Epoch 360, Batch 1, Loss: -28.953264\n",
            "Time step 6, Epoch 361, Batch 1, Loss: -28.953270\n",
            "Time step 6, Epoch 362, Batch 1, Loss: -28.953270\n",
            "Time step 6, Epoch 363, Batch 1, Loss: -28.953272\n",
            "Time step 6, Epoch 364, Batch 1, Loss: -28.953272\n",
            "Time step 6, Epoch 365, Batch 1, Loss: -28.953276\n",
            "Time step 6, Epoch 366, Batch 1, Loss: -28.953279\n",
            "Time step 6, Epoch 367, Batch 1, Loss: -28.953279\n",
            "Time step 6, Epoch 368, Batch 1, Loss: -28.953285\n",
            "Time step 6, Epoch 369, Batch 1, Loss: -28.953283\n",
            "Time step 6, Epoch 370, Batch 1, Loss: -28.953287\n",
            "Time step 6, Epoch 371, Batch 1, Loss: -28.953289\n",
            "Time step 6, Epoch 372, Batch 1, Loss: -28.953293\n",
            "Time step 6, Epoch 373, Batch 1, Loss: -28.953293\n",
            "Time step 6, Epoch 374, Batch 1, Loss: -28.953297\n",
            "Time step 6, Epoch 375, Batch 1, Loss: -28.953299\n",
            "Time step 6, Epoch 376, Batch 1, Loss: -28.953304\n",
            "Time step 6, Epoch 377, Batch 1, Loss: -28.953304\n",
            "Time step 6, Epoch 378, Batch 1, Loss: -28.953302\n",
            "Time step 6, Epoch 379, Batch 1, Loss: -28.953308\n",
            "Time step 6, Epoch 380, Batch 1, Loss: -28.953308\n",
            "Time step 6, Epoch 381, Batch 1, Loss: -28.953308\n",
            "Time step 6, Epoch 382, Batch 1, Loss: -28.953312\n",
            "Time step 6, Epoch 383, Batch 1, Loss: -28.953316\n",
            "Time step 6, Epoch 384, Batch 1, Loss: -28.953316\n",
            "Time step 6, Epoch 385, Batch 1, Loss: -28.953316\n",
            "Time step 6, Epoch 386, Batch 1, Loss: -28.953320\n",
            "Time step 6, Epoch 387, Batch 1, Loss: -28.953323\n",
            "Time step 6, Epoch 388, Batch 1, Loss: -28.953323\n",
            "Time step 6, Epoch 389, Batch 1, Loss: -28.953327\n",
            "Time step 6, Epoch 390, Batch 1, Loss: -28.953325\n",
            "Time step 6, Epoch 391, Batch 1, Loss: -28.953331\n",
            "Time step 6, Epoch 392, Batch 1, Loss: -28.953331\n",
            "Time step 6, Epoch 393, Batch 1, Loss: -28.953331\n",
            "Time step 6, Epoch 394, Batch 1, Loss: -28.953333\n",
            "Time step 6, Epoch 395, Batch 1, Loss: -28.953339\n",
            "Time step 6, Epoch 396, Batch 1, Loss: -28.953339\n",
            "Time step 6, Epoch 397, Batch 1, Loss: -28.953341\n",
            "Time step 6, Epoch 398, Batch 1, Loss: -28.953342\n",
            "Time step 6, Epoch 399, Batch 1, Loss: -28.953342\n",
            "Time step 6, Epoch 400, Batch 1, Loss: -28.953346\n",
            "Time step 6, Epoch 401, Batch 1, Loss: -28.953348\n",
            "Time step 6, Epoch 402, Batch 1, Loss: -28.953348\n",
            "Time step 6, Epoch 403, Batch 1, Loss: -28.953352\n",
            "Time step 6, Epoch 404, Batch 1, Loss: -28.953354\n",
            "Time step 6, Epoch 405, Batch 1, Loss: -28.953352\n",
            "Time step 6, Epoch 406, Batch 1, Loss: -28.953358\n",
            "Time step 6, Epoch 407, Batch 1, Loss: -28.953358\n",
            "Time step 6, Epoch 408, Batch 1, Loss: -28.953360\n",
            "Time step 6, Epoch 409, Batch 1, Loss: -28.953360\n",
            "Time step 6, Epoch 410, Batch 1, Loss: -28.953363\n",
            "Time step 6, Epoch 411, Batch 1, Loss: -28.953365\n",
            "Time step 6, Epoch 412, Batch 1, Loss: -28.953365\n",
            "Time step 6, Epoch 413, Batch 1, Loss: -28.953369\n",
            "Time step 6, Epoch 414, Batch 1, Loss: -28.953369\n",
            "Time step 6, Epoch 415, Batch 1, Loss: -28.953371\n",
            "Time step 6, Epoch 416, Batch 1, Loss: -28.953373\n",
            "Time step 6, Epoch 417, Batch 1, Loss: -28.953375\n",
            "Time step 6, Epoch 418, Batch 1, Loss: -28.953377\n",
            "Time step 6, Epoch 419, Batch 1, Loss: -28.953377\n",
            "Time step 6, Epoch 420, Batch 1, Loss: -28.953379\n",
            "Time step 6, Epoch 421, Batch 1, Loss: -28.953379\n",
            "Time step 6, Epoch 422, Batch 1, Loss: -28.953381\n",
            "Time step 6, Epoch 423, Batch 1, Loss: -28.953384\n",
            "Time step 6, Epoch 424, Batch 1, Loss: -28.953384\n",
            "Time step 6, Epoch 425, Batch 1, Loss: -28.953384\n",
            "Time step 6, Epoch 426, Batch 1, Loss: -28.953386\n",
            "Time step 6, Epoch 427, Batch 1, Loss: -28.953388\n",
            "Time step 6, Epoch 428, Batch 1, Loss: -28.953390\n",
            "Time step 6, Epoch 429, Batch 1, Loss: -28.953390\n",
            "Time step 6, Epoch 430, Batch 1, Loss: -28.953392\n",
            "Time step 6, Epoch 431, Batch 1, Loss: -28.953392\n",
            "Time step 6, Epoch 432, Batch 1, Loss: -28.953396\n",
            "Time step 6, Epoch 433, Batch 1, Loss: -28.953400\n",
            "Time step 6, Epoch 434, Batch 1, Loss: -28.953398\n",
            "Time step 6, Epoch 435, Batch 1, Loss: -28.953400\n",
            "Time step 6, Epoch 436, Batch 1, Loss: -28.953402\n",
            "Time step 6, Epoch 437, Batch 1, Loss: -28.953403\n",
            "Time step 6, Epoch 438, Batch 1, Loss: -28.953403\n",
            "Time step 6, Epoch 439, Batch 1, Loss: -28.953403\n",
            "Time step 6, Epoch 440, Batch 1, Loss: -28.953407\n",
            "Time step 6, Epoch 441, Batch 1, Loss: -28.953407\n",
            "Time step 6, Epoch 442, Batch 1, Loss: -28.953405\n",
            "Time step 6, Epoch 443, Batch 1, Loss: -28.953409\n",
            "Time step 6, Epoch 444, Batch 1, Loss: -28.953411\n",
            "Time step 6, Epoch 445, Batch 1, Loss: -28.953411\n",
            "Time step 6, Epoch 446, Batch 1, Loss: -28.953413\n",
            "Time step 6, Epoch 447, Batch 1, Loss: -28.953415\n",
            "Time step 6, Epoch 448, Batch 1, Loss: -28.953419\n",
            "Time step 6, Epoch 449, Batch 1, Loss: -28.953417\n",
            "Time step 6, Epoch 450, Batch 1, Loss: -28.953419\n",
            "Time step 6, Epoch 451, Batch 1, Loss: -28.953419\n",
            "Time step 6, Epoch 452, Batch 1, Loss: -28.953423\n",
            "Time step 6, Epoch 453, Batch 1, Loss: -28.953423\n",
            "Time step 6, Epoch 454, Batch 1, Loss: -28.953424\n",
            "Time step 6, Epoch 455, Batch 1, Loss: -28.953426\n",
            "Time step 6, Epoch 456, Batch 1, Loss: -28.953426\n",
            "Time step 6, Epoch 457, Batch 1, Loss: -28.953426\n",
            "Time step 6, Epoch 458, Batch 1, Loss: -28.953426\n",
            "Time step 6, Epoch 459, Batch 1, Loss: -28.953430\n",
            "Time step 6, Epoch 460, Batch 1, Loss: -28.953430\n",
            "Time step 6, Epoch 461, Batch 1, Loss: -28.953434\n",
            "Time step 6, Epoch 462, Batch 1, Loss: -28.953434\n",
            "Time step 6, Epoch 463, Batch 1, Loss: -28.953434\n",
            "Time step 6, Epoch 464, Batch 1, Loss: -28.953436\n",
            "Time step 6, Epoch 465, Batch 1, Loss: -28.953438\n",
            "Time step 6, Epoch 466, Batch 1, Loss: -28.953438\n",
            "Time step 6, Epoch 467, Batch 1, Loss: -28.953440\n",
            "Time step 6, Epoch 468, Batch 1, Loss: -28.953440\n",
            "Time step 6, Epoch 469, Batch 1, Loss: -28.953440\n",
            "Time step 6, Epoch 470, Batch 1, Loss: -28.953442\n",
            "Time step 6, Epoch 471, Batch 1, Loss: -28.953444\n",
            "Time step 6, Epoch 472, Batch 1, Loss: -28.953444\n",
            "Time step 6, Epoch 473, Batch 1, Loss: -28.953445\n",
            "Time step 6, Epoch 474, Batch 1, Loss: -28.953445\n",
            "Time step 6, Epoch 475, Batch 1, Loss: -28.953445\n",
            "Time step 6, Epoch 476, Batch 1, Loss: -28.953447\n",
            "Time step 6, Epoch 477, Batch 1, Loss: -28.953449\n",
            "Time step 6, Epoch 478, Batch 1, Loss: -28.953451\n",
            "Time step 6, Epoch 479, Batch 1, Loss: -28.953449\n",
            "Time step 6, Epoch 480, Batch 1, Loss: -28.953453\n",
            "Time step 6, Epoch 481, Batch 1, Loss: -28.953453\n",
            "Time step 6, Epoch 482, Batch 1, Loss: -28.953455\n",
            "Time step 6, Epoch 483, Batch 1, Loss: -28.953457\n",
            "Time step 6, Epoch 484, Batch 1, Loss: -28.953457\n",
            "Time step 6, Epoch 485, Batch 1, Loss: -28.953457\n",
            "Time step 6, Epoch 486, Batch 1, Loss: -28.953457\n",
            "Time step 6, Epoch 487, Batch 1, Loss: -28.953461\n",
            "Time step 6, Epoch 488, Batch 1, Loss: -28.953459\n",
            "Time step 6, Epoch 489, Batch 1, Loss: -28.953463\n",
            "Time step 6, Epoch 490, Batch 1, Loss: -28.953461\n",
            "Time step 6, Epoch 491, Batch 1, Loss: -28.953461\n",
            "Time step 6, Epoch 492, Batch 1, Loss: -28.953466\n",
            "Time step 6, Epoch 493, Batch 1, Loss: -28.953465\n",
            "Time step 6, Epoch 494, Batch 1, Loss: -28.953468\n",
            "Time step 6, Epoch 495, Batch 1, Loss: -28.953466\n",
            "Time step 6, Epoch 496, Batch 1, Loss: -28.953468\n",
            "Time step 6, Epoch 497, Batch 1, Loss: -28.953470\n",
            "Time step 6, Epoch 498, Batch 1, Loss: -28.953470\n",
            "Time step 6, Epoch 499, Batch 1, Loss: -28.953472\n",
            "Time step 6, Epoch 500, Batch 1, Loss: -28.953470\n",
            "Training model for time step 5...\n",
            "Time step 5, Epoch 1, Batch 1, Loss: -25.900410\n",
            "Time step 5, Epoch 2, Batch 1, Loss: -25.966990\n",
            "Time step 5, Epoch 3, Batch 1, Loss: -26.033369\n",
            "Time step 5, Epoch 4, Batch 1, Loss: -26.099852\n",
            "Time step 5, Epoch 5, Batch 1, Loss: -26.166759\n",
            "Time step 5, Epoch 6, Batch 1, Loss: -26.233940\n",
            "Time step 5, Epoch 7, Batch 1, Loss: -26.301392\n",
            "Time step 5, Epoch 8, Batch 1, Loss: -26.369005\n",
            "Time step 5, Epoch 9, Batch 1, Loss: -26.437462\n",
            "Time step 5, Epoch 10, Batch 1, Loss: -26.506821\n",
            "Time step 5, Epoch 11, Batch 1, Loss: -26.577339\n",
            "Time step 5, Epoch 12, Batch 1, Loss: -26.649349\n",
            "Time step 5, Epoch 13, Batch 1, Loss: -26.722828\n",
            "Time step 5, Epoch 14, Batch 1, Loss: -26.797806\n",
            "Time step 5, Epoch 15, Batch 1, Loss: -26.874712\n",
            "Time step 5, Epoch 16, Batch 1, Loss: -26.953571\n",
            "Time step 5, Epoch 17, Batch 1, Loss: -27.034470\n",
            "Time step 5, Epoch 18, Batch 1, Loss: -27.117306\n",
            "Time step 5, Epoch 19, Batch 1, Loss: -27.201883\n",
            "Time step 5, Epoch 20, Batch 1, Loss: -27.287996\n",
            "Time step 5, Epoch 21, Batch 1, Loss: -27.375189\n",
            "Time step 5, Epoch 22, Batch 1, Loss: -27.463362\n",
            "Time step 5, Epoch 23, Batch 1, Loss: -27.552364\n",
            "Time step 5, Epoch 24, Batch 1, Loss: -27.641674\n",
            "Time step 5, Epoch 25, Batch 1, Loss: -27.730843\n",
            "Time step 5, Epoch 26, Batch 1, Loss: -27.819344\n",
            "Time step 5, Epoch 27, Batch 1, Loss: -27.906731\n",
            "Time step 5, Epoch 28, Batch 1, Loss: -27.992376\n",
            "Time step 5, Epoch 29, Batch 1, Loss: -28.075632\n",
            "Time step 5, Epoch 30, Batch 1, Loss: -28.155941\n",
            "Time step 5, Epoch 31, Batch 1, Loss: -28.232740\n",
            "Time step 5, Epoch 32, Batch 1, Loss: -28.305508\n",
            "Time step 5, Epoch 33, Batch 1, Loss: -28.373829\n",
            "Time step 5, Epoch 34, Batch 1, Loss: -28.437296\n",
            "Time step 5, Epoch 35, Batch 1, Loss: -28.495676\n",
            "Time step 5, Epoch 36, Batch 1, Loss: -28.548880\n",
            "Time step 5, Epoch 37, Batch 1, Loss: -28.596947\n",
            "Time step 5, Epoch 38, Batch 1, Loss: -28.640009\n",
            "Time step 5, Epoch 39, Batch 1, Loss: -28.678347\n",
            "Time step 5, Epoch 40, Batch 1, Loss: -28.712267\n",
            "Time step 5, Epoch 41, Batch 1, Loss: -28.742125\n",
            "Time step 5, Epoch 42, Batch 1, Loss: -28.768270\n",
            "Time step 5, Epoch 43, Batch 1, Loss: -28.791082\n",
            "Time step 5, Epoch 44, Batch 1, Loss: -28.810925\n",
            "Time step 5, Epoch 45, Batch 1, Loss: -28.828135\n",
            "Time step 5, Epoch 46, Batch 1, Loss: -28.843033\n",
            "Time step 5, Epoch 47, Batch 1, Loss: -28.855923\n",
            "Time step 5, Epoch 48, Batch 1, Loss: -28.867064\n",
            "Time step 5, Epoch 49, Batch 1, Loss: -28.876692\n",
            "Time step 5, Epoch 50, Batch 1, Loss: -28.885021\n",
            "Time step 5, Epoch 51, Batch 1, Loss: -28.892233\n",
            "Time step 5, Epoch 52, Batch 1, Loss: -28.898487\n",
            "Time step 5, Epoch 53, Batch 1, Loss: -28.903923\n",
            "Time step 5, Epoch 54, Batch 1, Loss: -28.908649\n",
            "Time step 5, Epoch 55, Batch 1, Loss: -28.912777\n",
            "Time step 5, Epoch 56, Batch 1, Loss: -28.916384\n",
            "Time step 5, Epoch 57, Batch 1, Loss: -28.919544\n",
            "Time step 5, Epoch 58, Batch 1, Loss: -28.922327\n",
            "Time step 5, Epoch 59, Batch 1, Loss: -28.924778\n",
            "Time step 5, Epoch 60, Batch 1, Loss: -28.926943\n",
            "Time step 5, Epoch 61, Batch 1, Loss: -28.928862\n",
            "Time step 5, Epoch 62, Batch 1, Loss: -28.930573\n",
            "Time step 5, Epoch 63, Batch 1, Loss: -28.932095\n",
            "Time step 5, Epoch 64, Batch 1, Loss: -28.933456\n",
            "Time step 5, Epoch 65, Batch 1, Loss: -28.934675\n",
            "Time step 5, Epoch 66, Batch 1, Loss: -28.935776\n",
            "Time step 5, Epoch 67, Batch 1, Loss: -28.936771\n",
            "Time step 5, Epoch 68, Batch 1, Loss: -28.937668\n",
            "Time step 5, Epoch 69, Batch 1, Loss: -28.938480\n",
            "Time step 5, Epoch 70, Batch 1, Loss: -28.939222\n",
            "Time step 5, Epoch 71, Batch 1, Loss: -28.939899\n",
            "Time step 5, Epoch 72, Batch 1, Loss: -28.940516\n",
            "Time step 5, Epoch 73, Batch 1, Loss: -28.941084\n",
            "Time step 5, Epoch 74, Batch 1, Loss: -28.941607\n",
            "Time step 5, Epoch 75, Batch 1, Loss: -28.942085\n",
            "Time step 5, Epoch 76, Batch 1, Loss: -28.942530\n",
            "Time step 5, Epoch 77, Batch 1, Loss: -28.942944\n",
            "Time step 5, Epoch 78, Batch 1, Loss: -28.943325\n",
            "Time step 5, Epoch 79, Batch 1, Loss: -28.943680\n",
            "Time step 5, Epoch 80, Batch 1, Loss: -28.944012\n",
            "Time step 5, Epoch 81, Batch 1, Loss: -28.944324\n",
            "Time step 5, Epoch 82, Batch 1, Loss: -28.944614\n",
            "Time step 5, Epoch 83, Batch 1, Loss: -28.944889\n",
            "Time step 5, Epoch 84, Batch 1, Loss: -28.945145\n",
            "Time step 5, Epoch 85, Batch 1, Loss: -28.945387\n",
            "Time step 5, Epoch 86, Batch 1, Loss: -28.945618\n",
            "Time step 5, Epoch 87, Batch 1, Loss: -28.945833\n",
            "Time step 5, Epoch 88, Batch 1, Loss: -28.946039\n",
            "Time step 5, Epoch 89, Batch 1, Loss: -28.946234\n",
            "Time step 5, Epoch 90, Batch 1, Loss: -28.946419\n",
            "Time step 5, Epoch 91, Batch 1, Loss: -28.946598\n",
            "Time step 5, Epoch 92, Batch 1, Loss: -28.946766\n",
            "Time step 5, Epoch 93, Batch 1, Loss: -28.946926\n",
            "Time step 5, Epoch 94, Batch 1, Loss: -28.947083\n",
            "Time step 5, Epoch 95, Batch 1, Loss: -28.947231\n",
            "Time step 5, Epoch 96, Batch 1, Loss: -28.947374\n",
            "Time step 5, Epoch 97, Batch 1, Loss: -28.947510\n",
            "Time step 5, Epoch 98, Batch 1, Loss: -28.947639\n",
            "Time step 5, Epoch 99, Batch 1, Loss: -28.947765\n",
            "Time step 5, Epoch 100, Batch 1, Loss: -28.947889\n",
            "Time step 5, Epoch 101, Batch 1, Loss: -28.948008\n",
            "Time step 5, Epoch 102, Batch 1, Loss: -28.948124\n",
            "Time step 5, Epoch 103, Batch 1, Loss: -28.948231\n",
            "Time step 5, Epoch 104, Batch 1, Loss: -28.948336\n",
            "Time step 5, Epoch 105, Batch 1, Loss: -28.948439\n",
            "Time step 5, Epoch 106, Batch 1, Loss: -28.948538\n",
            "Time step 5, Epoch 107, Batch 1, Loss: -28.948633\n",
            "Time step 5, Epoch 108, Batch 1, Loss: -28.948729\n",
            "Time step 5, Epoch 109, Batch 1, Loss: -28.948816\n",
            "Time step 5, Epoch 110, Batch 1, Loss: -28.948906\n",
            "Time step 5, Epoch 111, Batch 1, Loss: -28.948988\n",
            "Time step 5, Epoch 112, Batch 1, Loss: -28.949072\n",
            "Time step 5, Epoch 113, Batch 1, Loss: -28.949152\n",
            "Time step 5, Epoch 114, Batch 1, Loss: -28.949230\n",
            "Time step 5, Epoch 115, Batch 1, Loss: -28.949306\n",
            "Time step 5, Epoch 116, Batch 1, Loss: -28.949379\n",
            "Time step 5, Epoch 117, Batch 1, Loss: -28.949451\n",
            "Time step 5, Epoch 118, Batch 1, Loss: -28.949520\n",
            "Time step 5, Epoch 119, Batch 1, Loss: -28.949593\n",
            "Time step 5, Epoch 120, Batch 1, Loss: -28.949657\n",
            "Time step 5, Epoch 121, Batch 1, Loss: -28.949722\n",
            "Time step 5, Epoch 122, Batch 1, Loss: -28.949785\n",
            "Time step 5, Epoch 123, Batch 1, Loss: -28.949846\n",
            "Time step 5, Epoch 124, Batch 1, Loss: -28.949909\n",
            "Time step 5, Epoch 125, Batch 1, Loss: -28.949968\n",
            "Time step 5, Epoch 126, Batch 1, Loss: -28.950026\n",
            "Time step 5, Epoch 127, Batch 1, Loss: -28.950083\n",
            "Time step 5, Epoch 128, Batch 1, Loss: -28.950138\n",
            "Time step 5, Epoch 129, Batch 1, Loss: -28.950191\n",
            "Time step 5, Epoch 130, Batch 1, Loss: -28.950245\n",
            "Time step 5, Epoch 131, Batch 1, Loss: -28.950294\n",
            "Time step 5, Epoch 132, Batch 1, Loss: -28.950348\n",
            "Time step 5, Epoch 133, Batch 1, Loss: -28.950396\n",
            "Time step 5, Epoch 134, Batch 1, Loss: -28.950443\n",
            "Time step 5, Epoch 135, Batch 1, Loss: -28.950493\n",
            "Time step 5, Epoch 136, Batch 1, Loss: -28.950541\n",
            "Time step 5, Epoch 137, Batch 1, Loss: -28.950583\n",
            "Time step 5, Epoch 138, Batch 1, Loss: -28.950630\n",
            "Time step 5, Epoch 139, Batch 1, Loss: -28.950670\n",
            "Time step 5, Epoch 140, Batch 1, Loss: -28.950712\n",
            "Time step 5, Epoch 141, Batch 1, Loss: -28.950756\n",
            "Time step 5, Epoch 142, Batch 1, Loss: -28.950798\n",
            "Time step 5, Epoch 143, Batch 1, Loss: -28.950838\n",
            "Time step 5, Epoch 144, Batch 1, Loss: -28.950878\n",
            "Time step 5, Epoch 145, Batch 1, Loss: -28.950916\n",
            "Time step 5, Epoch 146, Batch 1, Loss: -28.950953\n",
            "Time step 5, Epoch 147, Batch 1, Loss: -28.950993\n",
            "Time step 5, Epoch 148, Batch 1, Loss: -28.951027\n",
            "Time step 5, Epoch 149, Batch 1, Loss: -28.951063\n",
            "Time step 5, Epoch 150, Batch 1, Loss: -28.951099\n",
            "Time step 5, Epoch 151, Batch 1, Loss: -28.951134\n",
            "Time step 5, Epoch 152, Batch 1, Loss: -28.951168\n",
            "Time step 5, Epoch 153, Batch 1, Loss: -28.951202\n",
            "Time step 5, Epoch 154, Batch 1, Loss: -28.951233\n",
            "Time step 5, Epoch 155, Batch 1, Loss: -28.951267\n",
            "Time step 5, Epoch 156, Batch 1, Loss: -28.951300\n",
            "Time step 5, Epoch 157, Batch 1, Loss: -28.951328\n",
            "Time step 5, Epoch 158, Batch 1, Loss: -28.951359\n",
            "Time step 5, Epoch 159, Batch 1, Loss: -28.951389\n",
            "Time step 5, Epoch 160, Batch 1, Loss: -28.951418\n",
            "Time step 5, Epoch 161, Batch 1, Loss: -28.951448\n",
            "Time step 5, Epoch 162, Batch 1, Loss: -28.951475\n",
            "Time step 5, Epoch 163, Batch 1, Loss: -28.951502\n",
            "Time step 5, Epoch 164, Batch 1, Loss: -28.951530\n",
            "Time step 5, Epoch 165, Batch 1, Loss: -28.951557\n",
            "Time step 5, Epoch 166, Batch 1, Loss: -28.951582\n",
            "Time step 5, Epoch 167, Batch 1, Loss: -28.951609\n",
            "Time step 5, Epoch 168, Batch 1, Loss: -28.951635\n",
            "Time step 5, Epoch 169, Batch 1, Loss: -28.951660\n",
            "Time step 5, Epoch 170, Batch 1, Loss: -28.951685\n",
            "Time step 5, Epoch 171, Batch 1, Loss: -28.951708\n",
            "Time step 5, Epoch 172, Batch 1, Loss: -28.951733\n",
            "Time step 5, Epoch 173, Batch 1, Loss: -28.951756\n",
            "Time step 5, Epoch 174, Batch 1, Loss: -28.951780\n",
            "Time step 5, Epoch 175, Batch 1, Loss: -28.951801\n",
            "Time step 5, Epoch 176, Batch 1, Loss: -28.951824\n",
            "Time step 5, Epoch 177, Batch 1, Loss: -28.951847\n",
            "Time step 5, Epoch 178, Batch 1, Loss: -28.951866\n",
            "Time step 5, Epoch 179, Batch 1, Loss: -28.951889\n",
            "Time step 5, Epoch 180, Batch 1, Loss: -28.951912\n",
            "Time step 5, Epoch 181, Batch 1, Loss: -28.951931\n",
            "Time step 5, Epoch 182, Batch 1, Loss: -28.951954\n",
            "Time step 5, Epoch 183, Batch 1, Loss: -28.951973\n",
            "Time step 5, Epoch 184, Batch 1, Loss: -28.951992\n",
            "Time step 5, Epoch 185, Batch 1, Loss: -28.952011\n",
            "Time step 5, Epoch 186, Batch 1, Loss: -28.952032\n",
            "Time step 5, Epoch 187, Batch 1, Loss: -28.952049\n",
            "Time step 5, Epoch 188, Batch 1, Loss: -28.952068\n",
            "Time step 5, Epoch 189, Batch 1, Loss: -28.952085\n",
            "Time step 5, Epoch 190, Batch 1, Loss: -28.952103\n",
            "Time step 5, Epoch 191, Batch 1, Loss: -28.952124\n",
            "Time step 5, Epoch 192, Batch 1, Loss: -28.952139\n",
            "Time step 5, Epoch 193, Batch 1, Loss: -28.952158\n",
            "Time step 5, Epoch 194, Batch 1, Loss: -28.952175\n",
            "Time step 5, Epoch 195, Batch 1, Loss: -28.952190\n",
            "Time step 5, Epoch 196, Batch 1, Loss: -28.952206\n",
            "Time step 5, Epoch 197, Batch 1, Loss: -28.952225\n",
            "Time step 5, Epoch 198, Batch 1, Loss: -28.952240\n",
            "Time step 5, Epoch 199, Batch 1, Loss: -28.952257\n",
            "Time step 5, Epoch 200, Batch 1, Loss: -28.952274\n",
            "Time step 5, Epoch 201, Batch 1, Loss: -28.952290\n",
            "Time step 5, Epoch 202, Batch 1, Loss: -28.952303\n",
            "Time step 5, Epoch 203, Batch 1, Loss: -28.952320\n",
            "Time step 5, Epoch 204, Batch 1, Loss: -28.952333\n",
            "Time step 5, Epoch 205, Batch 1, Loss: -28.952349\n",
            "Time step 5, Epoch 206, Batch 1, Loss: -28.952362\n",
            "Time step 5, Epoch 207, Batch 1, Loss: -28.952375\n",
            "Time step 5, Epoch 208, Batch 1, Loss: -28.952389\n",
            "Time step 5, Epoch 209, Batch 1, Loss: -28.952404\n",
            "Time step 5, Epoch 210, Batch 1, Loss: -28.952417\n",
            "Time step 5, Epoch 211, Batch 1, Loss: -28.952435\n",
            "Time step 5, Epoch 212, Batch 1, Loss: -28.952446\n",
            "Time step 5, Epoch 213, Batch 1, Loss: -28.952459\n",
            "Time step 5, Epoch 214, Batch 1, Loss: -28.952473\n",
            "Time step 5, Epoch 215, Batch 1, Loss: -28.952484\n",
            "Time step 5, Epoch 216, Batch 1, Loss: -28.952501\n",
            "Time step 5, Epoch 217, Batch 1, Loss: -28.952511\n",
            "Time step 5, Epoch 218, Batch 1, Loss: -28.952524\n",
            "Time step 5, Epoch 219, Batch 1, Loss: -28.952538\n",
            "Time step 5, Epoch 220, Batch 1, Loss: -28.952551\n",
            "Time step 5, Epoch 221, Batch 1, Loss: -28.952564\n",
            "Time step 5, Epoch 222, Batch 1, Loss: -28.952578\n",
            "Time step 5, Epoch 223, Batch 1, Loss: -28.952589\n",
            "Time step 5, Epoch 224, Batch 1, Loss: -28.952602\n",
            "Time step 5, Epoch 225, Batch 1, Loss: -28.952614\n",
            "Time step 5, Epoch 226, Batch 1, Loss: -28.952625\n",
            "Time step 5, Epoch 227, Batch 1, Loss: -28.952639\n",
            "Time step 5, Epoch 228, Batch 1, Loss: -28.952650\n",
            "Time step 5, Epoch 229, Batch 1, Loss: -28.952665\n",
            "Time step 5, Epoch 230, Batch 1, Loss: -28.952673\n",
            "Time step 5, Epoch 231, Batch 1, Loss: -28.952686\n",
            "Time step 5, Epoch 232, Batch 1, Loss: -28.952698\n",
            "Time step 5, Epoch 233, Batch 1, Loss: -28.952713\n",
            "Time step 5, Epoch 234, Batch 1, Loss: -28.952723\n",
            "Time step 5, Epoch 235, Batch 1, Loss: -28.952736\n",
            "Time step 5, Epoch 236, Batch 1, Loss: -28.952747\n",
            "Time step 5, Epoch 237, Batch 1, Loss: -28.952763\n",
            "Time step 5, Epoch 238, Batch 1, Loss: -28.952772\n",
            "Time step 5, Epoch 239, Batch 1, Loss: -28.952789\n",
            "Time step 5, Epoch 240, Batch 1, Loss: -28.952801\n",
            "Time step 5, Epoch 241, Batch 1, Loss: -28.952812\n",
            "Time step 5, Epoch 242, Batch 1, Loss: -28.952827\n",
            "Time step 5, Epoch 243, Batch 1, Loss: -28.952841\n",
            "Time step 5, Epoch 244, Batch 1, Loss: -28.952856\n",
            "Time step 5, Epoch 245, Batch 1, Loss: -28.952871\n",
            "Time step 5, Epoch 246, Batch 1, Loss: -28.952887\n",
            "Time step 5, Epoch 247, Batch 1, Loss: -28.952904\n",
            "Time step 5, Epoch 248, Batch 1, Loss: -28.952919\n",
            "Time step 5, Epoch 249, Batch 1, Loss: -28.952936\n",
            "Time step 5, Epoch 250, Batch 1, Loss: -28.952957\n",
            "Time step 5, Epoch 251, Batch 1, Loss: -28.952976\n",
            "Time step 5, Epoch 252, Batch 1, Loss: -28.952995\n",
            "Time step 5, Epoch 253, Batch 1, Loss: -28.953018\n",
            "Time step 5, Epoch 254, Batch 1, Loss: -28.953045\n",
            "Time step 5, Epoch 255, Batch 1, Loss: -28.953072\n",
            "Time step 5, Epoch 256, Batch 1, Loss: -28.953102\n",
            "Time step 5, Epoch 257, Batch 1, Loss: -28.953135\n",
            "Time step 5, Epoch 258, Batch 1, Loss: -28.953169\n",
            "Time step 5, Epoch 259, Batch 1, Loss: -28.953215\n",
            "Time step 5, Epoch 260, Batch 1, Loss: -28.953259\n",
            "Time step 5, Epoch 261, Batch 1, Loss: -28.953316\n",
            "Time step 5, Epoch 262, Batch 1, Loss: -28.953379\n",
            "Time step 5, Epoch 263, Batch 1, Loss: -28.953455\n",
            "Time step 5, Epoch 264, Batch 1, Loss: -28.953545\n",
            "Time step 5, Epoch 265, Batch 1, Loss: -28.953655\n",
            "Time step 5, Epoch 266, Batch 1, Loss: -28.953793\n",
            "Time step 5, Epoch 267, Batch 1, Loss: -28.953960\n",
            "Time step 5, Epoch 268, Batch 1, Loss: -28.954182\n",
            "Time step 5, Epoch 269, Batch 1, Loss: -28.954472\n",
            "Time step 5, Epoch 270, Batch 1, Loss: -28.954859\n",
            "Time step 5, Epoch 271, Batch 1, Loss: -28.955397\n",
            "Time step 5, Epoch 272, Batch 1, Loss: -28.956173\n",
            "Time step 5, Epoch 273, Batch 1, Loss: -28.957333\n",
            "Time step 5, Epoch 274, Batch 1, Loss: -28.959112\n",
            "Time step 5, Epoch 275, Batch 1, Loss: -28.961796\n",
            "Time step 5, Epoch 276, Batch 1, Loss: -28.965727\n",
            "Time step 5, Epoch 277, Batch 1, Loss: -28.971254\n",
            "Time step 5, Epoch 278, Batch 1, Loss: -28.979021\n",
            "Time step 5, Epoch 279, Batch 1, Loss: -28.989929\n",
            "Time step 5, Epoch 280, Batch 1, Loss: -29.005007\n",
            "Time step 5, Epoch 281, Batch 1, Loss: -29.024124\n",
            "Time step 5, Epoch 282, Batch 1, Loss: -29.045084\n",
            "Time step 5, Epoch 283, Batch 1, Loss: -29.066093\n",
            "Time step 5, Epoch 284, Batch 1, Loss: -29.085268\n",
            "Time step 5, Epoch 285, Batch 1, Loss: -29.100084\n",
            "Time step 5, Epoch 286, Batch 1, Loss: -29.108200\n",
            "Time step 5, Epoch 287, Batch 1, Loss: -29.109192\n",
            "Time step 5, Epoch 288, Batch 1, Loss: -29.106417\n",
            "Time step 5, Epoch 289, Batch 1, Loss: -29.105103\n",
            "Time step 5, Epoch 290, Batch 1, Loss: -29.108461\n",
            "Time step 5, Epoch 291, Batch 1, Loss: -29.116550\n",
            "Time step 5, Epoch 292, Batch 1, Loss: -29.127472\n",
            "Time step 5, Epoch 293, Batch 1, Loss: -29.138905\n",
            "Time step 5, Epoch 294, Batch 1, Loss: -29.149120\n",
            "Time step 5, Epoch 295, Batch 1, Loss: -29.157270\n",
            "Time step 5, Epoch 296, Batch 1, Loss: -29.163061\n",
            "Time step 5, Epoch 297, Batch 1, Loss: -29.166864\n",
            "Time step 5, Epoch 298, Batch 1, Loss: -29.169319\n",
            "Time step 5, Epoch 299, Batch 1, Loss: -29.171217\n",
            "Time step 5, Epoch 300, Batch 1, Loss: -29.173347\n",
            "Time step 5, Epoch 301, Batch 1, Loss: -29.176188\n",
            "Time step 5, Epoch 302, Batch 1, Loss: -29.179878\n",
            "Time step 5, Epoch 303, Batch 1, Loss: -29.184141\n",
            "Time step 5, Epoch 304, Batch 1, Loss: -29.188501\n",
            "Time step 5, Epoch 305, Batch 1, Loss: -29.192461\n",
            "Time step 5, Epoch 306, Batch 1, Loss: -29.195688\n",
            "Time step 5, Epoch 307, Batch 1, Loss: -29.198103\n",
            "Time step 5, Epoch 308, Batch 1, Loss: -29.199730\n",
            "Time step 5, Epoch 309, Batch 1, Loss: -29.200916\n",
            "Time step 5, Epoch 310, Batch 1, Loss: -29.202021\n",
            "Time step 5, Epoch 311, Batch 1, Loss: -29.203346\n",
            "Time step 5, Epoch 312, Batch 1, Loss: -29.204941\n",
            "Time step 5, Epoch 313, Batch 1, Loss: -29.206785\n",
            "Time step 5, Epoch 314, Batch 1, Loss: -29.208683\n",
            "Time step 5, Epoch 315, Batch 1, Loss: -29.210430\n",
            "Time step 5, Epoch 316, Batch 1, Loss: -29.211901\n",
            "Time step 5, Epoch 317, Batch 1, Loss: -29.213083\n",
            "Time step 5, Epoch 318, Batch 1, Loss: -29.214096\n",
            "Time step 5, Epoch 319, Batch 1, Loss: -29.215052\n",
            "Time step 5, Epoch 320, Batch 1, Loss: -29.216080\n",
            "Time step 5, Epoch 321, Batch 1, Loss: -29.217226\n",
            "Time step 5, Epoch 322, Batch 1, Loss: -29.218489\n",
            "Time step 5, Epoch 323, Batch 1, Loss: -29.219799\n",
            "Time step 5, Epoch 324, Batch 1, Loss: -29.221088\n",
            "Time step 5, Epoch 325, Batch 1, Loss: -29.222282\n",
            "Time step 5, Epoch 326, Batch 1, Loss: -29.223366\n",
            "Time step 5, Epoch 327, Batch 1, Loss: -29.224348\n",
            "Time step 5, Epoch 328, Batch 1, Loss: -29.225252\n",
            "Time step 5, Epoch 329, Batch 1, Loss: -29.226120\n",
            "Time step 5, Epoch 330, Batch 1, Loss: -29.226994\n",
            "Time step 5, Epoch 331, Batch 1, Loss: -29.227894\n",
            "Time step 5, Epoch 332, Batch 1, Loss: -29.228809\n",
            "Time step 5, Epoch 333, Batch 1, Loss: -29.229723\n",
            "Time step 5, Epoch 334, Batch 1, Loss: -29.230608\n",
            "Time step 5, Epoch 335, Batch 1, Loss: -29.231445\n",
            "Time step 5, Epoch 336, Batch 1, Loss: -29.232241\n",
            "Time step 5, Epoch 337, Batch 1, Loss: -29.233007\n",
            "Time step 5, Epoch 338, Batch 1, Loss: -29.233765\n",
            "Time step 5, Epoch 339, Batch 1, Loss: -29.234533\n",
            "Time step 5, Epoch 340, Batch 1, Loss: -29.235329\n",
            "Time step 5, Epoch 341, Batch 1, Loss: -29.236147\n",
            "Time step 5, Epoch 342, Batch 1, Loss: -29.236969\n",
            "Time step 5, Epoch 343, Batch 1, Loss: -29.237774\n",
            "Time step 5, Epoch 344, Batch 1, Loss: -29.238573\n",
            "Time step 5, Epoch 345, Batch 1, Loss: -29.239368\n",
            "Time step 5, Epoch 346, Batch 1, Loss: -29.240166\n",
            "Time step 5, Epoch 347, Batch 1, Loss: -29.240965\n",
            "Time step 5, Epoch 348, Batch 1, Loss: -29.241756\n",
            "Time step 5, Epoch 349, Batch 1, Loss: -29.242546\n",
            "Time step 5, Epoch 350, Batch 1, Loss: -29.243330\n",
            "Time step 5, Epoch 351, Batch 1, Loss: -29.244104\n",
            "Time step 5, Epoch 352, Batch 1, Loss: -29.244850\n",
            "Time step 5, Epoch 353, Batch 1, Loss: -29.245583\n",
            "Time step 5, Epoch 354, Batch 1, Loss: -29.246288\n",
            "Time step 5, Epoch 355, Batch 1, Loss: -29.246975\n",
            "Time step 5, Epoch 356, Batch 1, Loss: -29.247650\n",
            "Time step 5, Epoch 357, Batch 1, Loss: -29.248316\n",
            "Time step 5, Epoch 358, Batch 1, Loss: -29.248970\n",
            "Time step 5, Epoch 359, Batch 1, Loss: -29.249619\n",
            "Time step 5, Epoch 360, Batch 1, Loss: -29.250275\n",
            "Time step 5, Epoch 361, Batch 1, Loss: -29.250925\n",
            "Time step 5, Epoch 362, Batch 1, Loss: -29.251560\n",
            "Time step 5, Epoch 363, Batch 1, Loss: -29.252169\n",
            "Time step 5, Epoch 364, Batch 1, Loss: -29.252762\n",
            "Time step 5, Epoch 365, Batch 1, Loss: -29.253340\n",
            "Time step 5, Epoch 366, Batch 1, Loss: -29.253910\n",
            "Time step 5, Epoch 367, Batch 1, Loss: -29.254473\n",
            "Time step 5, Epoch 368, Batch 1, Loss: -29.255030\n",
            "Time step 5, Epoch 369, Batch 1, Loss: -29.255594\n",
            "Time step 5, Epoch 370, Batch 1, Loss: -29.256153\n",
            "Time step 5, Epoch 371, Batch 1, Loss: -29.256697\n",
            "Time step 5, Epoch 372, Batch 1, Loss: -29.257240\n",
            "Time step 5, Epoch 373, Batch 1, Loss: -29.257786\n",
            "Time step 5, Epoch 374, Batch 1, Loss: -29.258320\n",
            "Time step 5, Epoch 375, Batch 1, Loss: -29.258867\n",
            "Time step 5, Epoch 376, Batch 1, Loss: -29.259420\n",
            "Time step 5, Epoch 377, Batch 1, Loss: -29.259954\n",
            "Time step 5, Epoch 378, Batch 1, Loss: -29.260489\n",
            "Time step 5, Epoch 379, Batch 1, Loss: -29.261005\n",
            "Time step 5, Epoch 380, Batch 1, Loss: -29.261528\n",
            "Time step 5, Epoch 381, Batch 1, Loss: -29.262037\n",
            "Time step 5, Epoch 382, Batch 1, Loss: -29.262550\n",
            "Time step 5, Epoch 383, Batch 1, Loss: -29.263056\n",
            "Time step 5, Epoch 384, Batch 1, Loss: -29.263548\n",
            "Time step 5, Epoch 385, Batch 1, Loss: -29.264042\n",
            "Time step 5, Epoch 386, Batch 1, Loss: -29.264540\n",
            "Time step 5, Epoch 387, Batch 1, Loss: -29.265036\n",
            "Time step 5, Epoch 388, Batch 1, Loss: -29.265528\n",
            "Time step 5, Epoch 389, Batch 1, Loss: -29.265984\n",
            "Time step 5, Epoch 390, Batch 1, Loss: -29.266441\n",
            "Time step 5, Epoch 391, Batch 1, Loss: -29.266897\n",
            "Time step 5, Epoch 392, Batch 1, Loss: -29.267365\n",
            "Time step 5, Epoch 393, Batch 1, Loss: -29.267849\n",
            "Time step 5, Epoch 394, Batch 1, Loss: -29.268339\n",
            "Time step 5, Epoch 395, Batch 1, Loss: -29.268829\n",
            "Time step 5, Epoch 396, Batch 1, Loss: -29.269323\n",
            "Time step 5, Epoch 397, Batch 1, Loss: -29.269800\n",
            "Time step 5, Epoch 398, Batch 1, Loss: -29.270271\n",
            "Time step 5, Epoch 399, Batch 1, Loss: -29.270748\n",
            "Time step 5, Epoch 400, Batch 1, Loss: -29.271223\n",
            "Time step 5, Epoch 401, Batch 1, Loss: -29.271698\n",
            "Time step 5, Epoch 402, Batch 1, Loss: -29.272169\n",
            "Time step 5, Epoch 403, Batch 1, Loss: -29.272646\n",
            "Time step 5, Epoch 404, Batch 1, Loss: -29.273109\n",
            "Time step 5, Epoch 405, Batch 1, Loss: -29.273577\n",
            "Time step 5, Epoch 406, Batch 1, Loss: -29.274050\n",
            "Time step 5, Epoch 407, Batch 1, Loss: -29.274527\n",
            "Time step 5, Epoch 408, Batch 1, Loss: -29.274990\n",
            "Time step 5, Epoch 409, Batch 1, Loss: -29.275452\n",
            "Time step 5, Epoch 410, Batch 1, Loss: -29.275906\n",
            "Time step 5, Epoch 411, Batch 1, Loss: -29.276373\n",
            "Time step 5, Epoch 412, Batch 1, Loss: -29.276833\n",
            "Time step 5, Epoch 413, Batch 1, Loss: -29.277283\n",
            "Time step 5, Epoch 414, Batch 1, Loss: -29.277725\n",
            "Time step 5, Epoch 415, Batch 1, Loss: -29.278173\n",
            "Time step 5, Epoch 416, Batch 1, Loss: -29.278620\n",
            "Time step 5, Epoch 417, Batch 1, Loss: -29.279062\n",
            "Time step 5, Epoch 418, Batch 1, Loss: -29.279507\n",
            "Time step 5, Epoch 419, Batch 1, Loss: -29.279942\n",
            "Time step 5, Epoch 420, Batch 1, Loss: -29.280380\n",
            "Time step 5, Epoch 421, Batch 1, Loss: -29.280823\n",
            "Time step 5, Epoch 422, Batch 1, Loss: -29.281258\n",
            "Time step 5, Epoch 423, Batch 1, Loss: -29.281700\n",
            "Time step 5, Epoch 424, Batch 1, Loss: -29.282133\n",
            "Time step 5, Epoch 425, Batch 1, Loss: -29.282566\n",
            "Time step 5, Epoch 426, Batch 1, Loss: -29.283003\n",
            "Time step 5, Epoch 427, Batch 1, Loss: -29.283438\n",
            "Time step 5, Epoch 428, Batch 1, Loss: -29.283875\n",
            "Time step 5, Epoch 429, Batch 1, Loss: -29.284319\n",
            "Time step 5, Epoch 430, Batch 1, Loss: -29.284763\n",
            "Time step 5, Epoch 431, Batch 1, Loss: -29.285212\n",
            "Time step 5, Epoch 432, Batch 1, Loss: -29.285664\n",
            "Time step 5, Epoch 433, Batch 1, Loss: -29.286121\n",
            "Time step 5, Epoch 434, Batch 1, Loss: -29.286587\n",
            "Time step 5, Epoch 435, Batch 1, Loss: -29.287048\n",
            "Time step 5, Epoch 436, Batch 1, Loss: -29.287518\n",
            "Time step 5, Epoch 437, Batch 1, Loss: -29.287989\n",
            "Time step 5, Epoch 438, Batch 1, Loss: -29.288458\n",
            "Time step 5, Epoch 439, Batch 1, Loss: -29.288944\n",
            "Time step 5, Epoch 440, Batch 1, Loss: -29.289433\n",
            "Time step 5, Epoch 441, Batch 1, Loss: -29.289934\n",
            "Time step 5, Epoch 442, Batch 1, Loss: -29.290442\n",
            "Time step 5, Epoch 443, Batch 1, Loss: -29.290960\n",
            "Time step 5, Epoch 444, Batch 1, Loss: -29.291498\n",
            "Time step 5, Epoch 445, Batch 1, Loss: -29.292042\n",
            "Time step 5, Epoch 446, Batch 1, Loss: -29.292587\n",
            "Time step 5, Epoch 447, Batch 1, Loss: -29.293150\n",
            "Time step 5, Epoch 448, Batch 1, Loss: -29.293734\n",
            "Time step 5, Epoch 449, Batch 1, Loss: -29.294334\n",
            "Time step 5, Epoch 450, Batch 1, Loss: -29.294945\n",
            "Time step 5, Epoch 451, Batch 1, Loss: -29.295593\n",
            "Time step 5, Epoch 452, Batch 1, Loss: -29.296270\n",
            "Time step 5, Epoch 453, Batch 1, Loss: -29.296978\n",
            "Time step 5, Epoch 454, Batch 1, Loss: -29.297720\n",
            "Time step 5, Epoch 455, Batch 1, Loss: -29.298502\n",
            "Time step 5, Epoch 456, Batch 1, Loss: -29.299305\n",
            "Time step 5, Epoch 457, Batch 1, Loss: -29.300097\n",
            "Time step 5, Epoch 458, Batch 1, Loss: -29.300819\n",
            "Time step 5, Epoch 459, Batch 1, Loss: -29.301516\n",
            "Time step 5, Epoch 460, Batch 1, Loss: -29.302166\n",
            "Time step 5, Epoch 461, Batch 1, Loss: -29.302799\n",
            "Time step 5, Epoch 462, Batch 1, Loss: -29.303429\n",
            "Time step 5, Epoch 463, Batch 1, Loss: -29.304024\n",
            "Time step 5, Epoch 464, Batch 1, Loss: -29.304604\n",
            "Time step 5, Epoch 465, Batch 1, Loss: -29.305174\n",
            "Time step 5, Epoch 466, Batch 1, Loss: -29.305719\n",
            "Time step 5, Epoch 467, Batch 1, Loss: -29.306240\n",
            "Time step 5, Epoch 468, Batch 1, Loss: -29.306711\n",
            "Time step 5, Epoch 469, Batch 1, Loss: -29.307167\n",
            "Time step 5, Epoch 470, Batch 1, Loss: -29.307631\n",
            "Time step 5, Epoch 471, Batch 1, Loss: -29.308117\n",
            "Time step 5, Epoch 472, Batch 1, Loss: -29.308611\n",
            "Time step 5, Epoch 473, Batch 1, Loss: -29.309080\n",
            "Time step 5, Epoch 474, Batch 1, Loss: -29.309557\n",
            "Time step 5, Epoch 475, Batch 1, Loss: -29.310036\n",
            "Time step 5, Epoch 476, Batch 1, Loss: -29.310507\n",
            "Time step 5, Epoch 477, Batch 1, Loss: -29.310972\n",
            "Time step 5, Epoch 478, Batch 1, Loss: -29.311436\n",
            "Time step 5, Epoch 479, Batch 1, Loss: -29.311893\n",
            "Time step 5, Epoch 480, Batch 1, Loss: -29.312344\n",
            "Time step 5, Epoch 481, Batch 1, Loss: -29.312788\n",
            "Time step 5, Epoch 482, Batch 1, Loss: -29.313217\n",
            "Time step 5, Epoch 483, Batch 1, Loss: -29.313637\n",
            "Time step 5, Epoch 484, Batch 1, Loss: -29.314053\n",
            "Time step 5, Epoch 485, Batch 1, Loss: -29.314457\n",
            "Time step 5, Epoch 486, Batch 1, Loss: -29.314848\n",
            "Time step 5, Epoch 487, Batch 1, Loss: -29.315248\n",
            "Time step 5, Epoch 488, Batch 1, Loss: -29.315657\n",
            "Time step 5, Epoch 489, Batch 1, Loss: -29.316051\n",
            "Time step 5, Epoch 490, Batch 1, Loss: -29.316452\n",
            "Time step 5, Epoch 491, Batch 1, Loss: -29.316856\n",
            "Time step 5, Epoch 492, Batch 1, Loss: -29.317268\n",
            "Time step 5, Epoch 493, Batch 1, Loss: -29.317673\n",
            "Time step 5, Epoch 494, Batch 1, Loss: -29.318068\n",
            "Time step 5, Epoch 495, Batch 1, Loss: -29.318464\n",
            "Time step 5, Epoch 496, Batch 1, Loss: -29.318865\n",
            "Time step 5, Epoch 497, Batch 1, Loss: -29.319263\n",
            "Time step 5, Epoch 498, Batch 1, Loss: -29.319653\n",
            "Time step 5, Epoch 499, Batch 1, Loss: -29.320047\n",
            "Time step 5, Epoch 500, Batch 1, Loss: -29.320429\n",
            "Training model for time step 4...\n",
            "Time step 4, Epoch 1, Batch 1, Loss: -25.796148\n",
            "Time step 4, Epoch 2, Batch 1, Loss: -25.952679\n",
            "Time step 4, Epoch 3, Batch 1, Loss: -26.109840\n",
            "Time step 4, Epoch 4, Batch 1, Loss: -26.266962\n",
            "Time step 4, Epoch 5, Batch 1, Loss: -26.423546\n",
            "Time step 4, Epoch 6, Batch 1, Loss: -26.579258\n",
            "Time step 4, Epoch 7, Batch 1, Loss: -26.733416\n",
            "Time step 4, Epoch 8, Batch 1, Loss: -26.886013\n",
            "Time step 4, Epoch 9, Batch 1, Loss: -27.036304\n",
            "Time step 4, Epoch 10, Batch 1, Loss: -27.183569\n",
            "Time step 4, Epoch 11, Batch 1, Loss: -27.327538\n",
            "Time step 4, Epoch 12, Batch 1, Loss: -27.467594\n",
            "Time step 4, Epoch 13, Batch 1, Loss: -27.603264\n",
            "Time step 4, Epoch 14, Batch 1, Loss: -27.733852\n",
            "Time step 4, Epoch 15, Batch 1, Loss: -27.858967\n",
            "Time step 4, Epoch 16, Batch 1, Loss: -27.978439\n",
            "Time step 4, Epoch 17, Batch 1, Loss: -28.092037\n",
            "Time step 4, Epoch 18, Batch 1, Loss: -28.199671\n",
            "Time step 4, Epoch 19, Batch 1, Loss: -28.301189\n",
            "Time step 4, Epoch 20, Batch 1, Loss: -28.396519\n",
            "Time step 4, Epoch 21, Batch 1, Loss: -28.485657\n",
            "Time step 4, Epoch 22, Batch 1, Loss: -28.568752\n",
            "Time step 4, Epoch 23, Batch 1, Loss: -28.645805\n",
            "Time step 4, Epoch 24, Batch 1, Loss: -28.716873\n",
            "Time step 4, Epoch 25, Batch 1, Loss: -28.782082\n",
            "Time step 4, Epoch 26, Batch 1, Loss: -28.841576\n",
            "Time step 4, Epoch 27, Batch 1, Loss: -28.895672\n",
            "Time step 4, Epoch 28, Batch 1, Loss: -28.944632\n",
            "Time step 4, Epoch 29, Batch 1, Loss: -28.988747\n",
            "Time step 4, Epoch 30, Batch 1, Loss: -29.028353\n",
            "Time step 4, Epoch 31, Batch 1, Loss: -29.063812\n",
            "Time step 4, Epoch 32, Batch 1, Loss: -29.095442\n",
            "Time step 4, Epoch 33, Batch 1, Loss: -29.123547\n",
            "Time step 4, Epoch 34, Batch 1, Loss: -29.148434\n",
            "Time step 4, Epoch 35, Batch 1, Loss: -29.170410\n",
            "Time step 4, Epoch 36, Batch 1, Loss: -29.189764\n",
            "Time step 4, Epoch 37, Batch 1, Loss: -29.206797\n",
            "Time step 4, Epoch 38, Batch 1, Loss: -29.221771\n",
            "Time step 4, Epoch 39, Batch 1, Loss: -29.234932\n",
            "Time step 4, Epoch 40, Batch 1, Loss: -29.246487\n",
            "Time step 4, Epoch 41, Batch 1, Loss: -29.256630\n",
            "Time step 4, Epoch 42, Batch 1, Loss: -29.265541\n",
            "Time step 4, Epoch 43, Batch 1, Loss: -29.273376\n",
            "Time step 4, Epoch 44, Batch 1, Loss: -29.280272\n",
            "Time step 4, Epoch 45, Batch 1, Loss: -29.286352\n",
            "Time step 4, Epoch 46, Batch 1, Loss: -29.291721\n",
            "Time step 4, Epoch 47, Batch 1, Loss: -29.296463\n",
            "Time step 4, Epoch 48, Batch 1, Loss: -29.300659\n",
            "Time step 4, Epoch 49, Batch 1, Loss: -29.304382\n",
            "Time step 4, Epoch 50, Batch 1, Loss: -29.307690\n",
            "Time step 4, Epoch 51, Batch 1, Loss: -29.310638\n",
            "Time step 4, Epoch 52, Batch 1, Loss: -29.313265\n",
            "Time step 4, Epoch 53, Batch 1, Loss: -29.315617\n",
            "Time step 4, Epoch 54, Batch 1, Loss: -29.317730\n",
            "Time step 4, Epoch 55, Batch 1, Loss: -29.319618\n",
            "Time step 4, Epoch 56, Batch 1, Loss: -29.321323\n",
            "Time step 4, Epoch 57, Batch 1, Loss: -29.322859\n",
            "Time step 4, Epoch 58, Batch 1, Loss: -29.324247\n",
            "Time step 4, Epoch 59, Batch 1, Loss: -29.325502\n",
            "Time step 4, Epoch 60, Batch 1, Loss: -29.326645\n",
            "Time step 4, Epoch 61, Batch 1, Loss: -29.327679\n",
            "Time step 4, Epoch 62, Batch 1, Loss: -29.328627\n",
            "Time step 4, Epoch 63, Batch 1, Loss: -29.329487\n",
            "Time step 4, Epoch 64, Batch 1, Loss: -29.330278\n",
            "Time step 4, Epoch 65, Batch 1, Loss: -29.331001\n",
            "Time step 4, Epoch 66, Batch 1, Loss: -29.331669\n",
            "Time step 4, Epoch 67, Batch 1, Loss: -29.332281\n",
            "Time step 4, Epoch 68, Batch 1, Loss: -29.332848\n",
            "Time step 4, Epoch 69, Batch 1, Loss: -29.333370\n",
            "Time step 4, Epoch 70, Batch 1, Loss: -29.333851\n",
            "Time step 4, Epoch 71, Batch 1, Loss: -29.334303\n",
            "Time step 4, Epoch 72, Batch 1, Loss: -29.334719\n",
            "Time step 4, Epoch 73, Batch 1, Loss: -29.335106\n",
            "Time step 4, Epoch 74, Batch 1, Loss: -29.335468\n",
            "Time step 4, Epoch 75, Batch 1, Loss: -29.335806\n",
            "Time step 4, Epoch 76, Batch 1, Loss: -29.336123\n",
            "Time step 4, Epoch 77, Batch 1, Loss: -29.336416\n",
            "Time step 4, Epoch 78, Batch 1, Loss: -29.336697\n",
            "Time step 4, Epoch 79, Batch 1, Loss: -29.336958\n",
            "Time step 4, Epoch 80, Batch 1, Loss: -29.337204\n",
            "Time step 4, Epoch 81, Batch 1, Loss: -29.337435\n",
            "Time step 4, Epoch 82, Batch 1, Loss: -29.337654\n",
            "Time step 4, Epoch 83, Batch 1, Loss: -29.337860\n",
            "Time step 4, Epoch 84, Batch 1, Loss: -29.338057\n",
            "Time step 4, Epoch 85, Batch 1, Loss: -29.338242\n",
            "Time step 4, Epoch 86, Batch 1, Loss: -29.338417\n",
            "Time step 4, Epoch 87, Batch 1, Loss: -29.338585\n",
            "Time step 4, Epoch 88, Batch 1, Loss: -29.338745\n",
            "Time step 4, Epoch 89, Batch 1, Loss: -29.338894\n",
            "Time step 4, Epoch 90, Batch 1, Loss: -29.339039\n",
            "Time step 4, Epoch 91, Batch 1, Loss: -29.339176\n",
            "Time step 4, Epoch 92, Batch 1, Loss: -29.339310\n",
            "Time step 4, Epoch 93, Batch 1, Loss: -29.339434\n",
            "Time step 4, Epoch 94, Batch 1, Loss: -29.339554\n",
            "Time step 4, Epoch 95, Batch 1, Loss: -29.339668\n",
            "Time step 4, Epoch 96, Batch 1, Loss: -29.339779\n",
            "Time step 4, Epoch 97, Batch 1, Loss: -29.339888\n",
            "Time step 4, Epoch 98, Batch 1, Loss: -29.339987\n",
            "Time step 4, Epoch 99, Batch 1, Loss: -29.340088\n",
            "Time step 4, Epoch 100, Batch 1, Loss: -29.340179\n",
            "Time step 4, Epoch 101, Batch 1, Loss: -29.340271\n",
            "Time step 4, Epoch 102, Batch 1, Loss: -29.340359\n",
            "Time step 4, Epoch 103, Batch 1, Loss: -29.340443\n",
            "Time step 4, Epoch 104, Batch 1, Loss: -29.340527\n",
            "Time step 4, Epoch 105, Batch 1, Loss: -29.340605\n",
            "Time step 4, Epoch 106, Batch 1, Loss: -29.340679\n",
            "Time step 4, Epoch 107, Batch 1, Loss: -29.340754\n",
            "Time step 4, Epoch 108, Batch 1, Loss: -29.340824\n",
            "Time step 4, Epoch 109, Batch 1, Loss: -29.340895\n",
            "Time step 4, Epoch 110, Batch 1, Loss: -29.340961\n",
            "Time step 4, Epoch 111, Batch 1, Loss: -29.341022\n",
            "Time step 4, Epoch 112, Batch 1, Loss: -29.341087\n",
            "Time step 4, Epoch 113, Batch 1, Loss: -29.341148\n",
            "Time step 4, Epoch 114, Batch 1, Loss: -29.341209\n",
            "Time step 4, Epoch 115, Batch 1, Loss: -29.341263\n",
            "Time step 4, Epoch 116, Batch 1, Loss: -29.341318\n",
            "Time step 4, Epoch 117, Batch 1, Loss: -29.341375\n",
            "Time step 4, Epoch 118, Batch 1, Loss: -29.341425\n",
            "Time step 4, Epoch 119, Batch 1, Loss: -29.341476\n",
            "Time step 4, Epoch 120, Batch 1, Loss: -29.341526\n",
            "Time step 4, Epoch 121, Batch 1, Loss: -29.341572\n",
            "Time step 4, Epoch 122, Batch 1, Loss: -29.341619\n",
            "Time step 4, Epoch 123, Batch 1, Loss: -29.341665\n",
            "Time step 4, Epoch 124, Batch 1, Loss: -29.341709\n",
            "Time step 4, Epoch 125, Batch 1, Loss: -29.341753\n",
            "Time step 4, Epoch 126, Batch 1, Loss: -29.341793\n",
            "Time step 4, Epoch 127, Batch 1, Loss: -29.341835\n",
            "Time step 4, Epoch 128, Batch 1, Loss: -29.341875\n",
            "Time step 4, Epoch 129, Batch 1, Loss: -29.341913\n",
            "Time step 4, Epoch 130, Batch 1, Loss: -29.341951\n",
            "Time step 4, Epoch 131, Batch 1, Loss: -29.341990\n",
            "Time step 4, Epoch 132, Batch 1, Loss: -29.342026\n",
            "Time step 4, Epoch 133, Batch 1, Loss: -29.342062\n",
            "Time step 4, Epoch 134, Batch 1, Loss: -29.342094\n",
            "Time step 4, Epoch 135, Batch 1, Loss: -29.342129\n",
            "Time step 4, Epoch 136, Batch 1, Loss: -29.342161\n",
            "Time step 4, Epoch 137, Batch 1, Loss: -29.342190\n",
            "Time step 4, Epoch 138, Batch 1, Loss: -29.342224\n",
            "Time step 4, Epoch 139, Batch 1, Loss: -29.342255\n",
            "Time step 4, Epoch 140, Batch 1, Loss: -29.342283\n",
            "Time step 4, Epoch 141, Batch 1, Loss: -29.342314\n",
            "Time step 4, Epoch 142, Batch 1, Loss: -29.342342\n",
            "Time step 4, Epoch 143, Batch 1, Loss: -29.342371\n",
            "Time step 4, Epoch 144, Batch 1, Loss: -29.342398\n",
            "Time step 4, Epoch 145, Batch 1, Loss: -29.342426\n",
            "Time step 4, Epoch 146, Batch 1, Loss: -29.342449\n",
            "Time step 4, Epoch 147, Batch 1, Loss: -29.342476\n",
            "Time step 4, Epoch 148, Batch 1, Loss: -29.342499\n",
            "Time step 4, Epoch 149, Batch 1, Loss: -29.342524\n",
            "Time step 4, Epoch 150, Batch 1, Loss: -29.342548\n",
            "Time step 4, Epoch 151, Batch 1, Loss: -29.342573\n",
            "Time step 4, Epoch 152, Batch 1, Loss: -29.342594\n",
            "Time step 4, Epoch 153, Batch 1, Loss: -29.342617\n",
            "Time step 4, Epoch 154, Batch 1, Loss: -29.342640\n",
            "Time step 4, Epoch 155, Batch 1, Loss: -29.342661\n",
            "Time step 4, Epoch 156, Batch 1, Loss: -29.342682\n",
            "Time step 4, Epoch 157, Batch 1, Loss: -29.342705\n",
            "Time step 4, Epoch 158, Batch 1, Loss: -29.342724\n",
            "Time step 4, Epoch 159, Batch 1, Loss: -29.342743\n",
            "Time step 4, Epoch 160, Batch 1, Loss: -29.342760\n",
            "Time step 4, Epoch 161, Batch 1, Loss: -29.342783\n",
            "Time step 4, Epoch 162, Batch 1, Loss: -29.342800\n",
            "Time step 4, Epoch 163, Batch 1, Loss: -29.342819\n",
            "Time step 4, Epoch 164, Batch 1, Loss: -29.342836\n",
            "Time step 4, Epoch 165, Batch 1, Loss: -29.342855\n",
            "Time step 4, Epoch 166, Batch 1, Loss: -29.342871\n",
            "Time step 4, Epoch 167, Batch 1, Loss: -29.342888\n",
            "Time step 4, Epoch 168, Batch 1, Loss: -29.342905\n",
            "Time step 4, Epoch 169, Batch 1, Loss: -29.342922\n",
            "Time step 4, Epoch 170, Batch 1, Loss: -29.342937\n",
            "Time step 4, Epoch 171, Batch 1, Loss: -29.342955\n",
            "Time step 4, Epoch 172, Batch 1, Loss: -29.342966\n",
            "Time step 4, Epoch 173, Batch 1, Loss: -29.342983\n",
            "Time step 4, Epoch 174, Batch 1, Loss: -29.342999\n",
            "Time step 4, Epoch 175, Batch 1, Loss: -29.343014\n",
            "Time step 4, Epoch 176, Batch 1, Loss: -29.343029\n",
            "Time step 4, Epoch 177, Batch 1, Loss: -29.343040\n",
            "Time step 4, Epoch 178, Batch 1, Loss: -29.343058\n",
            "Time step 4, Epoch 179, Batch 1, Loss: -29.343071\n",
            "Time step 4, Epoch 180, Batch 1, Loss: -29.343082\n",
            "Time step 4, Epoch 181, Batch 1, Loss: -29.343094\n",
            "Time step 4, Epoch 182, Batch 1, Loss: -29.343109\n",
            "Time step 4, Epoch 183, Batch 1, Loss: -29.343122\n",
            "Time step 4, Epoch 184, Batch 1, Loss: -29.343136\n",
            "Time step 4, Epoch 185, Batch 1, Loss: -29.343149\n",
            "Time step 4, Epoch 186, Batch 1, Loss: -29.343163\n",
            "Time step 4, Epoch 187, Batch 1, Loss: -29.343172\n",
            "Time step 4, Epoch 188, Batch 1, Loss: -29.343184\n",
            "Time step 4, Epoch 189, Batch 1, Loss: -29.343193\n",
            "Time step 4, Epoch 190, Batch 1, Loss: -29.343206\n",
            "Time step 4, Epoch 191, Batch 1, Loss: -29.343218\n",
            "Time step 4, Epoch 192, Batch 1, Loss: -29.343227\n",
            "Time step 4, Epoch 193, Batch 1, Loss: -29.343239\n",
            "Time step 4, Epoch 194, Batch 1, Loss: -29.343248\n",
            "Time step 4, Epoch 195, Batch 1, Loss: -29.343262\n",
            "Time step 4, Epoch 196, Batch 1, Loss: -29.343271\n",
            "Time step 4, Epoch 197, Batch 1, Loss: -29.343281\n",
            "Time step 4, Epoch 198, Batch 1, Loss: -29.343292\n",
            "Time step 4, Epoch 199, Batch 1, Loss: -29.343302\n",
            "Time step 4, Epoch 200, Batch 1, Loss: -29.343309\n",
            "Time step 4, Epoch 201, Batch 1, Loss: -29.343319\n",
            "Time step 4, Epoch 202, Batch 1, Loss: -29.343327\n",
            "Time step 4, Epoch 203, Batch 1, Loss: -29.343340\n",
            "Time step 4, Epoch 204, Batch 1, Loss: -29.343346\n",
            "Time step 4, Epoch 205, Batch 1, Loss: -29.343357\n",
            "Time step 4, Epoch 206, Batch 1, Loss: -29.343365\n",
            "Time step 4, Epoch 207, Batch 1, Loss: -29.343374\n",
            "Time step 4, Epoch 208, Batch 1, Loss: -29.343384\n",
            "Time step 4, Epoch 209, Batch 1, Loss: -29.343393\n",
            "Time step 4, Epoch 210, Batch 1, Loss: -29.343399\n",
            "Time step 4, Epoch 211, Batch 1, Loss: -29.343407\n",
            "Time step 4, Epoch 212, Batch 1, Loss: -29.343414\n",
            "Time step 4, Epoch 213, Batch 1, Loss: -29.343424\n",
            "Time step 4, Epoch 214, Batch 1, Loss: -29.343431\n",
            "Time step 4, Epoch 215, Batch 1, Loss: -29.343437\n",
            "Time step 4, Epoch 216, Batch 1, Loss: -29.343447\n",
            "Time step 4, Epoch 217, Batch 1, Loss: -29.343456\n",
            "Time step 4, Epoch 218, Batch 1, Loss: -29.343462\n",
            "Time step 4, Epoch 219, Batch 1, Loss: -29.343466\n",
            "Time step 4, Epoch 220, Batch 1, Loss: -29.343475\n",
            "Time step 4, Epoch 221, Batch 1, Loss: -29.343483\n",
            "Time step 4, Epoch 222, Batch 1, Loss: -29.343489\n",
            "Time step 4, Epoch 223, Batch 1, Loss: -29.343496\n",
            "Time step 4, Epoch 224, Batch 1, Loss: -29.343504\n",
            "Time step 4, Epoch 225, Batch 1, Loss: -29.343512\n",
            "Time step 4, Epoch 226, Batch 1, Loss: -29.343515\n",
            "Time step 4, Epoch 227, Batch 1, Loss: -29.343523\n",
            "Time step 4, Epoch 228, Batch 1, Loss: -29.343533\n",
            "Time step 4, Epoch 229, Batch 1, Loss: -29.343536\n",
            "Time step 4, Epoch 230, Batch 1, Loss: -29.343542\n",
            "Time step 4, Epoch 231, Batch 1, Loss: -29.343548\n",
            "Time step 4, Epoch 232, Batch 1, Loss: -29.343555\n",
            "Time step 4, Epoch 233, Batch 1, Loss: -29.343561\n",
            "Time step 4, Epoch 234, Batch 1, Loss: -29.343567\n",
            "Time step 4, Epoch 235, Batch 1, Loss: -29.343573\n",
            "Time step 4, Epoch 236, Batch 1, Loss: -29.343578\n",
            "Time step 4, Epoch 237, Batch 1, Loss: -29.343584\n",
            "Time step 4, Epoch 238, Batch 1, Loss: -29.343588\n",
            "Time step 4, Epoch 239, Batch 1, Loss: -29.343596\n",
            "Time step 4, Epoch 240, Batch 1, Loss: -29.343601\n",
            "Time step 4, Epoch 241, Batch 1, Loss: -29.343605\n",
            "Time step 4, Epoch 242, Batch 1, Loss: -29.343613\n",
            "Time step 4, Epoch 243, Batch 1, Loss: -29.343616\n",
            "Time step 4, Epoch 244, Batch 1, Loss: -29.343620\n",
            "Time step 4, Epoch 245, Batch 1, Loss: -29.343626\n",
            "Time step 4, Epoch 246, Batch 1, Loss: -29.343630\n",
            "Time step 4, Epoch 247, Batch 1, Loss: -29.343636\n",
            "Time step 4, Epoch 248, Batch 1, Loss: -29.343641\n",
            "Time step 4, Epoch 249, Batch 1, Loss: -29.343647\n",
            "Time step 4, Epoch 250, Batch 1, Loss: -29.343651\n",
            "Time step 4, Epoch 251, Batch 1, Loss: -29.343657\n",
            "Time step 4, Epoch 252, Batch 1, Loss: -29.343658\n",
            "Time step 4, Epoch 253, Batch 1, Loss: -29.343664\n",
            "Time step 4, Epoch 254, Batch 1, Loss: -29.343670\n",
            "Time step 4, Epoch 255, Batch 1, Loss: -29.343676\n",
            "Time step 4, Epoch 256, Batch 1, Loss: -29.343678\n",
            "Time step 4, Epoch 257, Batch 1, Loss: -29.343683\n",
            "Time step 4, Epoch 258, Batch 1, Loss: -29.343685\n",
            "Time step 4, Epoch 259, Batch 1, Loss: -29.343691\n",
            "Time step 4, Epoch 260, Batch 1, Loss: -29.343695\n",
            "Time step 4, Epoch 261, Batch 1, Loss: -29.343699\n",
            "Time step 4, Epoch 262, Batch 1, Loss: -29.343702\n",
            "Time step 4, Epoch 263, Batch 1, Loss: -29.343708\n",
            "Time step 4, Epoch 264, Batch 1, Loss: -29.343712\n",
            "Time step 4, Epoch 265, Batch 1, Loss: -29.343718\n",
            "Time step 4, Epoch 266, Batch 1, Loss: -29.343721\n",
            "Time step 4, Epoch 267, Batch 1, Loss: -29.343723\n",
            "Time step 4, Epoch 268, Batch 1, Loss: -29.343729\n",
            "Time step 4, Epoch 269, Batch 1, Loss: -29.343731\n",
            "Time step 4, Epoch 270, Batch 1, Loss: -29.343737\n",
            "Time step 4, Epoch 271, Batch 1, Loss: -29.343739\n",
            "Time step 4, Epoch 272, Batch 1, Loss: -29.343744\n",
            "Time step 4, Epoch 273, Batch 1, Loss: -29.343748\n",
            "Time step 4, Epoch 274, Batch 1, Loss: -29.343752\n",
            "Time step 4, Epoch 275, Batch 1, Loss: -29.343754\n",
            "Time step 4, Epoch 276, Batch 1, Loss: -29.343758\n",
            "Time step 4, Epoch 277, Batch 1, Loss: -29.343760\n",
            "Time step 4, Epoch 278, Batch 1, Loss: -29.343765\n",
            "Time step 4, Epoch 279, Batch 1, Loss: -29.343765\n",
            "Time step 4, Epoch 280, Batch 1, Loss: -29.343771\n",
            "Time step 4, Epoch 281, Batch 1, Loss: -29.343773\n",
            "Time step 4, Epoch 282, Batch 1, Loss: -29.343779\n",
            "Time step 4, Epoch 283, Batch 1, Loss: -29.343781\n",
            "Time step 4, Epoch 284, Batch 1, Loss: -29.343786\n",
            "Time step 4, Epoch 285, Batch 1, Loss: -29.343788\n",
            "Time step 4, Epoch 286, Batch 1, Loss: -29.343790\n",
            "Time step 4, Epoch 287, Batch 1, Loss: -29.343794\n",
            "Time step 4, Epoch 288, Batch 1, Loss: -29.343798\n",
            "Time step 4, Epoch 289, Batch 1, Loss: -29.343800\n",
            "Time step 4, Epoch 290, Batch 1, Loss: -29.343803\n",
            "Time step 4, Epoch 291, Batch 1, Loss: -29.343807\n",
            "Time step 4, Epoch 292, Batch 1, Loss: -29.343809\n",
            "Time step 4, Epoch 293, Batch 1, Loss: -29.343813\n",
            "Time step 4, Epoch 294, Batch 1, Loss: -29.343815\n",
            "Time step 4, Epoch 295, Batch 1, Loss: -29.343817\n",
            "Time step 4, Epoch 296, Batch 1, Loss: -29.343821\n",
            "Time step 4, Epoch 297, Batch 1, Loss: -29.343821\n",
            "Time step 4, Epoch 298, Batch 1, Loss: -29.343828\n",
            "Time step 4, Epoch 299, Batch 1, Loss: -29.343830\n",
            "Time step 4, Epoch 300, Batch 1, Loss: -29.343832\n",
            "Time step 4, Epoch 301, Batch 1, Loss: -29.343834\n",
            "Time step 4, Epoch 302, Batch 1, Loss: -29.343838\n",
            "Time step 4, Epoch 303, Batch 1, Loss: -29.343838\n",
            "Time step 4, Epoch 304, Batch 1, Loss: -29.343842\n",
            "Time step 4, Epoch 305, Batch 1, Loss: -29.343843\n",
            "Time step 4, Epoch 306, Batch 1, Loss: -29.343849\n",
            "Time step 4, Epoch 307, Batch 1, Loss: -29.343847\n",
            "Time step 4, Epoch 308, Batch 1, Loss: -29.343853\n",
            "Time step 4, Epoch 309, Batch 1, Loss: -29.343855\n",
            "Time step 4, Epoch 310, Batch 1, Loss: -29.343857\n",
            "Time step 4, Epoch 311, Batch 1, Loss: -29.343859\n",
            "Time step 4, Epoch 312, Batch 1, Loss: -29.343861\n",
            "Time step 4, Epoch 313, Batch 1, Loss: -29.343864\n",
            "Time step 4, Epoch 314, Batch 1, Loss: -29.343868\n",
            "Time step 4, Epoch 315, Batch 1, Loss: -29.343870\n",
            "Time step 4, Epoch 316, Batch 1, Loss: -29.343872\n",
            "Time step 4, Epoch 317, Batch 1, Loss: -29.343872\n",
            "Time step 4, Epoch 318, Batch 1, Loss: -29.343876\n",
            "Time step 4, Epoch 319, Batch 1, Loss: -29.343880\n",
            "Time step 4, Epoch 320, Batch 1, Loss: -29.343880\n",
            "Time step 4, Epoch 321, Batch 1, Loss: -29.343884\n",
            "Time step 4, Epoch 322, Batch 1, Loss: -29.343885\n",
            "Time step 4, Epoch 323, Batch 1, Loss: -29.343885\n",
            "Time step 4, Epoch 324, Batch 1, Loss: -29.343887\n",
            "Time step 4, Epoch 325, Batch 1, Loss: -29.343893\n",
            "Time step 4, Epoch 326, Batch 1, Loss: -29.343895\n",
            "Time step 4, Epoch 327, Batch 1, Loss: -29.343895\n",
            "Time step 4, Epoch 328, Batch 1, Loss: -29.343899\n",
            "Time step 4, Epoch 329, Batch 1, Loss: -29.343899\n",
            "Time step 4, Epoch 330, Batch 1, Loss: -29.343903\n",
            "Time step 4, Epoch 331, Batch 1, Loss: -29.343903\n",
            "Time step 4, Epoch 332, Batch 1, Loss: -29.343904\n",
            "Time step 4, Epoch 333, Batch 1, Loss: -29.343908\n",
            "Time step 4, Epoch 334, Batch 1, Loss: -29.343910\n",
            "Time step 4, Epoch 335, Batch 1, Loss: -29.343910\n",
            "Time step 4, Epoch 336, Batch 1, Loss: -29.343914\n",
            "Time step 4, Epoch 337, Batch 1, Loss: -29.343916\n",
            "Time step 4, Epoch 338, Batch 1, Loss: -29.343914\n",
            "Time step 4, Epoch 339, Batch 1, Loss: -29.343922\n",
            "Time step 4, Epoch 340, Batch 1, Loss: -29.343922\n",
            "Time step 4, Epoch 341, Batch 1, Loss: -29.343922\n",
            "Time step 4, Epoch 342, Batch 1, Loss: -29.343924\n",
            "Time step 4, Epoch 343, Batch 1, Loss: -29.343925\n",
            "Time step 4, Epoch 344, Batch 1, Loss: -29.343925\n",
            "Time step 4, Epoch 345, Batch 1, Loss: -29.343929\n",
            "Time step 4, Epoch 346, Batch 1, Loss: -29.343933\n",
            "Time step 4, Epoch 347, Batch 1, Loss: -29.343933\n",
            "Time step 4, Epoch 348, Batch 1, Loss: -29.343937\n",
            "Time step 4, Epoch 349, Batch 1, Loss: -29.343937\n",
            "Time step 4, Epoch 350, Batch 1, Loss: -29.343941\n",
            "Time step 4, Epoch 351, Batch 1, Loss: -29.343941\n",
            "Time step 4, Epoch 352, Batch 1, Loss: -29.343943\n",
            "Time step 4, Epoch 353, Batch 1, Loss: -29.343943\n",
            "Time step 4, Epoch 354, Batch 1, Loss: -29.343943\n",
            "Time step 4, Epoch 355, Batch 1, Loss: -29.343948\n",
            "Time step 4, Epoch 356, Batch 1, Loss: -29.343948\n",
            "Time step 4, Epoch 357, Batch 1, Loss: -29.343950\n",
            "Time step 4, Epoch 358, Batch 1, Loss: -29.343950\n",
            "Time step 4, Epoch 359, Batch 1, Loss: -29.343954\n",
            "Time step 4, Epoch 360, Batch 1, Loss: -29.343954\n",
            "Time step 4, Epoch 361, Batch 1, Loss: -29.343956\n",
            "Time step 4, Epoch 362, Batch 1, Loss: -29.343958\n",
            "Time step 4, Epoch 363, Batch 1, Loss: -29.343962\n",
            "Time step 4, Epoch 364, Batch 1, Loss: -29.343960\n",
            "Time step 4, Epoch 365, Batch 1, Loss: -29.343962\n",
            "Time step 4, Epoch 366, Batch 1, Loss: -29.343964\n",
            "Time step 4, Epoch 367, Batch 1, Loss: -29.343966\n",
            "Time step 4, Epoch 368, Batch 1, Loss: -29.343967\n",
            "Time step 4, Epoch 369, Batch 1, Loss: -29.343967\n",
            "Time step 4, Epoch 370, Batch 1, Loss: -29.343971\n",
            "Time step 4, Epoch 371, Batch 1, Loss: -29.343973\n",
            "Time step 4, Epoch 372, Batch 1, Loss: -29.343971\n",
            "Time step 4, Epoch 373, Batch 1, Loss: -29.343975\n",
            "Time step 4, Epoch 374, Batch 1, Loss: -29.343975\n",
            "Time step 4, Epoch 375, Batch 1, Loss: -29.343977\n",
            "Time step 4, Epoch 376, Batch 1, Loss: -29.343977\n",
            "Time step 4, Epoch 377, Batch 1, Loss: -29.343979\n",
            "Time step 4, Epoch 378, Batch 1, Loss: -29.343981\n",
            "Time step 4, Epoch 379, Batch 1, Loss: -29.343983\n",
            "Time step 4, Epoch 380, Batch 1, Loss: -29.343983\n",
            "Time step 4, Epoch 381, Batch 1, Loss: -29.343985\n",
            "Time step 4, Epoch 382, Batch 1, Loss: -29.343987\n",
            "Time step 4, Epoch 383, Batch 1, Loss: -29.343987\n",
            "Time step 4, Epoch 384, Batch 1, Loss: -29.343988\n",
            "Time step 4, Epoch 385, Batch 1, Loss: -29.343990\n",
            "Time step 4, Epoch 386, Batch 1, Loss: -29.343990\n",
            "Time step 4, Epoch 387, Batch 1, Loss: -29.343992\n",
            "Time step 4, Epoch 388, Batch 1, Loss: -29.343994\n",
            "Time step 4, Epoch 389, Batch 1, Loss: -29.343996\n",
            "Time step 4, Epoch 390, Batch 1, Loss: -29.343996\n",
            "Time step 4, Epoch 391, Batch 1, Loss: -29.343998\n",
            "Time step 4, Epoch 392, Batch 1, Loss: -29.343996\n",
            "Time step 4, Epoch 393, Batch 1, Loss: -29.344000\n",
            "Time step 4, Epoch 394, Batch 1, Loss: -29.344000\n",
            "Time step 4, Epoch 395, Batch 1, Loss: -29.344002\n",
            "Time step 4, Epoch 396, Batch 1, Loss: -29.344002\n",
            "Time step 4, Epoch 397, Batch 1, Loss: -29.344006\n",
            "Time step 4, Epoch 398, Batch 1, Loss: -29.344004\n",
            "Time step 4, Epoch 399, Batch 1, Loss: -29.344006\n",
            "Time step 4, Epoch 400, Batch 1, Loss: -29.344007\n",
            "Time step 4, Epoch 401, Batch 1, Loss: -29.344007\n",
            "Time step 4, Epoch 402, Batch 1, Loss: -29.344011\n",
            "Time step 4, Epoch 403, Batch 1, Loss: -29.344009\n",
            "Time step 4, Epoch 404, Batch 1, Loss: -29.344013\n",
            "Time step 4, Epoch 405, Batch 1, Loss: -29.344011\n",
            "Time step 4, Epoch 406, Batch 1, Loss: -29.344013\n",
            "Time step 4, Epoch 407, Batch 1, Loss: -29.344015\n",
            "Time step 4, Epoch 408, Batch 1, Loss: -29.344015\n",
            "Time step 4, Epoch 409, Batch 1, Loss: -29.344017\n",
            "Time step 4, Epoch 410, Batch 1, Loss: -29.344017\n",
            "Time step 4, Epoch 411, Batch 1, Loss: -29.344019\n",
            "Time step 4, Epoch 412, Batch 1, Loss: -29.344021\n",
            "Time step 4, Epoch 413, Batch 1, Loss: -29.344021\n",
            "Time step 4, Epoch 414, Batch 1, Loss: -29.344021\n",
            "Time step 4, Epoch 415, Batch 1, Loss: -29.344023\n",
            "Time step 4, Epoch 416, Batch 1, Loss: -29.344023\n",
            "Time step 4, Epoch 417, Batch 1, Loss: -29.344027\n",
            "Time step 4, Epoch 418, Batch 1, Loss: -29.344025\n",
            "Time step 4, Epoch 419, Batch 1, Loss: -29.344027\n",
            "Time step 4, Epoch 420, Batch 1, Loss: -29.344030\n",
            "Time step 4, Epoch 421, Batch 1, Loss: -29.344027\n",
            "Time step 4, Epoch 422, Batch 1, Loss: -29.344030\n",
            "Time step 4, Epoch 423, Batch 1, Loss: -29.344032\n",
            "Time step 4, Epoch 424, Batch 1, Loss: -29.344030\n",
            "Time step 4, Epoch 425, Batch 1, Loss: -29.344032\n",
            "Time step 4, Epoch 426, Batch 1, Loss: -29.344032\n",
            "Time step 4, Epoch 427, Batch 1, Loss: -29.344034\n",
            "Time step 4, Epoch 428, Batch 1, Loss: -29.344036\n",
            "Time step 4, Epoch 429, Batch 1, Loss: -29.344036\n",
            "Time step 4, Epoch 430, Batch 1, Loss: -29.344036\n",
            "Time step 4, Epoch 431, Batch 1, Loss: -29.344040\n",
            "Time step 4, Epoch 432, Batch 1, Loss: -29.344040\n",
            "Time step 4, Epoch 433, Batch 1, Loss: -29.344042\n",
            "Time step 4, Epoch 434, Batch 1, Loss: -29.344040\n",
            "Time step 4, Epoch 435, Batch 1, Loss: -29.344040\n",
            "Time step 4, Epoch 436, Batch 1, Loss: -29.344044\n",
            "Time step 4, Epoch 437, Batch 1, Loss: -29.344044\n",
            "Time step 4, Epoch 438, Batch 1, Loss: -29.344044\n",
            "Time step 4, Epoch 439, Batch 1, Loss: -29.344044\n",
            "Time step 4, Epoch 440, Batch 1, Loss: -29.344048\n",
            "Time step 4, Epoch 441, Batch 1, Loss: -29.344046\n",
            "Time step 4, Epoch 442, Batch 1, Loss: -29.344049\n",
            "Time step 4, Epoch 443, Batch 1, Loss: -29.344048\n",
            "Time step 4, Epoch 444, Batch 1, Loss: -29.344048\n",
            "Time step 4, Epoch 445, Batch 1, Loss: -29.344051\n",
            "Time step 4, Epoch 446, Batch 1, Loss: -29.344049\n",
            "Time step 4, Epoch 447, Batch 1, Loss: -29.344051\n",
            "Time step 4, Epoch 448, Batch 1, Loss: -29.344055\n",
            "Time step 4, Epoch 449, Batch 1, Loss: -29.344053\n",
            "Time step 4, Epoch 450, Batch 1, Loss: -29.344053\n",
            "Time step 4, Epoch 451, Batch 1, Loss: -29.344053\n",
            "Time step 4, Epoch 452, Batch 1, Loss: -29.344059\n",
            "Time step 4, Epoch 453, Batch 1, Loss: -29.344057\n",
            "Time step 4, Epoch 454, Batch 1, Loss: -29.344057\n",
            "Time step 4, Epoch 455, Batch 1, Loss: -29.344057\n",
            "Time step 4, Epoch 456, Batch 1, Loss: -29.344059\n",
            "Time step 4, Epoch 457, Batch 1, Loss: -29.344057\n",
            "Time step 4, Epoch 458, Batch 1, Loss: -29.344063\n",
            "Time step 4, Epoch 459, Batch 1, Loss: -29.344061\n",
            "Time step 4, Epoch 460, Batch 1, Loss: -29.344063\n",
            "Time step 4, Epoch 461, Batch 1, Loss: -29.344063\n",
            "Time step 4, Epoch 462, Batch 1, Loss: -29.344063\n",
            "Time step 4, Epoch 463, Batch 1, Loss: -29.344063\n",
            "Time step 4, Epoch 464, Batch 1, Loss: -29.344065\n",
            "Time step 4, Epoch 465, Batch 1, Loss: -29.344065\n",
            "Time step 4, Epoch 466, Batch 1, Loss: -29.344063\n",
            "Time step 4, Epoch 467, Batch 1, Loss: -29.344063\n",
            "Time step 4, Epoch 468, Batch 1, Loss: -29.344069\n",
            "Time step 4, Epoch 469, Batch 1, Loss: -29.344069\n",
            "Time step 4, Epoch 470, Batch 1, Loss: -29.344069\n",
            "Time step 4, Epoch 471, Batch 1, Loss: -29.344070\n",
            "Time step 4, Epoch 472, Batch 1, Loss: -29.344070\n",
            "Time step 4, Epoch 473, Batch 1, Loss: -29.344070\n",
            "Time step 4, Epoch 474, Batch 1, Loss: -29.344072\n",
            "Time step 4, Epoch 475, Batch 1, Loss: -29.344072\n",
            "Time step 4, Epoch 476, Batch 1, Loss: -29.344072\n",
            "Time step 4, Epoch 477, Batch 1, Loss: -29.344074\n",
            "Time step 4, Epoch 478, Batch 1, Loss: -29.344072\n",
            "Time step 4, Epoch 479, Batch 1, Loss: -29.344076\n",
            "Time step 4, Epoch 480, Batch 1, Loss: -29.344076\n",
            "Time step 4, Epoch 481, Batch 1, Loss: -29.344076\n",
            "Time step 4, Epoch 482, Batch 1, Loss: -29.344078\n",
            "Time step 4, Epoch 483, Batch 1, Loss: -29.344076\n",
            "Time step 4, Epoch 484, Batch 1, Loss: -29.344078\n",
            "Time step 4, Epoch 485, Batch 1, Loss: -29.344080\n",
            "Time step 4, Epoch 486, Batch 1, Loss: -29.344078\n",
            "Time step 4, Epoch 487, Batch 1, Loss: -29.344078\n",
            "Time step 4, Epoch 488, Batch 1, Loss: -29.344080\n",
            "Time step 4, Epoch 489, Batch 1, Loss: -29.344080\n",
            "Time step 4, Epoch 490, Batch 1, Loss: -29.344082\n",
            "Time step 4, Epoch 491, Batch 1, Loss: -29.344082\n",
            "Time step 4, Epoch 492, Batch 1, Loss: -29.344082\n",
            "Time step 4, Epoch 493, Batch 1, Loss: -29.344080\n",
            "Time step 4, Epoch 494, Batch 1, Loss: -29.344082\n",
            "Time step 4, Epoch 495, Batch 1, Loss: -29.344084\n",
            "Time step 4, Epoch 496, Batch 1, Loss: -29.344082\n",
            "Time step 4, Epoch 497, Batch 1, Loss: -29.344086\n",
            "Time step 4, Epoch 498, Batch 1, Loss: -29.344086\n",
            "Time step 4, Epoch 499, Batch 1, Loss: -29.344086\n",
            "Time step 4, Epoch 500, Batch 1, Loss: -29.344084\n",
            "Training model for time step 3...\n",
            "Time step 3, Epoch 1, Batch 1, Loss: -23.558901\n",
            "Time step 3, Epoch 2, Batch 1, Loss: -23.702681\n",
            "Time step 3, Epoch 3, Batch 1, Loss: -23.840240\n",
            "Time step 3, Epoch 4, Batch 1, Loss: -23.972626\n",
            "Time step 3, Epoch 5, Batch 1, Loss: -24.100904\n",
            "Time step 3, Epoch 6, Batch 1, Loss: -24.226189\n",
            "Time step 3, Epoch 7, Batch 1, Loss: -24.348978\n",
            "Time step 3, Epoch 8, Batch 1, Loss: -24.469734\n",
            "Time step 3, Epoch 9, Batch 1, Loss: -24.589018\n",
            "Time step 3, Epoch 10, Batch 1, Loss: -24.707350\n",
            "Time step 3, Epoch 11, Batch 1, Loss: -24.824833\n",
            "Time step 3, Epoch 12, Batch 1, Loss: -24.941757\n",
            "Time step 3, Epoch 13, Batch 1, Loss: -25.057932\n",
            "Time step 3, Epoch 14, Batch 1, Loss: -25.173929\n",
            "Time step 3, Epoch 15, Batch 1, Loss: -25.290899\n",
            "Time step 3, Epoch 16, Batch 1, Loss: -25.409433\n",
            "Time step 3, Epoch 17, Batch 1, Loss: -25.529560\n",
            "Time step 3, Epoch 18, Batch 1, Loss: -25.651823\n",
            "Time step 3, Epoch 19, Batch 1, Loss: -25.776289\n",
            "Time step 3, Epoch 20, Batch 1, Loss: -25.903248\n",
            "Time step 3, Epoch 21, Batch 1, Loss: -26.032696\n",
            "Time step 3, Epoch 22, Batch 1, Loss: -26.164309\n",
            "Time step 3, Epoch 23, Batch 1, Loss: -26.297916\n",
            "Time step 3, Epoch 24, Batch 1, Loss: -26.433044\n",
            "Time step 3, Epoch 25, Batch 1, Loss: -26.569401\n",
            "Time step 3, Epoch 26, Batch 1, Loss: -26.706442\n",
            "Time step 3, Epoch 27, Batch 1, Loss: -26.843739\n",
            "Time step 3, Epoch 28, Batch 1, Loss: -26.980770\n",
            "Time step 3, Epoch 29, Batch 1, Loss: -27.117060\n",
            "Time step 3, Epoch 30, Batch 1, Loss: -27.252096\n",
            "Time step 3, Epoch 31, Batch 1, Loss: -27.385220\n",
            "Time step 3, Epoch 32, Batch 1, Loss: -27.515789\n",
            "Time step 3, Epoch 33, Batch 1, Loss: -27.643217\n",
            "Time step 3, Epoch 34, Batch 1, Loss: -27.766972\n",
            "Time step 3, Epoch 35, Batch 1, Loss: -27.886517\n",
            "Time step 3, Epoch 36, Batch 1, Loss: -28.001305\n",
            "Time step 3, Epoch 37, Batch 1, Loss: -28.110937\n",
            "Time step 3, Epoch 38, Batch 1, Loss: -28.215048\n",
            "Time step 3, Epoch 39, Batch 1, Loss: -28.313358\n",
            "Time step 3, Epoch 40, Batch 1, Loss: -28.405602\n",
            "Time step 3, Epoch 41, Batch 1, Loss: -28.491739\n",
            "Time step 3, Epoch 42, Batch 1, Loss: -28.571724\n",
            "Time step 3, Epoch 43, Batch 1, Loss: -28.645634\n",
            "Time step 3, Epoch 44, Batch 1, Loss: -28.713606\n",
            "Time step 3, Epoch 45, Batch 1, Loss: -28.775757\n",
            "Time step 3, Epoch 46, Batch 1, Loss: -28.832323\n",
            "Time step 3, Epoch 47, Batch 1, Loss: -28.883646\n",
            "Time step 3, Epoch 48, Batch 1, Loss: -28.930050\n",
            "Time step 3, Epoch 49, Batch 1, Loss: -28.971855\n",
            "Time step 3, Epoch 50, Batch 1, Loss: -29.009380\n",
            "Time step 3, Epoch 51, Batch 1, Loss: -29.042992\n",
            "Time step 3, Epoch 52, Batch 1, Loss: -29.073032\n",
            "Time step 3, Epoch 53, Batch 1, Loss: -29.099834\n",
            "Time step 3, Epoch 54, Batch 1, Loss: -29.123724\n",
            "Time step 3, Epoch 55, Batch 1, Loss: -29.144985\n",
            "Time step 3, Epoch 56, Batch 1, Loss: -29.163883\n",
            "Time step 3, Epoch 57, Batch 1, Loss: -29.180683\n",
            "Time step 3, Epoch 58, Batch 1, Loss: -29.195608\n",
            "Time step 3, Epoch 59, Batch 1, Loss: -29.208878\n",
            "Time step 3, Epoch 60, Batch 1, Loss: -29.220669\n",
            "Time step 3, Epoch 61, Batch 1, Loss: -29.231165\n",
            "Time step 3, Epoch 62, Batch 1, Loss: -29.240509\n",
            "Time step 3, Epoch 63, Batch 1, Loss: -29.248840\n",
            "Time step 3, Epoch 64, Batch 1, Loss: -29.256275\n",
            "Time step 3, Epoch 65, Batch 1, Loss: -29.262918\n",
            "Time step 3, Epoch 66, Batch 1, Loss: -29.268867\n",
            "Time step 3, Epoch 67, Batch 1, Loss: -29.274204\n",
            "Time step 3, Epoch 68, Batch 1, Loss: -29.278997\n",
            "Time step 3, Epoch 69, Batch 1, Loss: -29.283312\n",
            "Time step 3, Epoch 70, Batch 1, Loss: -29.287203\n",
            "Time step 3, Epoch 71, Batch 1, Loss: -29.290720\n",
            "Time step 3, Epoch 72, Batch 1, Loss: -29.293905\n",
            "Time step 3, Epoch 73, Batch 1, Loss: -29.296791\n",
            "Time step 3, Epoch 74, Batch 1, Loss: -29.299419\n",
            "Time step 3, Epoch 75, Batch 1, Loss: -29.301811\n",
            "Time step 3, Epoch 76, Batch 1, Loss: -29.303995\n",
            "Time step 3, Epoch 77, Batch 1, Loss: -29.305992\n",
            "Time step 3, Epoch 78, Batch 1, Loss: -29.307823\n",
            "Time step 3, Epoch 79, Batch 1, Loss: -29.309502\n",
            "Time step 3, Epoch 80, Batch 1, Loss: -29.311050\n",
            "Time step 3, Epoch 81, Batch 1, Loss: -29.312477\n",
            "Time step 3, Epoch 82, Batch 1, Loss: -29.313795\n",
            "Time step 3, Epoch 83, Batch 1, Loss: -29.315016\n",
            "Time step 3, Epoch 84, Batch 1, Loss: -29.316143\n",
            "Time step 3, Epoch 85, Batch 1, Loss: -29.317194\n",
            "Time step 3, Epoch 86, Batch 1, Loss: -29.318174\n",
            "Time step 3, Epoch 87, Batch 1, Loss: -29.319090\n",
            "Time step 3, Epoch 88, Batch 1, Loss: -29.319944\n",
            "Time step 3, Epoch 89, Batch 1, Loss: -29.320744\n",
            "Time step 3, Epoch 90, Batch 1, Loss: -29.321491\n",
            "Time step 3, Epoch 91, Batch 1, Loss: -29.322197\n",
            "Time step 3, Epoch 92, Batch 1, Loss: -29.322859\n",
            "Time step 3, Epoch 93, Batch 1, Loss: -29.323488\n",
            "Time step 3, Epoch 94, Batch 1, Loss: -29.324074\n",
            "Time step 3, Epoch 95, Batch 1, Loss: -29.324635\n",
            "Time step 3, Epoch 96, Batch 1, Loss: -29.325161\n",
            "Time step 3, Epoch 97, Batch 1, Loss: -29.325666\n",
            "Time step 3, Epoch 98, Batch 1, Loss: -29.326139\n",
            "Time step 3, Epoch 99, Batch 1, Loss: -29.326593\n",
            "Time step 3, Epoch 100, Batch 1, Loss: -29.327026\n",
            "Time step 3, Epoch 101, Batch 1, Loss: -29.327438\n",
            "Time step 3, Epoch 102, Batch 1, Loss: -29.327831\n",
            "Time step 3, Epoch 103, Batch 1, Loss: -29.328207\n",
            "Time step 3, Epoch 104, Batch 1, Loss: -29.328568\n",
            "Time step 3, Epoch 105, Batch 1, Loss: -29.328911\n",
            "Time step 3, Epoch 106, Batch 1, Loss: -29.329243\n",
            "Time step 3, Epoch 107, Batch 1, Loss: -29.329557\n",
            "Time step 3, Epoch 108, Batch 1, Loss: -29.329865\n",
            "Time step 3, Epoch 109, Batch 1, Loss: -29.330156\n",
            "Time step 3, Epoch 110, Batch 1, Loss: -29.330442\n",
            "Time step 3, Epoch 111, Batch 1, Loss: -29.330713\n",
            "Time step 3, Epoch 112, Batch 1, Loss: -29.330976\n",
            "Time step 3, Epoch 113, Batch 1, Loss: -29.331232\n",
            "Time step 3, Epoch 114, Batch 1, Loss: -29.331476\n",
            "Time step 3, Epoch 115, Batch 1, Loss: -29.331713\n",
            "Time step 3, Epoch 116, Batch 1, Loss: -29.331944\n",
            "Time step 3, Epoch 117, Batch 1, Loss: -29.332167\n",
            "Time step 3, Epoch 118, Batch 1, Loss: -29.332380\n",
            "Time step 3, Epoch 119, Batch 1, Loss: -29.332592\n",
            "Time step 3, Epoch 120, Batch 1, Loss: -29.332794\n",
            "Time step 3, Epoch 121, Batch 1, Loss: -29.332991\n",
            "Time step 3, Epoch 122, Batch 1, Loss: -29.333183\n",
            "Time step 3, Epoch 123, Batch 1, Loss: -29.333366\n",
            "Time step 3, Epoch 124, Batch 1, Loss: -29.333549\n",
            "Time step 3, Epoch 125, Batch 1, Loss: -29.333723\n",
            "Time step 3, Epoch 126, Batch 1, Loss: -29.333893\n",
            "Time step 3, Epoch 127, Batch 1, Loss: -29.334061\n",
            "Time step 3, Epoch 128, Batch 1, Loss: -29.334225\n",
            "Time step 3, Epoch 129, Batch 1, Loss: -29.334381\n",
            "Time step 3, Epoch 130, Batch 1, Loss: -29.334534\n",
            "Time step 3, Epoch 131, Batch 1, Loss: -29.334686\n",
            "Time step 3, Epoch 132, Batch 1, Loss: -29.334831\n",
            "Time step 3, Epoch 133, Batch 1, Loss: -29.334972\n",
            "Time step 3, Epoch 134, Batch 1, Loss: -29.335114\n",
            "Time step 3, Epoch 135, Batch 1, Loss: -29.335251\n",
            "Time step 3, Epoch 136, Batch 1, Loss: -29.335381\n",
            "Time step 3, Epoch 137, Batch 1, Loss: -29.335512\n",
            "Time step 3, Epoch 138, Batch 1, Loss: -29.335638\n",
            "Time step 3, Epoch 139, Batch 1, Loss: -29.335764\n",
            "Time step 3, Epoch 140, Batch 1, Loss: -29.335884\n",
            "Time step 3, Epoch 141, Batch 1, Loss: -29.336004\n",
            "Time step 3, Epoch 142, Batch 1, Loss: -29.336121\n",
            "Time step 3, Epoch 143, Batch 1, Loss: -29.336233\n",
            "Time step 3, Epoch 144, Batch 1, Loss: -29.336344\n",
            "Time step 3, Epoch 145, Batch 1, Loss: -29.336451\n",
            "Time step 3, Epoch 146, Batch 1, Loss: -29.336559\n",
            "Time step 3, Epoch 147, Batch 1, Loss: -29.336664\n",
            "Time step 3, Epoch 148, Batch 1, Loss: -29.336765\n",
            "Time step 3, Epoch 149, Batch 1, Loss: -29.336866\n",
            "Time step 3, Epoch 150, Batch 1, Loss: -29.336962\n",
            "Time step 3, Epoch 151, Batch 1, Loss: -29.337059\n",
            "Time step 3, Epoch 152, Batch 1, Loss: -29.337152\n",
            "Time step 3, Epoch 153, Batch 1, Loss: -29.337244\n",
            "Time step 3, Epoch 154, Batch 1, Loss: -29.337336\n",
            "Time step 3, Epoch 155, Batch 1, Loss: -29.337425\n",
            "Time step 3, Epoch 156, Batch 1, Loss: -29.337509\n",
            "Time step 3, Epoch 157, Batch 1, Loss: -29.337599\n",
            "Time step 3, Epoch 158, Batch 1, Loss: -29.337681\n",
            "Time step 3, Epoch 159, Batch 1, Loss: -29.337761\n",
            "Time step 3, Epoch 160, Batch 1, Loss: -29.337845\n",
            "Time step 3, Epoch 161, Batch 1, Loss: -29.337921\n",
            "Time step 3, Epoch 162, Batch 1, Loss: -29.338001\n",
            "Time step 3, Epoch 163, Batch 1, Loss: -29.338078\n",
            "Time step 3, Epoch 164, Batch 1, Loss: -29.338152\n",
            "Time step 3, Epoch 165, Batch 1, Loss: -29.338223\n",
            "Time step 3, Epoch 166, Batch 1, Loss: -29.338295\n",
            "Time step 3, Epoch 167, Batch 1, Loss: -29.338367\n",
            "Time step 3, Epoch 168, Batch 1, Loss: -29.338434\n",
            "Time step 3, Epoch 169, Batch 1, Loss: -29.338505\n",
            "Time step 3, Epoch 170, Batch 1, Loss: -29.338572\n",
            "Time step 3, Epoch 171, Batch 1, Loss: -29.338638\n",
            "Time step 3, Epoch 172, Batch 1, Loss: -29.338703\n",
            "Time step 3, Epoch 173, Batch 1, Loss: -29.338766\n",
            "Time step 3, Epoch 174, Batch 1, Loss: -29.338831\n",
            "Time step 3, Epoch 175, Batch 1, Loss: -29.338890\n",
            "Time step 3, Epoch 176, Batch 1, Loss: -29.338951\n",
            "Time step 3, Epoch 177, Batch 1, Loss: -29.339010\n",
            "Time step 3, Epoch 178, Batch 1, Loss: -29.339069\n",
            "Time step 3, Epoch 179, Batch 1, Loss: -29.339125\n",
            "Time step 3, Epoch 180, Batch 1, Loss: -29.339184\n",
            "Time step 3, Epoch 181, Batch 1, Loss: -29.339237\n",
            "Time step 3, Epoch 182, Batch 1, Loss: -29.339293\n",
            "Time step 3, Epoch 183, Batch 1, Loss: -29.339348\n",
            "Time step 3, Epoch 184, Batch 1, Loss: -29.339399\n",
            "Time step 3, Epoch 185, Batch 1, Loss: -29.339453\n",
            "Time step 3, Epoch 186, Batch 1, Loss: -29.339504\n",
            "Time step 3, Epoch 187, Batch 1, Loss: -29.339554\n",
            "Time step 3, Epoch 188, Batch 1, Loss: -29.339603\n",
            "Time step 3, Epoch 189, Batch 1, Loss: -29.339655\n",
            "Time step 3, Epoch 190, Batch 1, Loss: -29.339701\n",
            "Time step 3, Epoch 191, Batch 1, Loss: -29.339750\n",
            "Time step 3, Epoch 192, Batch 1, Loss: -29.339794\n",
            "Time step 3, Epoch 193, Batch 1, Loss: -29.339842\n",
            "Time step 3, Epoch 194, Batch 1, Loss: -29.339890\n",
            "Time step 3, Epoch 195, Batch 1, Loss: -29.339931\n",
            "Time step 3, Epoch 196, Batch 1, Loss: -29.339973\n",
            "Time step 3, Epoch 197, Batch 1, Loss: -29.340017\n",
            "Time step 3, Epoch 198, Batch 1, Loss: -29.340059\n",
            "Time step 3, Epoch 199, Batch 1, Loss: -29.340101\n",
            "Time step 3, Epoch 200, Batch 1, Loss: -29.340145\n",
            "Time step 3, Epoch 201, Batch 1, Loss: -29.340185\n",
            "Time step 3, Epoch 202, Batch 1, Loss: -29.340223\n",
            "Time step 3, Epoch 203, Batch 1, Loss: -29.340263\n",
            "Time step 3, Epoch 204, Batch 1, Loss: -29.340305\n",
            "Time step 3, Epoch 205, Batch 1, Loss: -29.340340\n",
            "Time step 3, Epoch 206, Batch 1, Loss: -29.340380\n",
            "Time step 3, Epoch 207, Batch 1, Loss: -29.340414\n",
            "Time step 3, Epoch 208, Batch 1, Loss: -29.340452\n",
            "Time step 3, Epoch 209, Batch 1, Loss: -29.340488\n",
            "Time step 3, Epoch 210, Batch 1, Loss: -29.340525\n",
            "Time step 3, Epoch 211, Batch 1, Loss: -29.340561\n",
            "Time step 3, Epoch 212, Batch 1, Loss: -29.340595\n",
            "Time step 3, Epoch 213, Batch 1, Loss: -29.340628\n",
            "Time step 3, Epoch 214, Batch 1, Loss: -29.340664\n",
            "Time step 3, Epoch 215, Batch 1, Loss: -29.340696\n",
            "Time step 3, Epoch 216, Batch 1, Loss: -29.340727\n",
            "Time step 3, Epoch 217, Batch 1, Loss: -29.340761\n",
            "Time step 3, Epoch 218, Batch 1, Loss: -29.340792\n",
            "Time step 3, Epoch 219, Batch 1, Loss: -29.340824\n",
            "Time step 3, Epoch 220, Batch 1, Loss: -29.340858\n",
            "Time step 3, Epoch 221, Batch 1, Loss: -29.340887\n",
            "Time step 3, Epoch 222, Batch 1, Loss: -29.340916\n",
            "Time step 3, Epoch 223, Batch 1, Loss: -29.340946\n",
            "Time step 3, Epoch 224, Batch 1, Loss: -29.340979\n",
            "Time step 3, Epoch 225, Batch 1, Loss: -29.341005\n",
            "Time step 3, Epoch 226, Batch 1, Loss: -29.341032\n",
            "Time step 3, Epoch 227, Batch 1, Loss: -29.341063\n",
            "Time step 3, Epoch 228, Batch 1, Loss: -29.341091\n",
            "Time step 3, Epoch 229, Batch 1, Loss: -29.341118\n",
            "Time step 3, Epoch 230, Batch 1, Loss: -29.341148\n",
            "Time step 3, Epoch 231, Batch 1, Loss: -29.341173\n",
            "Time step 3, Epoch 232, Batch 1, Loss: -29.341198\n",
            "Time step 3, Epoch 233, Batch 1, Loss: -29.341225\n",
            "Time step 3, Epoch 234, Batch 1, Loss: -29.341253\n",
            "Time step 3, Epoch 235, Batch 1, Loss: -29.341278\n",
            "Time step 3, Epoch 236, Batch 1, Loss: -29.341301\n",
            "Time step 3, Epoch 237, Batch 1, Loss: -29.341324\n",
            "Time step 3, Epoch 238, Batch 1, Loss: -29.341351\n",
            "Time step 3, Epoch 239, Batch 1, Loss: -29.341375\n",
            "Time step 3, Epoch 240, Batch 1, Loss: -29.341400\n",
            "Time step 3, Epoch 241, Batch 1, Loss: -29.341421\n",
            "Time step 3, Epoch 242, Batch 1, Loss: -29.341446\n",
            "Time step 3, Epoch 243, Batch 1, Loss: -29.341469\n",
            "Time step 3, Epoch 244, Batch 1, Loss: -29.341492\n",
            "Time step 3, Epoch 245, Batch 1, Loss: -29.341515\n",
            "Time step 3, Epoch 246, Batch 1, Loss: -29.341536\n",
            "Time step 3, Epoch 247, Batch 1, Loss: -29.341557\n",
            "Time step 3, Epoch 248, Batch 1, Loss: -29.341579\n",
            "Time step 3, Epoch 249, Batch 1, Loss: -29.341602\n",
            "Time step 3, Epoch 250, Batch 1, Loss: -29.341623\n",
            "Time step 3, Epoch 251, Batch 1, Loss: -29.341642\n",
            "Time step 3, Epoch 252, Batch 1, Loss: -29.341663\n",
            "Time step 3, Epoch 253, Batch 1, Loss: -29.341682\n",
            "Time step 3, Epoch 254, Batch 1, Loss: -29.341705\n",
            "Time step 3, Epoch 255, Batch 1, Loss: -29.341724\n",
            "Time step 3, Epoch 256, Batch 1, Loss: -29.341743\n",
            "Time step 3, Epoch 257, Batch 1, Loss: -29.341763\n",
            "Time step 3, Epoch 258, Batch 1, Loss: -29.341785\n",
            "Time step 3, Epoch 259, Batch 1, Loss: -29.341805\n",
            "Time step 3, Epoch 260, Batch 1, Loss: -29.341822\n",
            "Time step 3, Epoch 261, Batch 1, Loss: -29.341839\n",
            "Time step 3, Epoch 262, Batch 1, Loss: -29.341856\n",
            "Time step 3, Epoch 263, Batch 1, Loss: -29.341877\n",
            "Time step 3, Epoch 264, Batch 1, Loss: -29.341896\n",
            "Time step 3, Epoch 265, Batch 1, Loss: -29.341915\n",
            "Time step 3, Epoch 266, Batch 1, Loss: -29.341928\n",
            "Time step 3, Epoch 267, Batch 1, Loss: -29.341948\n",
            "Time step 3, Epoch 268, Batch 1, Loss: -29.341963\n",
            "Time step 3, Epoch 269, Batch 1, Loss: -29.341982\n",
            "Time step 3, Epoch 270, Batch 1, Loss: -29.341999\n",
            "Time step 3, Epoch 271, Batch 1, Loss: -29.342014\n",
            "Time step 3, Epoch 272, Batch 1, Loss: -29.342031\n",
            "Time step 3, Epoch 273, Batch 1, Loss: -29.342047\n",
            "Time step 3, Epoch 274, Batch 1, Loss: -29.342064\n",
            "Time step 3, Epoch 275, Batch 1, Loss: -29.342081\n",
            "Time step 3, Epoch 276, Batch 1, Loss: -29.342094\n",
            "Time step 3, Epoch 277, Batch 1, Loss: -29.342110\n",
            "Time step 3, Epoch 278, Batch 1, Loss: -29.342125\n",
            "Time step 3, Epoch 279, Batch 1, Loss: -29.342144\n",
            "Time step 3, Epoch 280, Batch 1, Loss: -29.342157\n",
            "Time step 3, Epoch 281, Batch 1, Loss: -29.342171\n",
            "Time step 3, Epoch 282, Batch 1, Loss: -29.342188\n",
            "Time step 3, Epoch 283, Batch 1, Loss: -29.342201\n",
            "Time step 3, Epoch 284, Batch 1, Loss: -29.342216\n",
            "Time step 3, Epoch 285, Batch 1, Loss: -29.342228\n",
            "Time step 3, Epoch 286, Batch 1, Loss: -29.342245\n",
            "Time step 3, Epoch 287, Batch 1, Loss: -29.342255\n",
            "Time step 3, Epoch 288, Batch 1, Loss: -29.342272\n",
            "Time step 3, Epoch 289, Batch 1, Loss: -29.342285\n",
            "Time step 3, Epoch 290, Batch 1, Loss: -29.342299\n",
            "Time step 3, Epoch 291, Batch 1, Loss: -29.342312\n",
            "Time step 3, Epoch 292, Batch 1, Loss: -29.342325\n",
            "Time step 3, Epoch 293, Batch 1, Loss: -29.342339\n",
            "Time step 3, Epoch 294, Batch 1, Loss: -29.342352\n",
            "Time step 3, Epoch 295, Batch 1, Loss: -29.342365\n",
            "Time step 3, Epoch 296, Batch 1, Loss: -29.342377\n",
            "Time step 3, Epoch 297, Batch 1, Loss: -29.342388\n",
            "Time step 3, Epoch 298, Batch 1, Loss: -29.342403\n",
            "Time step 3, Epoch 299, Batch 1, Loss: -29.342415\n",
            "Time step 3, Epoch 300, Batch 1, Loss: -29.342426\n",
            "Time step 3, Epoch 301, Batch 1, Loss: -29.342440\n",
            "Time step 3, Epoch 302, Batch 1, Loss: -29.342449\n",
            "Time step 3, Epoch 303, Batch 1, Loss: -29.342463\n",
            "Time step 3, Epoch 304, Batch 1, Loss: -29.342476\n",
            "Time step 3, Epoch 305, Batch 1, Loss: -29.342485\n",
            "Time step 3, Epoch 306, Batch 1, Loss: -29.342497\n",
            "Time step 3, Epoch 307, Batch 1, Loss: -29.342508\n",
            "Time step 3, Epoch 308, Batch 1, Loss: -29.342520\n",
            "Time step 3, Epoch 309, Batch 1, Loss: -29.342533\n",
            "Time step 3, Epoch 310, Batch 1, Loss: -29.342545\n",
            "Time step 3, Epoch 311, Batch 1, Loss: -29.342552\n",
            "Time step 3, Epoch 312, Batch 1, Loss: -29.342564\n",
            "Time step 3, Epoch 313, Batch 1, Loss: -29.342575\n",
            "Time step 3, Epoch 314, Batch 1, Loss: -29.342587\n",
            "Time step 3, Epoch 315, Batch 1, Loss: -29.342598\n",
            "Time step 3, Epoch 316, Batch 1, Loss: -29.342606\n",
            "Time step 3, Epoch 317, Batch 1, Loss: -29.342617\n",
            "Time step 3, Epoch 318, Batch 1, Loss: -29.342628\n",
            "Time step 3, Epoch 319, Batch 1, Loss: -29.342640\n",
            "Time step 3, Epoch 320, Batch 1, Loss: -29.342648\n",
            "Time step 3, Epoch 321, Batch 1, Loss: -29.342659\n",
            "Time step 3, Epoch 322, Batch 1, Loss: -29.342670\n",
            "Time step 3, Epoch 323, Batch 1, Loss: -29.342678\n",
            "Time step 3, Epoch 324, Batch 1, Loss: -29.342688\n",
            "Time step 3, Epoch 325, Batch 1, Loss: -29.342699\n",
            "Time step 3, Epoch 326, Batch 1, Loss: -29.342707\n",
            "Time step 3, Epoch 327, Batch 1, Loss: -29.342716\n",
            "Time step 3, Epoch 328, Batch 1, Loss: -29.342724\n",
            "Time step 3, Epoch 329, Batch 1, Loss: -29.342733\n",
            "Time step 3, Epoch 330, Batch 1, Loss: -29.342745\n",
            "Time step 3, Epoch 331, Batch 1, Loss: -29.342754\n",
            "Time step 3, Epoch 332, Batch 1, Loss: -29.342764\n",
            "Time step 3, Epoch 333, Batch 1, Loss: -29.342773\n",
            "Time step 3, Epoch 334, Batch 1, Loss: -29.342777\n",
            "Time step 3, Epoch 335, Batch 1, Loss: -29.342791\n",
            "Time step 3, Epoch 336, Batch 1, Loss: -29.342796\n",
            "Time step 3, Epoch 337, Batch 1, Loss: -29.342806\n",
            "Time step 3, Epoch 338, Batch 1, Loss: -29.342815\n",
            "Time step 3, Epoch 339, Batch 1, Loss: -29.342823\n",
            "Time step 3, Epoch 340, Batch 1, Loss: -29.342833\n",
            "Time step 3, Epoch 341, Batch 1, Loss: -29.342842\n",
            "Time step 3, Epoch 342, Batch 1, Loss: -29.342848\n",
            "Time step 3, Epoch 343, Batch 1, Loss: -29.342857\n",
            "Time step 3, Epoch 344, Batch 1, Loss: -29.342863\n",
            "Time step 3, Epoch 345, Batch 1, Loss: -29.342873\n",
            "Time step 3, Epoch 346, Batch 1, Loss: -29.342882\n",
            "Time step 3, Epoch 347, Batch 1, Loss: -29.342890\n",
            "Time step 3, Epoch 348, Batch 1, Loss: -29.342897\n",
            "Time step 3, Epoch 349, Batch 1, Loss: -29.342903\n",
            "Time step 3, Epoch 350, Batch 1, Loss: -29.342913\n",
            "Time step 3, Epoch 351, Batch 1, Loss: -29.342920\n",
            "Time step 3, Epoch 352, Batch 1, Loss: -29.342926\n",
            "Time step 3, Epoch 353, Batch 1, Loss: -29.342934\n",
            "Time step 3, Epoch 354, Batch 1, Loss: -29.342941\n",
            "Time step 3, Epoch 355, Batch 1, Loss: -29.342949\n",
            "Time step 3, Epoch 356, Batch 1, Loss: -29.342957\n",
            "Time step 3, Epoch 357, Batch 1, Loss: -29.342966\n",
            "Time step 3, Epoch 358, Batch 1, Loss: -29.342972\n",
            "Time step 3, Epoch 359, Batch 1, Loss: -29.342979\n",
            "Time step 3, Epoch 360, Batch 1, Loss: -29.342985\n",
            "Time step 3, Epoch 361, Batch 1, Loss: -29.342993\n",
            "Time step 3, Epoch 362, Batch 1, Loss: -29.342999\n",
            "Time step 3, Epoch 363, Batch 1, Loss: -29.343006\n",
            "Time step 3, Epoch 364, Batch 1, Loss: -29.343012\n",
            "Time step 3, Epoch 365, Batch 1, Loss: -29.343021\n",
            "Time step 3, Epoch 366, Batch 1, Loss: -29.343025\n",
            "Time step 3, Epoch 367, Batch 1, Loss: -29.343035\n",
            "Time step 3, Epoch 368, Batch 1, Loss: -29.343039\n",
            "Time step 3, Epoch 369, Batch 1, Loss: -29.343048\n",
            "Time step 3, Epoch 370, Batch 1, Loss: -29.343054\n",
            "Time step 3, Epoch 371, Batch 1, Loss: -29.343060\n",
            "Time step 3, Epoch 372, Batch 1, Loss: -29.343067\n",
            "Time step 3, Epoch 373, Batch 1, Loss: -29.343071\n",
            "Time step 3, Epoch 374, Batch 1, Loss: -29.343077\n",
            "Time step 3, Epoch 375, Batch 1, Loss: -29.343084\n",
            "Time step 3, Epoch 376, Batch 1, Loss: -29.343090\n",
            "Time step 3, Epoch 377, Batch 1, Loss: -29.343098\n",
            "Time step 3, Epoch 378, Batch 1, Loss: -29.343102\n",
            "Time step 3, Epoch 379, Batch 1, Loss: -29.343109\n",
            "Time step 3, Epoch 380, Batch 1, Loss: -29.343117\n",
            "Time step 3, Epoch 381, Batch 1, Loss: -29.343121\n",
            "Time step 3, Epoch 382, Batch 1, Loss: -29.343128\n",
            "Time step 3, Epoch 383, Batch 1, Loss: -29.343134\n",
            "Time step 3, Epoch 384, Batch 1, Loss: -29.343138\n",
            "Time step 3, Epoch 385, Batch 1, Loss: -29.343147\n",
            "Time step 3, Epoch 386, Batch 1, Loss: -29.343151\n",
            "Time step 3, Epoch 387, Batch 1, Loss: -29.343155\n",
            "Time step 3, Epoch 388, Batch 1, Loss: -29.343163\n",
            "Time step 3, Epoch 389, Batch 1, Loss: -29.343168\n",
            "Time step 3, Epoch 390, Batch 1, Loss: -29.343172\n",
            "Time step 3, Epoch 391, Batch 1, Loss: -29.343178\n",
            "Time step 3, Epoch 392, Batch 1, Loss: -29.343184\n",
            "Time step 3, Epoch 393, Batch 1, Loss: -29.343189\n",
            "Time step 3, Epoch 394, Batch 1, Loss: -29.343195\n",
            "Time step 3, Epoch 395, Batch 1, Loss: -29.343197\n",
            "Time step 3, Epoch 396, Batch 1, Loss: -29.343204\n",
            "Time step 3, Epoch 397, Batch 1, Loss: -29.343210\n",
            "Time step 3, Epoch 398, Batch 1, Loss: -29.343216\n",
            "Time step 3, Epoch 399, Batch 1, Loss: -29.343220\n",
            "Time step 3, Epoch 400, Batch 1, Loss: -29.343227\n",
            "Time step 3, Epoch 401, Batch 1, Loss: -29.343231\n",
            "Time step 3, Epoch 402, Batch 1, Loss: -29.343239\n",
            "Time step 3, Epoch 403, Batch 1, Loss: -29.343239\n",
            "Time step 3, Epoch 404, Batch 1, Loss: -29.343246\n",
            "Time step 3, Epoch 405, Batch 1, Loss: -29.343250\n",
            "Time step 3, Epoch 406, Batch 1, Loss: -29.343258\n",
            "Time step 3, Epoch 407, Batch 1, Loss: -29.343260\n",
            "Time step 3, Epoch 408, Batch 1, Loss: -29.343266\n",
            "Time step 3, Epoch 409, Batch 1, Loss: -29.343271\n",
            "Time step 3, Epoch 410, Batch 1, Loss: -29.343275\n",
            "Time step 3, Epoch 411, Batch 1, Loss: -29.343281\n",
            "Time step 3, Epoch 412, Batch 1, Loss: -29.343285\n",
            "Time step 3, Epoch 413, Batch 1, Loss: -29.343287\n",
            "Time step 3, Epoch 414, Batch 1, Loss: -29.343296\n",
            "Time step 3, Epoch 415, Batch 1, Loss: -29.343300\n",
            "Time step 3, Epoch 416, Batch 1, Loss: -29.343304\n",
            "Time step 3, Epoch 417, Batch 1, Loss: -29.343309\n",
            "Time step 3, Epoch 418, Batch 1, Loss: -29.343313\n",
            "Time step 3, Epoch 419, Batch 1, Loss: -29.343317\n",
            "Time step 3, Epoch 420, Batch 1, Loss: -29.343321\n",
            "Time step 3, Epoch 421, Batch 1, Loss: -29.343325\n",
            "Time step 3, Epoch 422, Batch 1, Loss: -29.343330\n",
            "Time step 3, Epoch 423, Batch 1, Loss: -29.343334\n",
            "Time step 3, Epoch 424, Batch 1, Loss: -29.343340\n",
            "Time step 3, Epoch 425, Batch 1, Loss: -29.343344\n",
            "Time step 3, Epoch 426, Batch 1, Loss: -29.343349\n",
            "Time step 3, Epoch 427, Batch 1, Loss: -29.343351\n",
            "Time step 3, Epoch 428, Batch 1, Loss: -29.343355\n",
            "Time step 3, Epoch 429, Batch 1, Loss: -29.343359\n",
            "Time step 3, Epoch 430, Batch 1, Loss: -29.343365\n",
            "Time step 3, Epoch 431, Batch 1, Loss: -29.343369\n",
            "Time step 3, Epoch 432, Batch 1, Loss: -29.343372\n",
            "Time step 3, Epoch 433, Batch 1, Loss: -29.343376\n",
            "Time step 3, Epoch 434, Batch 1, Loss: -29.343380\n",
            "Time step 3, Epoch 435, Batch 1, Loss: -29.343384\n",
            "Time step 3, Epoch 436, Batch 1, Loss: -29.343390\n",
            "Time step 3, Epoch 437, Batch 1, Loss: -29.343390\n",
            "Time step 3, Epoch 438, Batch 1, Loss: -29.343395\n",
            "Time step 3, Epoch 439, Batch 1, Loss: -29.343401\n",
            "Time step 3, Epoch 440, Batch 1, Loss: -29.343403\n",
            "Time step 3, Epoch 441, Batch 1, Loss: -29.343409\n",
            "Time step 3, Epoch 442, Batch 1, Loss: -29.343410\n",
            "Time step 3, Epoch 443, Batch 1, Loss: -29.343414\n",
            "Time step 3, Epoch 444, Batch 1, Loss: -29.343418\n",
            "Time step 3, Epoch 445, Batch 1, Loss: -29.343424\n",
            "Time step 3, Epoch 446, Batch 1, Loss: -29.343428\n",
            "Time step 3, Epoch 447, Batch 1, Loss: -29.343430\n",
            "Time step 3, Epoch 448, Batch 1, Loss: -29.343435\n",
            "Time step 3, Epoch 449, Batch 1, Loss: -29.343437\n",
            "Time step 3, Epoch 450, Batch 1, Loss: -29.343441\n",
            "Time step 3, Epoch 451, Batch 1, Loss: -29.343445\n",
            "Time step 3, Epoch 452, Batch 1, Loss: -29.343449\n",
            "Time step 3, Epoch 453, Batch 1, Loss: -29.343452\n",
            "Time step 3, Epoch 454, Batch 1, Loss: -29.343456\n",
            "Time step 3, Epoch 455, Batch 1, Loss: -29.343458\n",
            "Time step 3, Epoch 456, Batch 1, Loss: -29.343460\n",
            "Time step 3, Epoch 457, Batch 1, Loss: -29.343468\n",
            "Time step 3, Epoch 458, Batch 1, Loss: -29.343472\n",
            "Time step 3, Epoch 459, Batch 1, Loss: -29.343473\n",
            "Time step 3, Epoch 460, Batch 1, Loss: -29.343475\n",
            "Time step 3, Epoch 461, Batch 1, Loss: -29.343479\n",
            "Time step 3, Epoch 462, Batch 1, Loss: -29.343483\n",
            "Time step 3, Epoch 463, Batch 1, Loss: -29.343487\n",
            "Time step 3, Epoch 464, Batch 1, Loss: -29.343491\n",
            "Time step 3, Epoch 465, Batch 1, Loss: -29.343493\n",
            "Time step 3, Epoch 466, Batch 1, Loss: -29.343494\n",
            "Time step 3, Epoch 467, Batch 1, Loss: -29.343502\n",
            "Time step 3, Epoch 468, Batch 1, Loss: -29.343502\n",
            "Time step 3, Epoch 469, Batch 1, Loss: -29.343506\n",
            "Time step 3, Epoch 470, Batch 1, Loss: -29.343510\n",
            "Time step 3, Epoch 471, Batch 1, Loss: -29.343513\n",
            "Time step 3, Epoch 472, Batch 1, Loss: -29.343515\n",
            "Time step 3, Epoch 473, Batch 1, Loss: -29.343517\n",
            "Time step 3, Epoch 474, Batch 1, Loss: -29.343519\n",
            "Time step 3, Epoch 475, Batch 1, Loss: -29.343525\n",
            "Time step 3, Epoch 476, Batch 1, Loss: -29.343529\n",
            "Time step 3, Epoch 477, Batch 1, Loss: -29.343529\n",
            "Time step 3, Epoch 478, Batch 1, Loss: -29.343536\n",
            "Time step 3, Epoch 479, Batch 1, Loss: -29.343536\n",
            "Time step 3, Epoch 480, Batch 1, Loss: -29.343536\n",
            "Time step 3, Epoch 481, Batch 1, Loss: -29.343544\n",
            "Time step 3, Epoch 482, Batch 1, Loss: -29.343546\n",
            "Time step 3, Epoch 483, Batch 1, Loss: -29.343548\n",
            "Time step 3, Epoch 484, Batch 1, Loss: -29.343552\n",
            "Time step 3, Epoch 485, Batch 1, Loss: -29.343555\n",
            "Time step 3, Epoch 486, Batch 1, Loss: -29.343555\n",
            "Time step 3, Epoch 487, Batch 1, Loss: -29.343559\n",
            "Time step 3, Epoch 488, Batch 1, Loss: -29.343563\n",
            "Time step 3, Epoch 489, Batch 1, Loss: -29.343565\n",
            "Time step 3, Epoch 490, Batch 1, Loss: -29.343569\n",
            "Time step 3, Epoch 491, Batch 1, Loss: -29.343569\n",
            "Time step 3, Epoch 492, Batch 1, Loss: -29.343575\n",
            "Time step 3, Epoch 493, Batch 1, Loss: -29.343575\n",
            "Time step 3, Epoch 494, Batch 1, Loss: -29.343578\n",
            "Time step 3, Epoch 495, Batch 1, Loss: -29.343580\n",
            "Time step 3, Epoch 496, Batch 1, Loss: -29.343584\n",
            "Time step 3, Epoch 497, Batch 1, Loss: -29.343586\n",
            "Time step 3, Epoch 498, Batch 1, Loss: -29.343588\n",
            "Time step 3, Epoch 499, Batch 1, Loss: -29.343592\n",
            "Time step 3, Epoch 500, Batch 1, Loss: -29.343594\n",
            "Training model for time step 2...\n",
            "Time step 2, Epoch 1, Batch 1, Loss: -22.276794\n",
            "Time step 2, Epoch 2, Batch 1, Loss: -22.458296\n",
            "Time step 2, Epoch 3, Batch 1, Loss: -22.638107\n",
            "Time step 2, Epoch 4, Batch 1, Loss: -22.816395\n",
            "Time step 2, Epoch 5, Batch 1, Loss: -22.993042\n",
            "Time step 2, Epoch 6, Batch 1, Loss: -23.167484\n",
            "Time step 2, Epoch 7, Batch 1, Loss: -23.340235\n",
            "Time step 2, Epoch 8, Batch 1, Loss: -23.511456\n",
            "Time step 2, Epoch 9, Batch 1, Loss: -23.681257\n",
            "Time step 2, Epoch 10, Batch 1, Loss: -23.850002\n",
            "Time step 2, Epoch 11, Batch 1, Loss: -24.018106\n",
            "Time step 2, Epoch 12, Batch 1, Loss: -24.186296\n",
            "Time step 2, Epoch 13, Batch 1, Loss: -24.354620\n",
            "Time step 2, Epoch 14, Batch 1, Loss: -24.523348\n",
            "Time step 2, Epoch 15, Batch 1, Loss: -24.692608\n",
            "Time step 2, Epoch 16, Batch 1, Loss: -24.862419\n",
            "Time step 2, Epoch 17, Batch 1, Loss: -25.032974\n",
            "Time step 2, Epoch 18, Batch 1, Loss: -25.204262\n",
            "Time step 2, Epoch 19, Batch 1, Loss: -25.376389\n",
            "Time step 2, Epoch 20, Batch 1, Loss: -25.549297\n",
            "Time step 2, Epoch 21, Batch 1, Loss: -25.723034\n",
            "Time step 2, Epoch 22, Batch 1, Loss: -25.897587\n",
            "Time step 2, Epoch 23, Batch 1, Loss: -26.072901\n",
            "Time step 2, Epoch 24, Batch 1, Loss: -26.248535\n",
            "Time step 2, Epoch 25, Batch 1, Loss: -26.424238\n",
            "Time step 2, Epoch 26, Batch 1, Loss: -26.599297\n",
            "Time step 2, Epoch 27, Batch 1, Loss: -26.772852\n",
            "Time step 2, Epoch 28, Batch 1, Loss: -26.944172\n",
            "Time step 2, Epoch 29, Batch 1, Loss: -27.112663\n",
            "Time step 2, Epoch 30, Batch 1, Loss: -27.277529\n",
            "Time step 2, Epoch 31, Batch 1, Loss: -27.437834\n",
            "Time step 2, Epoch 32, Batch 1, Loss: -27.592924\n",
            "Time step 2, Epoch 33, Batch 1, Loss: -27.741865\n",
            "Time step 2, Epoch 34, Batch 1, Loss: -27.884008\n",
            "Time step 2, Epoch 35, Batch 1, Loss: -28.018667\n",
            "Time step 2, Epoch 36, Batch 1, Loss: -28.145319\n",
            "Time step 2, Epoch 37, Batch 1, Loss: -28.263493\n",
            "Time step 2, Epoch 38, Batch 1, Loss: -28.372955\n",
            "Time step 2, Epoch 39, Batch 1, Loss: -28.473625\n",
            "Time step 2, Epoch 40, Batch 1, Loss: -28.565571\n",
            "Time step 2, Epoch 41, Batch 1, Loss: -28.649033\n",
            "Time step 2, Epoch 42, Batch 1, Loss: -28.724337\n",
            "Time step 2, Epoch 43, Batch 1, Loss: -28.791889\n",
            "Time step 2, Epoch 44, Batch 1, Loss: -28.852238\n",
            "Time step 2, Epoch 45, Batch 1, Loss: -28.905951\n",
            "Time step 2, Epoch 46, Batch 1, Loss: -28.953571\n",
            "Time step 2, Epoch 47, Batch 1, Loss: -28.995676\n",
            "Time step 2, Epoch 48, Batch 1, Loss: -29.032829\n",
            "Time step 2, Epoch 49, Batch 1, Loss: -29.065544\n",
            "Time step 2, Epoch 50, Batch 1, Loss: -29.094332\n",
            "Time step 2, Epoch 51, Batch 1, Loss: -29.119650\n",
            "Time step 2, Epoch 52, Batch 1, Loss: -29.141916\n",
            "Time step 2, Epoch 53, Batch 1, Loss: -29.161510\n",
            "Time step 2, Epoch 54, Batch 1, Loss: -29.178759\n",
            "Time step 2, Epoch 55, Batch 1, Loss: -29.193960\n",
            "Time step 2, Epoch 56, Batch 1, Loss: -29.207375\n",
            "Time step 2, Epoch 57, Batch 1, Loss: -29.219234\n",
            "Time step 2, Epoch 58, Batch 1, Loss: -29.229733\n",
            "Time step 2, Epoch 59, Batch 1, Loss: -29.239046\n",
            "Time step 2, Epoch 60, Batch 1, Loss: -29.247332\n",
            "Time step 2, Epoch 61, Batch 1, Loss: -29.254707\n",
            "Time step 2, Epoch 62, Batch 1, Loss: -29.261293\n",
            "Time step 2, Epoch 63, Batch 1, Loss: -29.267185\n",
            "Time step 2, Epoch 64, Batch 1, Loss: -29.272469\n",
            "Time step 2, Epoch 65, Batch 1, Loss: -29.277218\n",
            "Time step 2, Epoch 66, Batch 1, Loss: -29.281494\n",
            "Time step 2, Epoch 67, Batch 1, Loss: -29.285360\n",
            "Time step 2, Epoch 68, Batch 1, Loss: -29.288860\n",
            "Time step 2, Epoch 69, Batch 1, Loss: -29.292034\n",
            "Time step 2, Epoch 70, Batch 1, Loss: -29.294920\n",
            "Time step 2, Epoch 71, Batch 1, Loss: -29.297550\n",
            "Time step 2, Epoch 72, Batch 1, Loss: -29.299950\n",
            "Time step 2, Epoch 73, Batch 1, Loss: -29.302145\n",
            "Time step 2, Epoch 74, Batch 1, Loss: -29.304159\n",
            "Time step 2, Epoch 75, Batch 1, Loss: -29.306005\n",
            "Time step 2, Epoch 76, Batch 1, Loss: -29.307707\n",
            "Time step 2, Epoch 77, Batch 1, Loss: -29.309275\n",
            "Time step 2, Epoch 78, Batch 1, Loss: -29.310722\n",
            "Time step 2, Epoch 79, Batch 1, Loss: -29.312065\n",
            "Time step 2, Epoch 80, Batch 1, Loss: -29.313309\n",
            "Time step 2, Epoch 81, Batch 1, Loss: -29.314465\n",
            "Time step 2, Epoch 82, Batch 1, Loss: -29.315544\n",
            "Time step 2, Epoch 83, Batch 1, Loss: -29.316547\n",
            "Time step 2, Epoch 84, Batch 1, Loss: -29.317488\n",
            "Time step 2, Epoch 85, Batch 1, Loss: -29.318367\n",
            "Time step 2, Epoch 86, Batch 1, Loss: -29.319191\n",
            "Time step 2, Epoch 87, Batch 1, Loss: -29.319969\n",
            "Time step 2, Epoch 88, Batch 1, Loss: -29.320700\n",
            "Time step 2, Epoch 89, Batch 1, Loss: -29.321390\n",
            "Time step 2, Epoch 90, Batch 1, Loss: -29.322042\n",
            "Time step 2, Epoch 91, Batch 1, Loss: -29.322657\n",
            "Time step 2, Epoch 92, Batch 1, Loss: -29.323242\n",
            "Time step 2, Epoch 93, Batch 1, Loss: -29.323795\n",
            "Time step 2, Epoch 94, Batch 1, Loss: -29.324322\n",
            "Time step 2, Epoch 95, Batch 1, Loss: -29.324825\n",
            "Time step 2, Epoch 96, Batch 1, Loss: -29.325300\n",
            "Time step 2, Epoch 97, Batch 1, Loss: -29.325758\n",
            "Time step 2, Epoch 98, Batch 1, Loss: -29.326191\n",
            "Time step 2, Epoch 99, Batch 1, Loss: -29.326609\n",
            "Time step 2, Epoch 100, Batch 1, Loss: -29.327007\n",
            "Time step 2, Epoch 101, Batch 1, Loss: -29.327391\n",
            "Time step 2, Epoch 102, Batch 1, Loss: -29.327761\n",
            "Time step 2, Epoch 103, Batch 1, Loss: -29.328112\n",
            "Time step 2, Epoch 104, Batch 1, Loss: -29.328451\n",
            "Time step 2, Epoch 105, Batch 1, Loss: -29.328775\n",
            "Time step 2, Epoch 106, Batch 1, Loss: -29.329094\n",
            "Time step 2, Epoch 107, Batch 1, Loss: -29.329395\n",
            "Time step 2, Epoch 108, Batch 1, Loss: -29.329687\n",
            "Time step 2, Epoch 109, Batch 1, Loss: -29.329971\n",
            "Time step 2, Epoch 110, Batch 1, Loss: -29.330244\n",
            "Time step 2, Epoch 111, Batch 1, Loss: -29.330509\n",
            "Time step 2, Epoch 112, Batch 1, Loss: -29.330765\n",
            "Time step 2, Epoch 113, Batch 1, Loss: -29.331015\n",
            "Time step 2, Epoch 114, Batch 1, Loss: -29.331253\n",
            "Time step 2, Epoch 115, Batch 1, Loss: -29.331488\n",
            "Time step 2, Epoch 116, Batch 1, Loss: -29.331715\n",
            "Time step 2, Epoch 117, Batch 1, Loss: -29.331934\n",
            "Time step 2, Epoch 118, Batch 1, Loss: -29.332146\n",
            "Time step 2, Epoch 119, Batch 1, Loss: -29.332355\n",
            "Time step 2, Epoch 120, Batch 1, Loss: -29.332558\n",
            "Time step 2, Epoch 121, Batch 1, Loss: -29.332752\n",
            "Time step 2, Epoch 122, Batch 1, Loss: -29.332943\n",
            "Time step 2, Epoch 123, Batch 1, Loss: -29.333128\n",
            "Time step 2, Epoch 124, Batch 1, Loss: -29.333309\n",
            "Time step 2, Epoch 125, Batch 1, Loss: -29.333485\n",
            "Time step 2, Epoch 126, Batch 1, Loss: -29.333656\n",
            "Time step 2, Epoch 127, Batch 1, Loss: -29.333824\n",
            "Time step 2, Epoch 128, Batch 1, Loss: -29.333988\n",
            "Time step 2, Epoch 129, Batch 1, Loss: -29.334150\n",
            "Time step 2, Epoch 130, Batch 1, Loss: -29.334307\n",
            "Time step 2, Epoch 131, Batch 1, Loss: -29.334457\n",
            "Time step 2, Epoch 132, Batch 1, Loss: -29.334602\n",
            "Time step 2, Epoch 133, Batch 1, Loss: -29.334751\n",
            "Time step 2, Epoch 134, Batch 1, Loss: -29.334892\n",
            "Time step 2, Epoch 135, Batch 1, Loss: -29.335028\n",
            "Time step 2, Epoch 136, Batch 1, Loss: -29.335165\n",
            "Time step 2, Epoch 137, Batch 1, Loss: -29.335299\n",
            "Time step 2, Epoch 138, Batch 1, Loss: -29.335430\n",
            "Time step 2, Epoch 139, Batch 1, Loss: -29.335554\n",
            "Time step 2, Epoch 140, Batch 1, Loss: -29.335678\n",
            "Time step 2, Epoch 141, Batch 1, Loss: -29.335802\n",
            "Time step 2, Epoch 142, Batch 1, Loss: -29.335918\n",
            "Time step 2, Epoch 143, Batch 1, Loss: -29.336037\n",
            "Time step 2, Epoch 144, Batch 1, Loss: -29.336147\n",
            "Time step 2, Epoch 145, Batch 1, Loss: -29.336262\n",
            "Time step 2, Epoch 146, Batch 1, Loss: -29.336369\n",
            "Time step 2, Epoch 147, Batch 1, Loss: -29.336475\n",
            "Time step 2, Epoch 148, Batch 1, Loss: -29.336580\n",
            "Time step 2, Epoch 149, Batch 1, Loss: -29.336685\n",
            "Time step 2, Epoch 150, Batch 1, Loss: -29.336782\n",
            "Time step 2, Epoch 151, Batch 1, Loss: -29.336884\n",
            "Time step 2, Epoch 152, Batch 1, Loss: -29.336979\n",
            "Time step 2, Epoch 153, Batch 1, Loss: -29.337074\n",
            "Time step 2, Epoch 154, Batch 1, Loss: -29.337166\n",
            "Time step 2, Epoch 155, Batch 1, Loss: -29.337257\n",
            "Time step 2, Epoch 156, Batch 1, Loss: -29.337347\n",
            "Time step 2, Epoch 157, Batch 1, Loss: -29.337435\n",
            "Time step 2, Epoch 158, Batch 1, Loss: -29.337523\n",
            "Time step 2, Epoch 159, Batch 1, Loss: -29.337606\n",
            "Time step 2, Epoch 160, Batch 1, Loss: -29.337688\n",
            "Time step 2, Epoch 161, Batch 1, Loss: -29.337772\n",
            "Time step 2, Epoch 162, Batch 1, Loss: -29.337852\n",
            "Time step 2, Epoch 163, Batch 1, Loss: -29.337931\n",
            "Time step 2, Epoch 164, Batch 1, Loss: -29.338007\n",
            "Time step 2, Epoch 165, Batch 1, Loss: -29.338083\n",
            "Time step 2, Epoch 166, Batch 1, Loss: -29.338158\n",
            "Time step 2, Epoch 167, Batch 1, Loss: -29.338228\n",
            "Time step 2, Epoch 168, Batch 1, Loss: -29.338301\n",
            "Time step 2, Epoch 169, Batch 1, Loss: -29.338371\n",
            "Time step 2, Epoch 170, Batch 1, Loss: -29.338440\n",
            "Time step 2, Epoch 171, Batch 1, Loss: -29.338507\n",
            "Time step 2, Epoch 172, Batch 1, Loss: -29.338575\n",
            "Time step 2, Epoch 173, Batch 1, Loss: -29.338642\n",
            "Time step 2, Epoch 174, Batch 1, Loss: -29.338705\n",
            "Time step 2, Epoch 175, Batch 1, Loss: -29.338768\n",
            "Time step 2, Epoch 176, Batch 1, Loss: -29.338831\n",
            "Time step 2, Epoch 177, Batch 1, Loss: -29.338892\n",
            "Time step 2, Epoch 178, Batch 1, Loss: -29.338955\n",
            "Time step 2, Epoch 179, Batch 1, Loss: -29.339012\n",
            "Time step 2, Epoch 180, Batch 1, Loss: -29.339073\n",
            "Time step 2, Epoch 181, Batch 1, Loss: -29.339130\n",
            "Time step 2, Epoch 182, Batch 1, Loss: -29.339184\n",
            "Time step 2, Epoch 183, Batch 1, Loss: -29.339239\n",
            "Time step 2, Epoch 184, Batch 1, Loss: -29.339296\n",
            "Time step 2, Epoch 185, Batch 1, Loss: -29.339350\n",
            "Time step 2, Epoch 186, Batch 1, Loss: -29.339403\n",
            "Time step 2, Epoch 187, Batch 1, Loss: -29.339455\n",
            "Time step 2, Epoch 188, Batch 1, Loss: -29.339504\n",
            "Time step 2, Epoch 189, Batch 1, Loss: -29.339558\n",
            "Time step 2, Epoch 190, Batch 1, Loss: -29.339607\n",
            "Time step 2, Epoch 191, Batch 1, Loss: -29.339655\n",
            "Time step 2, Epoch 192, Batch 1, Loss: -29.339703\n",
            "Time step 2, Epoch 193, Batch 1, Loss: -29.339752\n",
            "Time step 2, Epoch 194, Batch 1, Loss: -29.339798\n",
            "Time step 2, Epoch 195, Batch 1, Loss: -29.339844\n",
            "Time step 2, Epoch 196, Batch 1, Loss: -29.339890\n",
            "Time step 2, Epoch 197, Batch 1, Loss: -29.339933\n",
            "Time step 2, Epoch 198, Batch 1, Loss: -29.339977\n",
            "Time step 2, Epoch 199, Batch 1, Loss: -29.340021\n",
            "Time step 2, Epoch 200, Batch 1, Loss: -29.340063\n",
            "Time step 2, Epoch 201, Batch 1, Loss: -29.340107\n",
            "Time step 2, Epoch 202, Batch 1, Loss: -29.340147\n",
            "Time step 2, Epoch 203, Batch 1, Loss: -29.340189\n",
            "Time step 2, Epoch 204, Batch 1, Loss: -29.340227\n",
            "Time step 2, Epoch 205, Batch 1, Loss: -29.340267\n",
            "Time step 2, Epoch 206, Batch 1, Loss: -29.340309\n",
            "Time step 2, Epoch 207, Batch 1, Loss: -29.340345\n",
            "Time step 2, Epoch 208, Batch 1, Loss: -29.340385\n",
            "Time step 2, Epoch 209, Batch 1, Loss: -29.340422\n",
            "Time step 2, Epoch 210, Batch 1, Loss: -29.340460\n",
            "Time step 2, Epoch 211, Batch 1, Loss: -29.340494\n",
            "Time step 2, Epoch 212, Batch 1, Loss: -29.340530\n",
            "Time step 2, Epoch 213, Batch 1, Loss: -29.340565\n",
            "Time step 2, Epoch 214, Batch 1, Loss: -29.340599\n",
            "Time step 2, Epoch 215, Batch 1, Loss: -29.340633\n",
            "Time step 2, Epoch 216, Batch 1, Loss: -29.340668\n",
            "Time step 2, Epoch 217, Batch 1, Loss: -29.340702\n",
            "Time step 2, Epoch 218, Batch 1, Loss: -29.340733\n",
            "Time step 2, Epoch 219, Batch 1, Loss: -29.340767\n",
            "Time step 2, Epoch 220, Batch 1, Loss: -29.340797\n",
            "Time step 2, Epoch 221, Batch 1, Loss: -29.340830\n",
            "Time step 2, Epoch 222, Batch 1, Loss: -29.340860\n",
            "Time step 2, Epoch 223, Batch 1, Loss: -29.340893\n",
            "Time step 2, Epoch 224, Batch 1, Loss: -29.340925\n",
            "Time step 2, Epoch 225, Batch 1, Loss: -29.340952\n",
            "Time step 2, Epoch 226, Batch 1, Loss: -29.340984\n",
            "Time step 2, Epoch 227, Batch 1, Loss: -29.341013\n",
            "Time step 2, Epoch 228, Batch 1, Loss: -29.341042\n",
            "Time step 2, Epoch 229, Batch 1, Loss: -29.341070\n",
            "Time step 2, Epoch 230, Batch 1, Loss: -29.341097\n",
            "Time step 2, Epoch 231, Batch 1, Loss: -29.341127\n",
            "Time step 2, Epoch 232, Batch 1, Loss: -29.341152\n",
            "Time step 2, Epoch 233, Batch 1, Loss: -29.341181\n",
            "Time step 2, Epoch 234, Batch 1, Loss: -29.341208\n",
            "Time step 2, Epoch 235, Batch 1, Loss: -29.341232\n",
            "Time step 2, Epoch 236, Batch 1, Loss: -29.341257\n",
            "Time step 2, Epoch 237, Batch 1, Loss: -29.341284\n",
            "Time step 2, Epoch 238, Batch 1, Loss: -29.341309\n",
            "Time step 2, Epoch 239, Batch 1, Loss: -29.341335\n",
            "Time step 2, Epoch 240, Batch 1, Loss: -29.341362\n",
            "Time step 2, Epoch 241, Batch 1, Loss: -29.341381\n",
            "Time step 2, Epoch 242, Batch 1, Loss: -29.341408\n",
            "Time step 2, Epoch 243, Batch 1, Loss: -29.341433\n",
            "Time step 2, Epoch 244, Batch 1, Loss: -29.341454\n",
            "Time step 2, Epoch 245, Batch 1, Loss: -29.341476\n",
            "Time step 2, Epoch 246, Batch 1, Loss: -29.341501\n",
            "Time step 2, Epoch 247, Batch 1, Loss: -29.341524\n",
            "Time step 2, Epoch 248, Batch 1, Loss: -29.341545\n",
            "Time step 2, Epoch 249, Batch 1, Loss: -29.341568\n",
            "Time step 2, Epoch 250, Batch 1, Loss: -29.341591\n",
            "Time step 2, Epoch 251, Batch 1, Loss: -29.341610\n",
            "Time step 2, Epoch 252, Batch 1, Loss: -29.341635\n",
            "Time step 2, Epoch 253, Batch 1, Loss: -29.341652\n",
            "Time step 2, Epoch 254, Batch 1, Loss: -29.341673\n",
            "Time step 2, Epoch 255, Batch 1, Loss: -29.341694\n",
            "Time step 2, Epoch 256, Batch 1, Loss: -29.341713\n",
            "Time step 2, Epoch 257, Batch 1, Loss: -29.341734\n",
            "Time step 2, Epoch 258, Batch 1, Loss: -29.341753\n",
            "Time step 2, Epoch 259, Batch 1, Loss: -29.341774\n",
            "Time step 2, Epoch 260, Batch 1, Loss: -29.341793\n",
            "Time step 2, Epoch 261, Batch 1, Loss: -29.341812\n",
            "Time step 2, Epoch 262, Batch 1, Loss: -29.341831\n",
            "Time step 2, Epoch 263, Batch 1, Loss: -29.341850\n",
            "Time step 2, Epoch 264, Batch 1, Loss: -29.341869\n",
            "Time step 2, Epoch 265, Batch 1, Loss: -29.341888\n",
            "Time step 2, Epoch 266, Batch 1, Loss: -29.341904\n",
            "Time step 2, Epoch 267, Batch 1, Loss: -29.341923\n",
            "Time step 2, Epoch 268, Batch 1, Loss: -29.341940\n",
            "Time step 2, Epoch 269, Batch 1, Loss: -29.341957\n",
            "Time step 2, Epoch 270, Batch 1, Loss: -29.341976\n",
            "Time step 2, Epoch 271, Batch 1, Loss: -29.341993\n",
            "Time step 2, Epoch 272, Batch 1, Loss: -29.342009\n",
            "Time step 2, Epoch 273, Batch 1, Loss: -29.342026\n",
            "Time step 2, Epoch 274, Batch 1, Loss: -29.342043\n",
            "Time step 2, Epoch 275, Batch 1, Loss: -29.342060\n",
            "Time step 2, Epoch 276, Batch 1, Loss: -29.342075\n",
            "Time step 2, Epoch 277, Batch 1, Loss: -29.342091\n",
            "Time step 2, Epoch 278, Batch 1, Loss: -29.342108\n",
            "Time step 2, Epoch 279, Batch 1, Loss: -29.342123\n",
            "Time step 2, Epoch 280, Batch 1, Loss: -29.342138\n",
            "Time step 2, Epoch 281, Batch 1, Loss: -29.342152\n",
            "Time step 2, Epoch 282, Batch 1, Loss: -29.342169\n",
            "Time step 2, Epoch 283, Batch 1, Loss: -29.342182\n",
            "Time step 2, Epoch 284, Batch 1, Loss: -29.342197\n",
            "Time step 2, Epoch 285, Batch 1, Loss: -29.342213\n",
            "Time step 2, Epoch 286, Batch 1, Loss: -29.342228\n",
            "Time step 2, Epoch 287, Batch 1, Loss: -29.342243\n",
            "Time step 2, Epoch 288, Batch 1, Loss: -29.342257\n",
            "Time step 2, Epoch 289, Batch 1, Loss: -29.342270\n",
            "Time step 2, Epoch 290, Batch 1, Loss: -29.342283\n",
            "Time step 2, Epoch 291, Batch 1, Loss: -29.342299\n",
            "Time step 2, Epoch 292, Batch 1, Loss: -29.342312\n",
            "Time step 2, Epoch 293, Batch 1, Loss: -29.342325\n",
            "Time step 2, Epoch 294, Batch 1, Loss: -29.342339\n",
            "Time step 2, Epoch 295, Batch 1, Loss: -29.342352\n",
            "Time step 2, Epoch 296, Batch 1, Loss: -29.342363\n",
            "Time step 2, Epoch 297, Batch 1, Loss: -29.342375\n",
            "Time step 2, Epoch 298, Batch 1, Loss: -29.342390\n",
            "Time step 2, Epoch 299, Batch 1, Loss: -29.342403\n",
            "Time step 2, Epoch 300, Batch 1, Loss: -29.342415\n",
            "Time step 2, Epoch 301, Batch 1, Loss: -29.342426\n",
            "Time step 2, Epoch 302, Batch 1, Loss: -29.342440\n",
            "Time step 2, Epoch 303, Batch 1, Loss: -29.342449\n",
            "Time step 2, Epoch 304, Batch 1, Loss: -29.342466\n",
            "Time step 2, Epoch 305, Batch 1, Loss: -29.342476\n",
            "Time step 2, Epoch 306, Batch 1, Loss: -29.342485\n",
            "Time step 2, Epoch 307, Batch 1, Loss: -29.342501\n",
            "Time step 2, Epoch 308, Batch 1, Loss: -29.342512\n",
            "Time step 2, Epoch 309, Batch 1, Loss: -29.342524\n",
            "Time step 2, Epoch 310, Batch 1, Loss: -29.342535\n",
            "Time step 2, Epoch 311, Batch 1, Loss: -29.342543\n",
            "Time step 2, Epoch 312, Batch 1, Loss: -29.342558\n",
            "Time step 2, Epoch 313, Batch 1, Loss: -29.342566\n",
            "Time step 2, Epoch 314, Batch 1, Loss: -29.342581\n",
            "Time step 2, Epoch 315, Batch 1, Loss: -29.342590\n",
            "Time step 2, Epoch 316, Batch 1, Loss: -29.342600\n",
            "Time step 2, Epoch 317, Batch 1, Loss: -29.342609\n",
            "Time step 2, Epoch 318, Batch 1, Loss: -29.342621\n",
            "Time step 2, Epoch 319, Batch 1, Loss: -29.342630\n",
            "Time step 2, Epoch 320, Batch 1, Loss: -29.342642\n",
            "Time step 2, Epoch 321, Batch 1, Loss: -29.342653\n",
            "Time step 2, Epoch 322, Batch 1, Loss: -29.342663\n",
            "Time step 2, Epoch 323, Batch 1, Loss: -29.342670\n",
            "Time step 2, Epoch 324, Batch 1, Loss: -29.342678\n",
            "Time step 2, Epoch 325, Batch 1, Loss: -29.342693\n",
            "Time step 2, Epoch 326, Batch 1, Loss: -29.342701\n",
            "Time step 2, Epoch 327, Batch 1, Loss: -29.342710\n",
            "Time step 2, Epoch 328, Batch 1, Loss: -29.342720\n",
            "Time step 2, Epoch 329, Batch 1, Loss: -29.342731\n",
            "Time step 2, Epoch 330, Batch 1, Loss: -29.342739\n",
            "Time step 2, Epoch 331, Batch 1, Loss: -29.342747\n",
            "Time step 2, Epoch 332, Batch 1, Loss: -29.342756\n",
            "Time step 2, Epoch 333, Batch 1, Loss: -29.342768\n",
            "Time step 2, Epoch 334, Batch 1, Loss: -29.342775\n",
            "Time step 2, Epoch 335, Batch 1, Loss: -29.342789\n",
            "Time step 2, Epoch 336, Batch 1, Loss: -29.342793\n",
            "Time step 2, Epoch 337, Batch 1, Loss: -29.342802\n",
            "Time step 2, Epoch 338, Batch 1, Loss: -29.342810\n",
            "Time step 2, Epoch 339, Batch 1, Loss: -29.342819\n",
            "Time step 2, Epoch 340, Batch 1, Loss: -29.342827\n",
            "Time step 2, Epoch 341, Batch 1, Loss: -29.342836\n",
            "Time step 2, Epoch 342, Batch 1, Loss: -29.342844\n",
            "Time step 2, Epoch 343, Batch 1, Loss: -29.342854\n",
            "Time step 2, Epoch 344, Batch 1, Loss: -29.342863\n",
            "Time step 2, Epoch 345, Batch 1, Loss: -29.342869\n",
            "Time step 2, Epoch 346, Batch 1, Loss: -29.342878\n",
            "Time step 2, Epoch 347, Batch 1, Loss: -29.342884\n",
            "Time step 2, Epoch 348, Batch 1, Loss: -29.342892\n",
            "Time step 2, Epoch 349, Batch 1, Loss: -29.342901\n",
            "Time step 2, Epoch 350, Batch 1, Loss: -29.342911\n",
            "Time step 2, Epoch 351, Batch 1, Loss: -29.342920\n",
            "Time step 2, Epoch 352, Batch 1, Loss: -29.342926\n",
            "Time step 2, Epoch 353, Batch 1, Loss: -29.342934\n",
            "Time step 2, Epoch 354, Batch 1, Loss: -29.342941\n",
            "Time step 2, Epoch 355, Batch 1, Loss: -29.342949\n",
            "Time step 2, Epoch 356, Batch 1, Loss: -29.342955\n",
            "Time step 2, Epoch 357, Batch 1, Loss: -29.342962\n",
            "Time step 2, Epoch 358, Batch 1, Loss: -29.342970\n",
            "Time step 2, Epoch 359, Batch 1, Loss: -29.342978\n",
            "Time step 2, Epoch 360, Batch 1, Loss: -29.342987\n",
            "Time step 2, Epoch 361, Batch 1, Loss: -29.342991\n",
            "Time step 2, Epoch 362, Batch 1, Loss: -29.342999\n",
            "Time step 2, Epoch 363, Batch 1, Loss: -29.343006\n",
            "Time step 2, Epoch 364, Batch 1, Loss: -29.343014\n",
            "Time step 2, Epoch 365, Batch 1, Loss: -29.343019\n",
            "Time step 2, Epoch 366, Batch 1, Loss: -29.343025\n",
            "Time step 2, Epoch 367, Batch 1, Loss: -29.343035\n",
            "Time step 2, Epoch 368, Batch 1, Loss: -29.343040\n",
            "Time step 2, Epoch 369, Batch 1, Loss: -29.343048\n",
            "Time step 2, Epoch 370, Batch 1, Loss: -29.343054\n",
            "Time step 2, Epoch 371, Batch 1, Loss: -29.343058\n",
            "Time step 2, Epoch 372, Batch 1, Loss: -29.343069\n",
            "Time step 2, Epoch 373, Batch 1, Loss: -29.343073\n",
            "Time step 2, Epoch 374, Batch 1, Loss: -29.343079\n",
            "Time step 2, Epoch 375, Batch 1, Loss: -29.343086\n",
            "Time step 2, Epoch 376, Batch 1, Loss: -29.343090\n",
            "Time step 2, Epoch 377, Batch 1, Loss: -29.343096\n",
            "Time step 2, Epoch 378, Batch 1, Loss: -29.343103\n",
            "Time step 2, Epoch 379, Batch 1, Loss: -29.343109\n",
            "Time step 2, Epoch 380, Batch 1, Loss: -29.343117\n",
            "Time step 2, Epoch 381, Batch 1, Loss: -29.343122\n",
            "Time step 2, Epoch 382, Batch 1, Loss: -29.343128\n",
            "Time step 2, Epoch 383, Batch 1, Loss: -29.343134\n",
            "Time step 2, Epoch 384, Batch 1, Loss: -29.343140\n",
            "Time step 2, Epoch 385, Batch 1, Loss: -29.343145\n",
            "Time step 2, Epoch 386, Batch 1, Loss: -29.343151\n",
            "Time step 2, Epoch 387, Batch 1, Loss: -29.343155\n",
            "Time step 2, Epoch 388, Batch 1, Loss: -29.343163\n",
            "Time step 2, Epoch 389, Batch 1, Loss: -29.343168\n",
            "Time step 2, Epoch 390, Batch 1, Loss: -29.343176\n",
            "Time step 2, Epoch 391, Batch 1, Loss: -29.343180\n",
            "Time step 2, Epoch 392, Batch 1, Loss: -29.343187\n",
            "Time step 2, Epoch 393, Batch 1, Loss: -29.343193\n",
            "Time step 2, Epoch 394, Batch 1, Loss: -29.343197\n",
            "Time step 2, Epoch 395, Batch 1, Loss: -29.343203\n",
            "Time step 2, Epoch 396, Batch 1, Loss: -29.343208\n",
            "Time step 2, Epoch 397, Batch 1, Loss: -29.343212\n",
            "Time step 2, Epoch 398, Batch 1, Loss: -29.343220\n",
            "Time step 2, Epoch 399, Batch 1, Loss: -29.343224\n",
            "Time step 2, Epoch 400, Batch 1, Loss: -29.343225\n",
            "Time step 2, Epoch 401, Batch 1, Loss: -29.343233\n",
            "Time step 2, Epoch 402, Batch 1, Loss: -29.343239\n",
            "Time step 2, Epoch 403, Batch 1, Loss: -29.343245\n",
            "Time step 2, Epoch 404, Batch 1, Loss: -29.343250\n",
            "Time step 2, Epoch 405, Batch 1, Loss: -29.343256\n",
            "Time step 2, Epoch 406, Batch 1, Loss: -29.343262\n",
            "Time step 2, Epoch 407, Batch 1, Loss: -29.343264\n",
            "Time step 2, Epoch 408, Batch 1, Loss: -29.343269\n",
            "Time step 2, Epoch 409, Batch 1, Loss: -29.343273\n",
            "Time step 2, Epoch 410, Batch 1, Loss: -29.343277\n",
            "Time step 2, Epoch 411, Batch 1, Loss: -29.343283\n",
            "Time step 2, Epoch 412, Batch 1, Loss: -29.343288\n",
            "Time step 2, Epoch 413, Batch 1, Loss: -29.343292\n",
            "Time step 2, Epoch 414, Batch 1, Loss: -29.343296\n",
            "Time step 2, Epoch 415, Batch 1, Loss: -29.343304\n",
            "Time step 2, Epoch 416, Batch 1, Loss: -29.343307\n",
            "Time step 2, Epoch 417, Batch 1, Loss: -29.343311\n",
            "Time step 2, Epoch 418, Batch 1, Loss: -29.343315\n",
            "Time step 2, Epoch 419, Batch 1, Loss: -29.343319\n",
            "Time step 2, Epoch 420, Batch 1, Loss: -29.343327\n",
            "Time step 2, Epoch 421, Batch 1, Loss: -29.343327\n",
            "Time step 2, Epoch 422, Batch 1, Loss: -29.343336\n",
            "Time step 2, Epoch 423, Batch 1, Loss: -29.343338\n",
            "Time step 2, Epoch 424, Batch 1, Loss: -29.343342\n",
            "Time step 2, Epoch 425, Batch 1, Loss: -29.343348\n",
            "Time step 2, Epoch 426, Batch 1, Loss: -29.343351\n",
            "Time step 2, Epoch 427, Batch 1, Loss: -29.343357\n",
            "Time step 2, Epoch 428, Batch 1, Loss: -29.343357\n",
            "Time step 2, Epoch 429, Batch 1, Loss: -29.343363\n",
            "Time step 2, Epoch 430, Batch 1, Loss: -29.343369\n",
            "Time step 2, Epoch 431, Batch 1, Loss: -29.343374\n",
            "Time step 2, Epoch 432, Batch 1, Loss: -29.343376\n",
            "Time step 2, Epoch 433, Batch 1, Loss: -29.343380\n",
            "Time step 2, Epoch 434, Batch 1, Loss: -29.343382\n",
            "Time step 2, Epoch 435, Batch 1, Loss: -29.343391\n",
            "Time step 2, Epoch 436, Batch 1, Loss: -29.343391\n",
            "Time step 2, Epoch 437, Batch 1, Loss: -29.343397\n",
            "Time step 2, Epoch 438, Batch 1, Loss: -29.343399\n",
            "Time step 2, Epoch 439, Batch 1, Loss: -29.343403\n",
            "Time step 2, Epoch 440, Batch 1, Loss: -29.343407\n",
            "Time step 2, Epoch 441, Batch 1, Loss: -29.343410\n",
            "Time step 2, Epoch 442, Batch 1, Loss: -29.343416\n",
            "Time step 2, Epoch 443, Batch 1, Loss: -29.343422\n",
            "Time step 2, Epoch 444, Batch 1, Loss: -29.343422\n",
            "Time step 2, Epoch 445, Batch 1, Loss: -29.343428\n",
            "Time step 2, Epoch 446, Batch 1, Loss: -29.343430\n",
            "Time step 2, Epoch 447, Batch 1, Loss: -29.343433\n",
            "Time step 2, Epoch 448, Batch 1, Loss: -29.343439\n",
            "Time step 2, Epoch 449, Batch 1, Loss: -29.343443\n",
            "Time step 2, Epoch 450, Batch 1, Loss: -29.343445\n",
            "Time step 2, Epoch 451, Batch 1, Loss: -29.343449\n",
            "Time step 2, Epoch 452, Batch 1, Loss: -29.343454\n",
            "Time step 2, Epoch 453, Batch 1, Loss: -29.343456\n",
            "Time step 2, Epoch 454, Batch 1, Loss: -29.343458\n",
            "Time step 2, Epoch 455, Batch 1, Loss: -29.343464\n",
            "Time step 2, Epoch 456, Batch 1, Loss: -29.343468\n",
            "Time step 2, Epoch 457, Batch 1, Loss: -29.343472\n",
            "Time step 2, Epoch 458, Batch 1, Loss: -29.343473\n",
            "Time step 2, Epoch 459, Batch 1, Loss: -29.343475\n",
            "Time step 2, Epoch 460, Batch 1, Loss: -29.343479\n",
            "Time step 2, Epoch 461, Batch 1, Loss: -29.343483\n",
            "Time step 2, Epoch 462, Batch 1, Loss: -29.343487\n",
            "Time step 2, Epoch 463, Batch 1, Loss: -29.343491\n",
            "Time step 2, Epoch 464, Batch 1, Loss: -29.343496\n",
            "Time step 2, Epoch 465, Batch 1, Loss: -29.343494\n",
            "Time step 2, Epoch 466, Batch 1, Loss: -29.343502\n",
            "Time step 2, Epoch 467, Batch 1, Loss: -29.343506\n",
            "Time step 2, Epoch 468, Batch 1, Loss: -29.343506\n",
            "Time step 2, Epoch 469, Batch 1, Loss: -29.343510\n",
            "Time step 2, Epoch 470, Batch 1, Loss: -29.343513\n",
            "Time step 2, Epoch 471, Batch 1, Loss: -29.343515\n",
            "Time step 2, Epoch 472, Batch 1, Loss: -29.343519\n",
            "Time step 2, Epoch 473, Batch 1, Loss: -29.343523\n",
            "Time step 2, Epoch 474, Batch 1, Loss: -29.343529\n",
            "Time step 2, Epoch 475, Batch 1, Loss: -29.343529\n",
            "Time step 2, Epoch 476, Batch 1, Loss: -29.343533\n",
            "Time step 2, Epoch 477, Batch 1, Loss: -29.343534\n",
            "Time step 2, Epoch 478, Batch 1, Loss: -29.343534\n",
            "Time step 2, Epoch 479, Batch 1, Loss: -29.343540\n",
            "Time step 2, Epoch 480, Batch 1, Loss: -29.343544\n",
            "Time step 2, Epoch 481, Batch 1, Loss: -29.343548\n",
            "Time step 2, Epoch 482, Batch 1, Loss: -29.343550\n",
            "Time step 2, Epoch 483, Batch 1, Loss: -29.343552\n",
            "Time step 2, Epoch 484, Batch 1, Loss: -29.343555\n",
            "Time step 2, Epoch 485, Batch 1, Loss: -29.343557\n",
            "Time step 2, Epoch 486, Batch 1, Loss: -29.343563\n",
            "Time step 2, Epoch 487, Batch 1, Loss: -29.343567\n",
            "Time step 2, Epoch 488, Batch 1, Loss: -29.343567\n",
            "Time step 2, Epoch 489, Batch 1, Loss: -29.343569\n",
            "Time step 2, Epoch 490, Batch 1, Loss: -29.343575\n",
            "Time step 2, Epoch 491, Batch 1, Loss: -29.343575\n",
            "Time step 2, Epoch 492, Batch 1, Loss: -29.343578\n",
            "Time step 2, Epoch 493, Batch 1, Loss: -29.343580\n",
            "Time step 2, Epoch 494, Batch 1, Loss: -29.343584\n",
            "Time step 2, Epoch 495, Batch 1, Loss: -29.343586\n",
            "Time step 2, Epoch 496, Batch 1, Loss: -29.343588\n",
            "Time step 2, Epoch 497, Batch 1, Loss: -29.343592\n",
            "Time step 2, Epoch 498, Batch 1, Loss: -29.343594\n",
            "Time step 2, Epoch 499, Batch 1, Loss: -29.343597\n",
            "Time step 2, Epoch 500, Batch 1, Loss: -29.343601\n",
            "Training model for time step 1...\n",
            "Time step 1, Epoch 1, Batch 1, Loss: -20.618973\n",
            "Time step 1, Epoch 2, Batch 1, Loss: -20.908958\n",
            "Time step 1, Epoch 3, Batch 1, Loss: -21.199791\n",
            "Time step 1, Epoch 4, Batch 1, Loss: -21.491781\n",
            "Time step 1, Epoch 5, Batch 1, Loss: -21.785067\n",
            "Time step 1, Epoch 6, Batch 1, Loss: -22.079672\n",
            "Time step 1, Epoch 7, Batch 1, Loss: -22.375498\n",
            "Time step 1, Epoch 8, Batch 1, Loss: -22.672134\n",
            "Time step 1, Epoch 9, Batch 1, Loss: -22.969604\n",
            "Time step 1, Epoch 10, Batch 1, Loss: -23.267483\n",
            "Time step 1, Epoch 11, Batch 1, Loss: -23.564936\n",
            "Time step 1, Epoch 12, Batch 1, Loss: -23.861265\n",
            "Time step 1, Epoch 13, Batch 1, Loss: -24.155653\n",
            "Time step 1, Epoch 14, Batch 1, Loss: -24.447935\n",
            "Time step 1, Epoch 15, Batch 1, Loss: -24.737377\n",
            "Time step 1, Epoch 16, Batch 1, Loss: -25.022898\n",
            "Time step 1, Epoch 17, Batch 1, Loss: -25.303532\n",
            "Time step 1, Epoch 18, Batch 1, Loss: -25.578316\n",
            "Time step 1, Epoch 19, Batch 1, Loss: -25.846048\n",
            "Time step 1, Epoch 20, Batch 1, Loss: -26.105841\n",
            "Time step 1, Epoch 21, Batch 1, Loss: -26.356653\n",
            "Time step 1, Epoch 22, Batch 1, Loss: -26.597462\n",
            "Time step 1, Epoch 23, Batch 1, Loss: -26.827507\n",
            "Time step 1, Epoch 24, Batch 1, Loss: -27.046198\n",
            "Time step 1, Epoch 25, Batch 1, Loss: -27.252760\n",
            "Time step 1, Epoch 26, Batch 1, Loss: -27.446957\n",
            "Time step 1, Epoch 27, Batch 1, Loss: -27.628616\n",
            "Time step 1, Epoch 28, Batch 1, Loss: -27.797569\n",
            "Time step 1, Epoch 29, Batch 1, Loss: -27.953825\n",
            "Time step 1, Epoch 30, Batch 1, Loss: -28.097443\n",
            "Time step 1, Epoch 31, Batch 1, Loss: -28.228682\n",
            "Time step 1, Epoch 32, Batch 1, Loss: -28.348032\n",
            "Time step 1, Epoch 33, Batch 1, Loss: -28.455950\n",
            "Time step 1, Epoch 34, Batch 1, Loss: -28.553091\n",
            "Time step 1, Epoch 35, Batch 1, Loss: -28.640181\n",
            "Time step 1, Epoch 36, Batch 1, Loss: -28.717976\n",
            "Time step 1, Epoch 37, Batch 1, Loss: -28.787289\n",
            "Time step 1, Epoch 38, Batch 1, Loss: -28.848850\n",
            "Time step 1, Epoch 39, Batch 1, Loss: -28.903414\n",
            "Time step 1, Epoch 40, Batch 1, Loss: -28.951681\n",
            "Time step 1, Epoch 41, Batch 1, Loss: -28.994316\n",
            "Time step 1, Epoch 42, Batch 1, Loss: -29.031937\n",
            "Time step 1, Epoch 43, Batch 1, Loss: -29.065084\n",
            "Time step 1, Epoch 44, Batch 1, Loss: -29.094301\n",
            "Time step 1, Epoch 45, Batch 1, Loss: -29.120031\n",
            "Time step 1, Epoch 46, Batch 1, Loss: -29.142691\n",
            "Time step 1, Epoch 47, Batch 1, Loss: -29.162634\n",
            "Time step 1, Epoch 48, Batch 1, Loss: -29.180199\n",
            "Time step 1, Epoch 49, Batch 1, Loss: -29.195683\n",
            "Time step 1, Epoch 50, Batch 1, Loss: -29.209347\n",
            "Time step 1, Epoch 51, Batch 1, Loss: -29.221420\n",
            "Time step 1, Epoch 52, Batch 1, Loss: -29.232094\n",
            "Time step 1, Epoch 53, Batch 1, Loss: -29.241545\n",
            "Time step 1, Epoch 54, Batch 1, Loss: -29.249928\n",
            "Time step 1, Epoch 55, Batch 1, Loss: -29.257372\n",
            "Time step 1, Epoch 56, Batch 1, Loss: -29.263996\n",
            "Time step 1, Epoch 57, Batch 1, Loss: -29.269907\n",
            "Time step 1, Epoch 58, Batch 1, Loss: -29.275188\n",
            "Time step 1, Epoch 59, Batch 1, Loss: -29.279915\n",
            "Time step 1, Epoch 60, Batch 1, Loss: -29.284161\n",
            "Time step 1, Epoch 61, Batch 1, Loss: -29.287983\n",
            "Time step 1, Epoch 62, Batch 1, Loss: -29.291428\n",
            "Time step 1, Epoch 63, Batch 1, Loss: -29.294544\n",
            "Time step 1, Epoch 64, Batch 1, Loss: -29.297365\n",
            "Time step 1, Epoch 65, Batch 1, Loss: -29.299923\n",
            "Time step 1, Epoch 66, Batch 1, Loss: -29.302256\n",
            "Time step 1, Epoch 67, Batch 1, Loss: -29.304382\n",
            "Time step 1, Epoch 68, Batch 1, Loss: -29.306324\n",
            "Time step 1, Epoch 69, Batch 1, Loss: -29.308107\n",
            "Time step 1, Epoch 70, Batch 1, Loss: -29.309740\n",
            "Time step 1, Epoch 71, Batch 1, Loss: -29.311247\n",
            "Time step 1, Epoch 72, Batch 1, Loss: -29.312637\n",
            "Time step 1, Epoch 73, Batch 1, Loss: -29.313917\n",
            "Time step 1, Epoch 74, Batch 1, Loss: -29.315107\n",
            "Time step 1, Epoch 75, Batch 1, Loss: -29.316212\n",
            "Time step 1, Epoch 76, Batch 1, Loss: -29.317238\n",
            "Time step 1, Epoch 77, Batch 1, Loss: -29.318193\n",
            "Time step 1, Epoch 78, Batch 1, Loss: -29.319088\n",
            "Time step 1, Epoch 79, Batch 1, Loss: -29.319923\n",
            "Time step 1, Epoch 80, Batch 1, Loss: -29.320707\n",
            "Time step 1, Epoch 81, Batch 1, Loss: -29.321445\n",
            "Time step 1, Epoch 82, Batch 1, Loss: -29.322136\n",
            "Time step 1, Epoch 83, Batch 1, Loss: -29.322786\n",
            "Time step 1, Epoch 84, Batch 1, Loss: -29.323402\n",
            "Time step 1, Epoch 85, Batch 1, Loss: -29.323986\n",
            "Time step 1, Epoch 86, Batch 1, Loss: -29.324537\n",
            "Time step 1, Epoch 87, Batch 1, Loss: -29.325058\n",
            "Time step 1, Epoch 88, Batch 1, Loss: -29.325556\n",
            "Time step 1, Epoch 89, Batch 1, Loss: -29.326029\n",
            "Time step 1, Epoch 90, Batch 1, Loss: -29.326477\n",
            "Time step 1, Epoch 91, Batch 1, Loss: -29.326908\n",
            "Time step 1, Epoch 92, Batch 1, Loss: -29.327316\n",
            "Time step 1, Epoch 93, Batch 1, Loss: -29.327709\n",
            "Time step 1, Epoch 94, Batch 1, Loss: -29.328083\n",
            "Time step 1, Epoch 95, Batch 1, Loss: -29.328444\n",
            "Time step 1, Epoch 96, Batch 1, Loss: -29.328789\n",
            "Time step 1, Epoch 97, Batch 1, Loss: -29.329119\n",
            "Time step 1, Epoch 98, Batch 1, Loss: -29.329437\n",
            "Time step 1, Epoch 99, Batch 1, Loss: -29.329744\n",
            "Time step 1, Epoch 100, Batch 1, Loss: -29.330042\n",
            "Time step 1, Epoch 101, Batch 1, Loss: -29.330326\n",
            "Time step 1, Epoch 102, Batch 1, Loss: -29.330599\n",
            "Time step 1, Epoch 103, Batch 1, Loss: -29.330862\n",
            "Time step 1, Epoch 104, Batch 1, Loss: -29.331120\n",
            "Time step 1, Epoch 105, Batch 1, Loss: -29.331367\n",
            "Time step 1, Epoch 106, Batch 1, Loss: -29.331608\n",
            "Time step 1, Epoch 107, Batch 1, Loss: -29.331841\n",
            "Time step 1, Epoch 108, Batch 1, Loss: -29.332066\n",
            "Time step 1, Epoch 109, Batch 1, Loss: -29.332285\n",
            "Time step 1, Epoch 110, Batch 1, Loss: -29.332497\n",
            "Time step 1, Epoch 111, Batch 1, Loss: -29.332701\n",
            "Time step 1, Epoch 112, Batch 1, Loss: -29.332903\n",
            "Time step 1, Epoch 113, Batch 1, Loss: -29.333096\n",
            "Time step 1, Epoch 114, Batch 1, Loss: -29.333282\n",
            "Time step 1, Epoch 115, Batch 1, Loss: -29.333469\n",
            "Time step 1, Epoch 116, Batch 1, Loss: -29.333647\n",
            "Time step 1, Epoch 117, Batch 1, Loss: -29.333820\n",
            "Time step 1, Epoch 118, Batch 1, Loss: -29.333988\n",
            "Time step 1, Epoch 119, Batch 1, Loss: -29.334154\n",
            "Time step 1, Epoch 120, Batch 1, Loss: -29.334312\n",
            "Time step 1, Epoch 121, Batch 1, Loss: -29.334473\n",
            "Time step 1, Epoch 122, Batch 1, Loss: -29.334623\n",
            "Time step 1, Epoch 123, Batch 1, Loss: -29.334774\n",
            "Time step 1, Epoch 124, Batch 1, Loss: -29.334921\n",
            "Time step 1, Epoch 125, Batch 1, Loss: -29.335060\n",
            "Time step 1, Epoch 126, Batch 1, Loss: -29.335199\n",
            "Time step 1, Epoch 127, Batch 1, Loss: -29.335333\n",
            "Time step 1, Epoch 128, Batch 1, Loss: -29.335466\n",
            "Time step 1, Epoch 129, Batch 1, Loss: -29.335598\n",
            "Time step 1, Epoch 130, Batch 1, Loss: -29.335720\n",
            "Time step 1, Epoch 131, Batch 1, Loss: -29.335848\n",
            "Time step 1, Epoch 132, Batch 1, Loss: -29.335966\n",
            "Time step 1, Epoch 133, Batch 1, Loss: -29.336086\n",
            "Time step 1, Epoch 134, Batch 1, Loss: -29.336201\n",
            "Time step 1, Epoch 135, Batch 1, Loss: -29.336311\n",
            "Time step 1, Epoch 136, Batch 1, Loss: -29.336426\n",
            "Time step 1, Epoch 137, Batch 1, Loss: -29.336533\n",
            "Time step 1, Epoch 138, Batch 1, Loss: -29.336639\n",
            "Time step 1, Epoch 139, Batch 1, Loss: -29.336742\n",
            "Time step 1, Epoch 140, Batch 1, Loss: -29.336845\n",
            "Time step 1, Epoch 141, Batch 1, Loss: -29.336946\n",
            "Time step 1, Epoch 142, Batch 1, Loss: -29.337042\n",
            "Time step 1, Epoch 143, Batch 1, Loss: -29.337135\n",
            "Time step 1, Epoch 144, Batch 1, Loss: -29.337231\n",
            "Time step 1, Epoch 145, Batch 1, Loss: -29.337322\n",
            "Time step 1, Epoch 146, Batch 1, Loss: -29.337412\n",
            "Time step 1, Epoch 147, Batch 1, Loss: -29.337502\n",
            "Time step 1, Epoch 148, Batch 1, Loss: -29.337589\n",
            "Time step 1, Epoch 149, Batch 1, Loss: -29.337673\n",
            "Time step 1, Epoch 150, Batch 1, Loss: -29.337755\n",
            "Time step 1, Epoch 151, Batch 1, Loss: -29.337837\n",
            "Time step 1, Epoch 152, Batch 1, Loss: -29.337919\n",
            "Time step 1, Epoch 153, Batch 1, Loss: -29.337997\n",
            "Time step 1, Epoch 154, Batch 1, Loss: -29.338074\n",
            "Time step 1, Epoch 155, Batch 1, Loss: -29.338150\n",
            "Time step 1, Epoch 156, Batch 1, Loss: -29.338226\n",
            "Time step 1, Epoch 157, Batch 1, Loss: -29.338297\n",
            "Time step 1, Epoch 158, Batch 1, Loss: -29.338369\n",
            "Time step 1, Epoch 159, Batch 1, Loss: -29.338442\n",
            "Time step 1, Epoch 160, Batch 1, Loss: -29.338511\n",
            "Time step 1, Epoch 161, Batch 1, Loss: -29.338577\n",
            "Time step 1, Epoch 162, Batch 1, Loss: -29.338644\n",
            "Time step 1, Epoch 163, Batch 1, Loss: -29.338709\n",
            "Time step 1, Epoch 164, Batch 1, Loss: -29.338776\n",
            "Time step 1, Epoch 165, Batch 1, Loss: -29.338837\n",
            "Time step 1, Epoch 166, Batch 1, Loss: -29.338900\n",
            "Time step 1, Epoch 167, Batch 1, Loss: -29.338963\n",
            "Time step 1, Epoch 168, Batch 1, Loss: -29.339022\n",
            "Time step 1, Epoch 169, Batch 1, Loss: -29.339081\n",
            "Time step 1, Epoch 170, Batch 1, Loss: -29.339140\n",
            "Time step 1, Epoch 171, Batch 1, Loss: -29.339195\n",
            "Time step 1, Epoch 172, Batch 1, Loss: -29.339252\n",
            "Time step 1, Epoch 173, Batch 1, Loss: -29.339308\n",
            "Time step 1, Epoch 174, Batch 1, Loss: -29.339363\n",
            "Time step 1, Epoch 175, Batch 1, Loss: -29.339417\n",
            "Time step 1, Epoch 176, Batch 1, Loss: -29.339470\n",
            "Time step 1, Epoch 177, Batch 1, Loss: -29.339521\n",
            "Time step 1, Epoch 178, Batch 1, Loss: -29.339569\n",
            "Time step 1, Epoch 179, Batch 1, Loss: -29.339619\n",
            "Time step 1, Epoch 180, Batch 1, Loss: -29.339670\n",
            "Time step 1, Epoch 181, Batch 1, Loss: -29.339720\n",
            "Time step 1, Epoch 182, Batch 1, Loss: -29.339769\n",
            "Time step 1, Epoch 183, Batch 1, Loss: -29.339813\n",
            "Time step 1, Epoch 184, Batch 1, Loss: -29.339863\n",
            "Time step 1, Epoch 185, Batch 1, Loss: -29.339907\n",
            "Time step 1, Epoch 186, Batch 1, Loss: -29.339952\n",
            "Time step 1, Epoch 187, Batch 1, Loss: -29.339998\n",
            "Time step 1, Epoch 188, Batch 1, Loss: -29.340040\n",
            "Time step 1, Epoch 189, Batch 1, Loss: -29.340082\n",
            "Time step 1, Epoch 190, Batch 1, Loss: -29.340126\n",
            "Time step 1, Epoch 191, Batch 1, Loss: -29.340166\n",
            "Time step 1, Epoch 192, Batch 1, Loss: -29.340206\n",
            "Time step 1, Epoch 193, Batch 1, Loss: -29.340248\n",
            "Time step 1, Epoch 194, Batch 1, Loss: -29.340288\n",
            "Time step 1, Epoch 195, Batch 1, Loss: -29.340326\n",
            "Time step 1, Epoch 196, Batch 1, Loss: -29.340364\n",
            "Time step 1, Epoch 197, Batch 1, Loss: -29.340403\n",
            "Time step 1, Epoch 198, Batch 1, Loss: -29.340441\n",
            "Time step 1, Epoch 199, Batch 1, Loss: -29.340477\n",
            "Time step 1, Epoch 200, Batch 1, Loss: -29.340515\n",
            "Time step 1, Epoch 201, Batch 1, Loss: -29.340549\n",
            "Time step 1, Epoch 202, Batch 1, Loss: -29.340584\n",
            "Time step 1, Epoch 203, Batch 1, Loss: -29.340618\n",
            "Time step 1, Epoch 204, Batch 1, Loss: -29.340656\n",
            "Time step 1, Epoch 205, Batch 1, Loss: -29.340691\n",
            "Time step 1, Epoch 206, Batch 1, Loss: -29.340721\n",
            "Time step 1, Epoch 207, Batch 1, Loss: -29.340752\n",
            "Time step 1, Epoch 208, Batch 1, Loss: -29.340786\n",
            "Time step 1, Epoch 209, Batch 1, Loss: -29.340820\n",
            "Time step 1, Epoch 210, Batch 1, Loss: -29.340851\n",
            "Time step 1, Epoch 211, Batch 1, Loss: -29.340881\n",
            "Time step 1, Epoch 212, Batch 1, Loss: -29.340914\n",
            "Time step 1, Epoch 213, Batch 1, Loss: -29.340944\n",
            "Time step 1, Epoch 214, Batch 1, Loss: -29.340973\n",
            "Time step 1, Epoch 215, Batch 1, Loss: -29.341003\n",
            "Time step 1, Epoch 216, Batch 1, Loss: -29.341034\n",
            "Time step 1, Epoch 217, Batch 1, Loss: -29.341059\n",
            "Time step 1, Epoch 218, Batch 1, Loss: -29.341087\n",
            "Time step 1, Epoch 219, Batch 1, Loss: -29.341118\n",
            "Time step 1, Epoch 220, Batch 1, Loss: -29.341145\n",
            "Time step 1, Epoch 221, Batch 1, Loss: -29.341173\n",
            "Time step 1, Epoch 222, Batch 1, Loss: -29.341198\n",
            "Time step 1, Epoch 223, Batch 1, Loss: -29.341227\n",
            "Time step 1, Epoch 224, Batch 1, Loss: -29.341253\n",
            "Time step 1, Epoch 225, Batch 1, Loss: -29.341278\n",
            "Time step 1, Epoch 226, Batch 1, Loss: -29.341305\n",
            "Time step 1, Epoch 227, Batch 1, Loss: -29.341328\n",
            "Time step 1, Epoch 228, Batch 1, Loss: -29.341352\n",
            "Time step 1, Epoch 229, Batch 1, Loss: -29.341377\n",
            "Time step 1, Epoch 230, Batch 1, Loss: -29.341404\n",
            "Time step 1, Epoch 231, Batch 1, Loss: -29.341425\n",
            "Time step 1, Epoch 232, Batch 1, Loss: -29.341450\n",
            "Time step 1, Epoch 233, Batch 1, Loss: -29.341475\n",
            "Time step 1, Epoch 234, Batch 1, Loss: -29.341496\n",
            "Time step 1, Epoch 235, Batch 1, Loss: -29.341520\n",
            "Time step 1, Epoch 236, Batch 1, Loss: -29.341541\n",
            "Time step 1, Epoch 237, Batch 1, Loss: -29.341564\n",
            "Time step 1, Epoch 238, Batch 1, Loss: -29.341583\n",
            "Time step 1, Epoch 239, Batch 1, Loss: -29.341608\n",
            "Time step 1, Epoch 240, Batch 1, Loss: -29.341627\n",
            "Time step 1, Epoch 241, Batch 1, Loss: -29.341652\n",
            "Time step 1, Epoch 242, Batch 1, Loss: -29.341669\n",
            "Time step 1, Epoch 243, Batch 1, Loss: -29.341692\n",
            "Time step 1, Epoch 244, Batch 1, Loss: -29.341713\n",
            "Time step 1, Epoch 245, Batch 1, Loss: -29.341736\n",
            "Time step 1, Epoch 246, Batch 1, Loss: -29.341751\n",
            "Time step 1, Epoch 247, Batch 1, Loss: -29.341772\n",
            "Time step 1, Epoch 248, Batch 1, Loss: -29.341789\n",
            "Time step 1, Epoch 249, Batch 1, Loss: -29.341812\n",
            "Time step 1, Epoch 250, Batch 1, Loss: -29.341829\n",
            "Time step 1, Epoch 251, Batch 1, Loss: -29.341848\n",
            "Time step 1, Epoch 252, Batch 1, Loss: -29.341867\n",
            "Time step 1, Epoch 253, Batch 1, Loss: -29.341887\n",
            "Time step 1, Epoch 254, Batch 1, Loss: -29.341904\n",
            "Time step 1, Epoch 255, Batch 1, Loss: -29.341923\n",
            "Time step 1, Epoch 256, Batch 1, Loss: -29.341940\n",
            "Time step 1, Epoch 257, Batch 1, Loss: -29.341955\n",
            "Time step 1, Epoch 258, Batch 1, Loss: -29.341972\n",
            "Time step 1, Epoch 259, Batch 1, Loss: -29.341991\n",
            "Time step 1, Epoch 260, Batch 1, Loss: -29.342009\n",
            "Time step 1, Epoch 261, Batch 1, Loss: -29.342026\n",
            "Time step 1, Epoch 262, Batch 1, Loss: -29.342043\n",
            "Time step 1, Epoch 263, Batch 1, Loss: -29.342060\n",
            "Time step 1, Epoch 264, Batch 1, Loss: -29.342075\n",
            "Time step 1, Epoch 265, Batch 1, Loss: -29.342091\n",
            "Time step 1, Epoch 266, Batch 1, Loss: -29.342106\n",
            "Time step 1, Epoch 267, Batch 1, Loss: -29.342123\n",
            "Time step 1, Epoch 268, Batch 1, Loss: -29.342136\n",
            "Time step 1, Epoch 269, Batch 1, Loss: -29.342154\n",
            "Time step 1, Epoch 270, Batch 1, Loss: -29.342167\n",
            "Time step 1, Epoch 271, Batch 1, Loss: -29.342180\n",
            "Time step 1, Epoch 272, Batch 1, Loss: -29.342197\n",
            "Time step 1, Epoch 273, Batch 1, Loss: -29.342213\n",
            "Time step 1, Epoch 274, Batch 1, Loss: -29.342228\n",
            "Time step 1, Epoch 275, Batch 1, Loss: -29.342241\n",
            "Time step 1, Epoch 276, Batch 1, Loss: -29.342255\n",
            "Time step 1, Epoch 277, Batch 1, Loss: -29.342272\n",
            "Time step 1, Epoch 278, Batch 1, Loss: -29.342285\n",
            "Time step 1, Epoch 279, Batch 1, Loss: -29.342299\n",
            "Time step 1, Epoch 280, Batch 1, Loss: -29.342312\n",
            "Time step 1, Epoch 281, Batch 1, Loss: -29.342325\n",
            "Time step 1, Epoch 282, Batch 1, Loss: -29.342337\n",
            "Time step 1, Epoch 283, Batch 1, Loss: -29.342352\n",
            "Time step 1, Epoch 284, Batch 1, Loss: -29.342363\n",
            "Time step 1, Epoch 285, Batch 1, Loss: -29.342379\n",
            "Time step 1, Epoch 286, Batch 1, Loss: -29.342392\n",
            "Time step 1, Epoch 287, Batch 1, Loss: -29.342403\n",
            "Time step 1, Epoch 288, Batch 1, Loss: -29.342417\n",
            "Time step 1, Epoch 289, Batch 1, Loss: -29.342432\n",
            "Time step 1, Epoch 290, Batch 1, Loss: -29.342440\n",
            "Time step 1, Epoch 291, Batch 1, Loss: -29.342453\n",
            "Time step 1, Epoch 292, Batch 1, Loss: -29.342464\n",
            "Time step 1, Epoch 293, Batch 1, Loss: -29.342476\n",
            "Time step 1, Epoch 294, Batch 1, Loss: -29.342487\n",
            "Time step 1, Epoch 295, Batch 1, Loss: -29.342503\n",
            "Time step 1, Epoch 296, Batch 1, Loss: -29.342512\n",
            "Time step 1, Epoch 297, Batch 1, Loss: -29.342524\n",
            "Time step 1, Epoch 298, Batch 1, Loss: -29.342535\n",
            "Time step 1, Epoch 299, Batch 1, Loss: -29.342548\n",
            "Time step 1, Epoch 300, Batch 1, Loss: -29.342556\n",
            "Time step 1, Epoch 301, Batch 1, Loss: -29.342569\n",
            "Time step 1, Epoch 302, Batch 1, Loss: -29.342579\n",
            "Time step 1, Epoch 303, Batch 1, Loss: -29.342588\n",
            "Time step 1, Epoch 304, Batch 1, Loss: -29.342600\n",
            "Time step 1, Epoch 305, Batch 1, Loss: -29.342613\n",
            "Time step 1, Epoch 306, Batch 1, Loss: -29.342623\n",
            "Time step 1, Epoch 307, Batch 1, Loss: -29.342632\n",
            "Time step 1, Epoch 308, Batch 1, Loss: -29.342642\n",
            "Time step 1, Epoch 309, Batch 1, Loss: -29.342655\n",
            "Time step 1, Epoch 310, Batch 1, Loss: -29.342663\n",
            "Time step 1, Epoch 311, Batch 1, Loss: -29.342672\n",
            "Time step 1, Epoch 312, Batch 1, Loss: -29.342684\n",
            "Time step 1, Epoch 313, Batch 1, Loss: -29.342695\n",
            "Time step 1, Epoch 314, Batch 1, Loss: -29.342701\n",
            "Time step 1, Epoch 315, Batch 1, Loss: -29.342712\n",
            "Time step 1, Epoch 316, Batch 1, Loss: -29.342724\n",
            "Time step 1, Epoch 317, Batch 1, Loss: -29.342731\n",
            "Time step 1, Epoch 318, Batch 1, Loss: -29.342741\n",
            "Time step 1, Epoch 319, Batch 1, Loss: -29.342751\n",
            "Time step 1, Epoch 320, Batch 1, Loss: -29.342758\n",
            "Time step 1, Epoch 321, Batch 1, Loss: -29.342770\n",
            "Time step 1, Epoch 322, Batch 1, Loss: -29.342781\n",
            "Time step 1, Epoch 323, Batch 1, Loss: -29.342785\n",
            "Time step 1, Epoch 324, Batch 1, Loss: -29.342796\n",
            "Time step 1, Epoch 325, Batch 1, Loss: -29.342806\n",
            "Time step 1, Epoch 326, Batch 1, Loss: -29.342813\n",
            "Time step 1, Epoch 327, Batch 1, Loss: -29.342819\n",
            "Time step 1, Epoch 328, Batch 1, Loss: -29.342831\n",
            "Time step 1, Epoch 329, Batch 1, Loss: -29.342842\n",
            "Time step 1, Epoch 330, Batch 1, Loss: -29.342846\n",
            "Time step 1, Epoch 331, Batch 1, Loss: -29.342855\n",
            "Time step 1, Epoch 332, Batch 1, Loss: -29.342865\n",
            "Time step 1, Epoch 333, Batch 1, Loss: -29.342873\n",
            "Time step 1, Epoch 334, Batch 1, Loss: -29.342880\n",
            "Time step 1, Epoch 335, Batch 1, Loss: -29.342888\n",
            "Time step 1, Epoch 336, Batch 1, Loss: -29.342897\n",
            "Time step 1, Epoch 337, Batch 1, Loss: -29.342907\n",
            "Time step 1, Epoch 338, Batch 1, Loss: -29.342915\n",
            "Time step 1, Epoch 339, Batch 1, Loss: -29.342920\n",
            "Time step 1, Epoch 340, Batch 1, Loss: -29.342928\n",
            "Time step 1, Epoch 341, Batch 1, Loss: -29.342936\n",
            "Time step 1, Epoch 342, Batch 1, Loss: -29.342941\n",
            "Time step 1, Epoch 343, Batch 1, Loss: -29.342951\n",
            "Time step 1, Epoch 344, Batch 1, Loss: -29.342958\n",
            "Time step 1, Epoch 345, Batch 1, Loss: -29.342966\n",
            "Time step 1, Epoch 346, Batch 1, Loss: -29.342974\n",
            "Time step 1, Epoch 347, Batch 1, Loss: -29.342981\n",
            "Time step 1, Epoch 348, Batch 1, Loss: -29.342987\n",
            "Time step 1, Epoch 349, Batch 1, Loss: -29.342995\n",
            "Time step 1, Epoch 350, Batch 1, Loss: -29.343002\n",
            "Time step 1, Epoch 351, Batch 1, Loss: -29.343010\n",
            "Time step 1, Epoch 352, Batch 1, Loss: -29.343016\n",
            "Time step 1, Epoch 353, Batch 1, Loss: -29.343023\n",
            "Time step 1, Epoch 354, Batch 1, Loss: -29.343031\n",
            "Time step 1, Epoch 355, Batch 1, Loss: -29.343037\n",
            "Time step 1, Epoch 356, Batch 1, Loss: -29.343044\n",
            "Time step 1, Epoch 357, Batch 1, Loss: -29.343052\n",
            "Time step 1, Epoch 358, Batch 1, Loss: -29.343058\n",
            "Time step 1, Epoch 359, Batch 1, Loss: -29.343063\n",
            "Time step 1, Epoch 360, Batch 1, Loss: -29.343071\n",
            "Time step 1, Epoch 361, Batch 1, Loss: -29.343077\n",
            "Time step 1, Epoch 362, Batch 1, Loss: -29.343082\n",
            "Time step 1, Epoch 363, Batch 1, Loss: -29.343088\n",
            "Time step 1, Epoch 364, Batch 1, Loss: -29.343096\n",
            "Time step 1, Epoch 365, Batch 1, Loss: -29.343103\n",
            "Time step 1, Epoch 366, Batch 1, Loss: -29.343109\n",
            "Time step 1, Epoch 367, Batch 1, Loss: -29.343113\n",
            "Time step 1, Epoch 368, Batch 1, Loss: -29.343121\n",
            "Time step 1, Epoch 369, Batch 1, Loss: -29.343126\n",
            "Time step 1, Epoch 370, Batch 1, Loss: -29.343132\n",
            "Time step 1, Epoch 371, Batch 1, Loss: -29.343138\n",
            "Time step 1, Epoch 372, Batch 1, Loss: -29.343145\n",
            "Time step 1, Epoch 373, Batch 1, Loss: -29.343149\n",
            "Time step 1, Epoch 374, Batch 1, Loss: -29.343157\n",
            "Time step 1, Epoch 375, Batch 1, Loss: -29.343163\n",
            "Time step 1, Epoch 376, Batch 1, Loss: -29.343168\n",
            "Time step 1, Epoch 377, Batch 1, Loss: -29.343176\n",
            "Time step 1, Epoch 378, Batch 1, Loss: -29.343178\n",
            "Time step 1, Epoch 379, Batch 1, Loss: -29.343184\n",
            "Time step 1, Epoch 380, Batch 1, Loss: -29.343189\n",
            "Time step 1, Epoch 381, Batch 1, Loss: -29.343197\n",
            "Time step 1, Epoch 382, Batch 1, Loss: -29.343201\n",
            "Time step 1, Epoch 383, Batch 1, Loss: -29.343206\n",
            "Time step 1, Epoch 384, Batch 1, Loss: -29.343216\n",
            "Time step 1, Epoch 385, Batch 1, Loss: -29.343216\n",
            "Time step 1, Epoch 386, Batch 1, Loss: -29.343224\n",
            "Time step 1, Epoch 387, Batch 1, Loss: -29.343227\n",
            "Time step 1, Epoch 388, Batch 1, Loss: -29.343235\n",
            "Time step 1, Epoch 389, Batch 1, Loss: -29.343239\n",
            "Time step 1, Epoch 390, Batch 1, Loss: -29.343243\n",
            "Time step 1, Epoch 391, Batch 1, Loss: -29.343248\n",
            "Time step 1, Epoch 392, Batch 1, Loss: -29.343254\n",
            "Time step 1, Epoch 393, Batch 1, Loss: -29.343260\n",
            "Time step 1, Epoch 394, Batch 1, Loss: -29.343266\n",
            "Time step 1, Epoch 395, Batch 1, Loss: -29.343271\n",
            "Time step 1, Epoch 396, Batch 1, Loss: -29.343277\n",
            "Time step 1, Epoch 397, Batch 1, Loss: -29.343279\n",
            "Time step 1, Epoch 398, Batch 1, Loss: -29.343285\n",
            "Time step 1, Epoch 399, Batch 1, Loss: -29.343290\n",
            "Time step 1, Epoch 400, Batch 1, Loss: -29.343296\n",
            "Time step 1, Epoch 401, Batch 1, Loss: -29.343298\n",
            "Time step 1, Epoch 402, Batch 1, Loss: -29.343304\n",
            "Time step 1, Epoch 403, Batch 1, Loss: -29.343307\n",
            "Time step 1, Epoch 404, Batch 1, Loss: -29.343313\n",
            "Time step 1, Epoch 405, Batch 1, Loss: -29.343315\n",
            "Time step 1, Epoch 406, Batch 1, Loss: -29.343319\n",
            "Time step 1, Epoch 407, Batch 1, Loss: -29.343328\n",
            "Time step 1, Epoch 408, Batch 1, Loss: -29.343330\n",
            "Time step 1, Epoch 409, Batch 1, Loss: -29.343336\n",
            "Time step 1, Epoch 410, Batch 1, Loss: -29.343338\n",
            "Time step 1, Epoch 411, Batch 1, Loss: -29.343346\n",
            "Time step 1, Epoch 412, Batch 1, Loss: -29.343349\n",
            "Time step 1, Epoch 413, Batch 1, Loss: -29.343353\n",
            "Time step 1, Epoch 414, Batch 1, Loss: -29.343357\n",
            "Time step 1, Epoch 415, Batch 1, Loss: -29.343363\n",
            "Time step 1, Epoch 416, Batch 1, Loss: -29.343367\n",
            "Time step 1, Epoch 417, Batch 1, Loss: -29.343370\n",
            "Time step 1, Epoch 418, Batch 1, Loss: -29.343376\n",
            "Time step 1, Epoch 419, Batch 1, Loss: -29.343378\n",
            "Time step 1, Epoch 420, Batch 1, Loss: -29.343382\n",
            "Time step 1, Epoch 421, Batch 1, Loss: -29.343388\n",
            "Time step 1, Epoch 422, Batch 1, Loss: -29.343391\n",
            "Time step 1, Epoch 423, Batch 1, Loss: -29.343395\n",
            "Time step 1, Epoch 424, Batch 1, Loss: -29.343399\n",
            "Time step 1, Epoch 425, Batch 1, Loss: -29.343405\n",
            "Time step 1, Epoch 426, Batch 1, Loss: -29.343409\n",
            "Time step 1, Epoch 427, Batch 1, Loss: -29.343414\n",
            "Time step 1, Epoch 428, Batch 1, Loss: -29.343412\n",
            "Time step 1, Epoch 429, Batch 1, Loss: -29.343418\n",
            "Time step 1, Epoch 430, Batch 1, Loss: -29.343422\n",
            "Time step 1, Epoch 431, Batch 1, Loss: -29.343428\n",
            "Time step 1, Epoch 432, Batch 1, Loss: -29.343430\n",
            "Time step 1, Epoch 433, Batch 1, Loss: -29.343435\n",
            "Time step 1, Epoch 434, Batch 1, Loss: -29.343439\n",
            "Time step 1, Epoch 435, Batch 1, Loss: -29.343443\n",
            "Time step 1, Epoch 436, Batch 1, Loss: -29.343445\n",
            "Time step 1, Epoch 437, Batch 1, Loss: -29.343451\n",
            "Time step 1, Epoch 438, Batch 1, Loss: -29.343452\n",
            "Time step 1, Epoch 439, Batch 1, Loss: -29.343458\n",
            "Time step 1, Epoch 440, Batch 1, Loss: -29.343462\n",
            "Time step 1, Epoch 441, Batch 1, Loss: -29.343464\n",
            "Time step 1, Epoch 442, Batch 1, Loss: -29.343468\n",
            "Time step 1, Epoch 443, Batch 1, Loss: -29.343472\n",
            "Time step 1, Epoch 444, Batch 1, Loss: -29.343473\n",
            "Time step 1, Epoch 445, Batch 1, Loss: -29.343477\n",
            "Time step 1, Epoch 446, Batch 1, Loss: -29.343483\n",
            "Time step 1, Epoch 447, Batch 1, Loss: -29.343487\n",
            "Time step 1, Epoch 448, Batch 1, Loss: -29.343487\n",
            "Time step 1, Epoch 449, Batch 1, Loss: -29.343493\n",
            "Time step 1, Epoch 450, Batch 1, Loss: -29.343494\n",
            "Time step 1, Epoch 451, Batch 1, Loss: -29.343496\n",
            "Time step 1, Epoch 452, Batch 1, Loss: -29.343502\n",
            "Time step 1, Epoch 453, Batch 1, Loss: -29.343508\n",
            "Time step 1, Epoch 454, Batch 1, Loss: -29.343510\n",
            "Time step 1, Epoch 455, Batch 1, Loss: -29.343510\n",
            "Time step 1, Epoch 456, Batch 1, Loss: -29.343515\n",
            "Time step 1, Epoch 457, Batch 1, Loss: -29.343521\n",
            "Time step 1, Epoch 458, Batch 1, Loss: -29.343521\n",
            "Time step 1, Epoch 459, Batch 1, Loss: -29.343525\n",
            "Time step 1, Epoch 460, Batch 1, Loss: -29.343529\n",
            "Time step 1, Epoch 461, Batch 1, Loss: -29.343533\n",
            "Time step 1, Epoch 462, Batch 1, Loss: -29.343534\n",
            "Time step 1, Epoch 463, Batch 1, Loss: -29.343540\n",
            "Time step 1, Epoch 464, Batch 1, Loss: -29.343538\n",
            "Time step 1, Epoch 465, Batch 1, Loss: -29.343544\n",
            "Time step 1, Epoch 466, Batch 1, Loss: -29.343548\n",
            "Time step 1, Epoch 467, Batch 1, Loss: -29.343552\n",
            "Time step 1, Epoch 468, Batch 1, Loss: -29.343554\n",
            "Time step 1, Epoch 469, Batch 1, Loss: -29.343557\n",
            "Time step 1, Epoch 470, Batch 1, Loss: -29.343559\n",
            "Time step 1, Epoch 471, Batch 1, Loss: -29.343563\n",
            "Time step 1, Epoch 472, Batch 1, Loss: -29.343567\n",
            "Time step 1, Epoch 473, Batch 1, Loss: -29.343569\n",
            "Time step 1, Epoch 474, Batch 1, Loss: -29.343571\n",
            "Time step 1, Epoch 475, Batch 1, Loss: -29.343576\n",
            "Time step 1, Epoch 476, Batch 1, Loss: -29.343578\n",
            "Time step 1, Epoch 477, Batch 1, Loss: -29.343580\n",
            "Time step 1, Epoch 478, Batch 1, Loss: -29.343584\n",
            "Time step 1, Epoch 479, Batch 1, Loss: -29.343586\n",
            "Time step 1, Epoch 480, Batch 1, Loss: -29.343590\n",
            "Time step 1, Epoch 481, Batch 1, Loss: -29.343592\n",
            "Time step 1, Epoch 482, Batch 1, Loss: -29.343594\n",
            "Time step 1, Epoch 483, Batch 1, Loss: -29.343597\n",
            "Time step 1, Epoch 484, Batch 1, Loss: -29.343599\n",
            "Time step 1, Epoch 485, Batch 1, Loss: -29.343601\n",
            "Time step 1, Epoch 486, Batch 1, Loss: -29.343603\n",
            "Time step 1, Epoch 487, Batch 1, Loss: -29.343607\n",
            "Time step 1, Epoch 488, Batch 1, Loss: -29.343611\n",
            "Time step 1, Epoch 489, Batch 1, Loss: -29.343613\n",
            "Time step 1, Epoch 490, Batch 1, Loss: -29.343615\n",
            "Time step 1, Epoch 491, Batch 1, Loss: -29.343620\n",
            "Time step 1, Epoch 492, Batch 1, Loss: -29.343624\n",
            "Time step 1, Epoch 493, Batch 1, Loss: -29.343622\n",
            "Time step 1, Epoch 494, Batch 1, Loss: -29.343628\n",
            "Time step 1, Epoch 495, Batch 1, Loss: -29.343632\n",
            "Time step 1, Epoch 496, Batch 1, Loss: -29.343632\n",
            "Time step 1, Epoch 497, Batch 1, Loss: -29.343636\n",
            "Time step 1, Epoch 498, Batch 1, Loss: -29.343637\n",
            "Time step 1, Epoch 499, Batch 1, Loss: -29.343643\n",
            "Time step 1, Epoch 500, Batch 1, Loss: -29.343643\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construction des martingales candidates pour le calcul de la borne supérieure"
      ],
      "metadata": {
        "id": "o0EdtjqzprcM"
      },
      "id": "o0EdtjqzprcM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exchange option martingale (discounted)"
      ],
      "metadata": {
        "id": "rABSCN1lJs_Z"
      },
      "id": "rABSCN1lJs_Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construction de la martingale candidate $M^{(1)}$ : option d'échange\n",
        "\n",
        "Cette fonction construit une martingale basée sur la valeur actualisée d'une option européenne d’échange entre les deux actifs les plus performants à chaque date $t$. Elle repose sur la formule de Margrabe, activée uniquement lorsque $\\max_i S_i(t) > K$. La martingale résultante est utilisée comme composante dans l’estimateur dual de Rogers.\n",
        "\n",
        "$$\n",
        "\\text{Ex}(t) = S^{(1)}_t e^{-\\delta (T - t)} \\Phi(d_1) - S^{(2)}_t e^{-\\delta (T - t)} \\Phi(d_2),\n",
        "\\quad\n",
        "d_1 = \\frac{\\log\\left(S^{(1)}_t / S^{(2)}_t\\right) + \\frac{1}{2} \\sigma^2 (T - t)}{\\sigma \\sqrt{2(T - t)}}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "DQkWMcV3iIoH"
      },
      "id": "DQkWMcV3iIoH"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_exchange_option_martingale(S_paths, K, r, delta, sigma, T):\n",
        "    \"\"\"\n",
        "    Construit correctement la martingale de l'option d'échange :\n",
        "    - Prix européen actualisé à chaque date,\n",
        "    - Incréments cumulés uniquement lorsque S_max > K à CHAQUE date,\n",
        "    - Pas de désactivation définitive.\n",
        "    \"\"\"\n",
        "\n",
        "    N_plus_1, M, d = S_paths.shape\n",
        "    times = torch.linspace(0, T, N_plus_1, device=S_paths.device)\n",
        "    discount_factors = torch.exp(-r * (times))\n",
        "\n",
        "    normal = dist.Normal(0., 1.)\n",
        "\n",
        "    # --- Construction du processus brut M_t ---\n",
        "    M_brut = torch.zeros(N_plus_1, M, device=S_paths.device)\n",
        "\n",
        "    for t in range(N_plus_1):\n",
        "        S = S_paths[t]\n",
        "        sorted_S, _ = torch.sort(S, dim=-1, descending=True)\n",
        "        S_max = sorted_S[:, 0]\n",
        "        S_second = sorted_S[:, 1]\n",
        "\n",
        "        tau = T - times[t]\n",
        "\n",
        "        if tau > 0:\n",
        "            vol_sqrt_tau = sigma * math.sqrt(2) * torch.sqrt(torch.tensor(tau, device=S.device))\n",
        "            d1 = (torch.log(S_max / S_second) + sigma**2 * tau) / vol_sqrt_tau\n",
        "            d2 = d1 - vol_sqrt_tau\n",
        "\n",
        "            european_exchange = torch.exp(-delta * tau) * (S_max * normal.cdf(d1) - S_second * normal.cdf(d2))\n",
        "        else:\n",
        "            european_exchange = torch.clamp(S_max - S_second, min=0.0)\n",
        "\n",
        "        M_brut[t] = discount_factors[t] * european_exchange\n",
        "\n",
        "    # --- Calcul des incréments bruts ---\n",
        "    increments = M_brut[1:] - M_brut[:-1]  # (N, M)\n",
        "\n",
        "    # --- Correction dynamique sur les incréments ---\n",
        "    corrected_increments = torch.zeros_like(increments)\n",
        "\n",
        "    for t in range(N_plus_1 - 1):\n",
        "        S = S_paths[t]\n",
        "        sorted_S, _ = torch.sort(S, dim=-1, descending=True)\n",
        "        S_max = sorted_S[:, 0]\n",
        "\n",
        "        # Test dynamique : si S_max > K à l'instant t\n",
        "        active_now = (S_max >= K).float()\n",
        "\n",
        "        corrected_increments[t] = increments[t] * active_now\n",
        "\n",
        "    # --- Reconstruction finale de la martingale ---\n",
        "    martingale = torch.zeros(N_plus_1, M, device=S_paths.device)\n",
        "    for t in range(1, N_plus_1):\n",
        "        martingale[t] = martingale[t-1] + corrected_increments[t-1]\n",
        "\n",
        "    return martingale\n"
      ],
      "metadata": {
        "id": "4CHL7yXiJwSV"
      },
      "id": "4CHL7yXiJwSV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leading asset european call (discounted)"
      ],
      "metadata": {
        "id": "yWKEfKX8J2_o"
      },
      "id": "yWKEfKX8J2_o"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construction de la martingale $M^{(2)}$ : call européen sur l’actif leader\n",
        "\n",
        "Cette fonction construit une martingale à partir du prix actualisé d’un call européen sur l’actif ayant le plus haut prix à chaque date $t$. La formule utilisée est celle de Black–Scholes avec dividende, et les incréments sont conservés uniquement lorsque $S_{\\text{lead}} > K$ :\n"
      ],
      "metadata": {
        "id": "Y2MjDo6IiXMt"
      },
      "id": "Y2MjDo6IiXMt"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_leading_asset_call_martingale(S_paths, K, r, delta, sigma, T):\n",
        "    \"\"\"\n",
        "    Construit correctement la martingale du call européen sur l'actif leader :\n",
        "    - Calcul du prix européen actualisé à chaque date,\n",
        "    - Incréments cumulés uniquement lorsque S_lead > K à CHAQUE date,\n",
        "    - Pas d'activation définitive.\n",
        "    \"\"\"\n",
        "\n",
        "    N_plus_1, M, d = S_paths.shape\n",
        "    times = torch.linspace(0, T, N_plus_1, device=S_paths.device)\n",
        "    discount_factors = torch.exp(-r * (times))\n",
        "\n",
        "    normal = dist.Normal(0., 1.)\n",
        "\n",
        "    # --- Construction du processus brut M_t ---\n",
        "    M_brut = torch.zeros(N_plus_1, M, device=S_paths.device)\n",
        "\n",
        "    for t in range(N_plus_1):\n",
        "        S = S_paths[t]\n",
        "        # leader_idx = torch.argmax(S, dim=-1)\n",
        "        # S_lead = S.gather(1, leader_idx.unsqueeze(-1)).squeeze(-1)\n",
        "        sorted_S, _ = torch.sort(S, dim=-1, descending=True)\n",
        "        S_lead = sorted_S[:, 0]\n",
        "\n",
        "        tau = T - times[t]\n",
        "\n",
        "        if tau > 0:\n",
        "            vol_sqrt_tau = sigma * torch.sqrt(torch.tensor(tau, device=S.device))\n",
        "            d1 = (torch.log(S_lead / K) + (r - delta + 0.5 * sigma**2) * tau) / vol_sqrt_tau\n",
        "            d2 = d1 - vol_sqrt_tau\n",
        "\n",
        "            european_call = S_lead * torch.exp(-delta * tau) * normal.cdf(d1) \\\n",
        "                            - K * torch.exp(-r * tau) * normal.cdf(d2)\n",
        "        else:\n",
        "            european_call = torch.clamp(S_lead - K, min=0.0)\n",
        "\n",
        "        M_brut[t] = discount_factors[t] * european_call\n",
        "\n",
        "    # --- Calcul des incréments bruts ---\n",
        "    increments = M_brut[1:] - M_brut[:-1]  # (N, M)\n",
        "\n",
        "    # --- Correction dynamique sur les incréments ---\n",
        "    corrected_increments = torch.zeros_like(increments)\n",
        "\n",
        "    for t in range(N_plus_1 - 1):\n",
        "        S = S_paths[t]\n",
        "        # leader_idx = torch.argmax(S, dim=-1)\n",
        "        # S_lead = S.gather(1, leader_idx.unsqueeze(-1)).squeeze(-1)\n",
        "        sorted_S, _ = torch.sort(S, dim=-1, descending=True)\n",
        "        S_lead = sorted_S[:, 0]\n",
        "\n",
        "        # Test dynamique : si S_lead > K à l'instant t\n",
        "        active_now = (S_lead > K).float()\n",
        "\n",
        "        corrected_increments[t] = increments[t] * active_now\n",
        "\n",
        "    # --- Reconstruction finale de la martingale ---\n",
        "    martingale = torch.zeros(N_plus_1, M, device=S_paths.device)\n",
        "    for t in range(1, N_plus_1):\n",
        "        martingale[t] = martingale[t-1] + corrected_increments[t-1]\n",
        "\n",
        "    return martingale\n"
      ],
      "metadata": {
        "id": "4mRFuYWFKXqy"
      },
      "id": "4mRFuYWFKXqy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimisation des coefficients $\\lambda$\n",
        "\n",
        "Cette fonction ajuste les coefficients de combinaison linéaire des deux martingales candidates $(M^{(1)}, M^{(2)})$ afin de minimiser l'espérance de l'expression duale de la borne supérieure :\n",
        "$$\n",
        "\\hat{U}(\\lambda) = \\mathbb{E}\\left[ \\max_{0 \\leq n \\leq N} \\left( g(n, X_n) - M_n^\\lambda \\right) \\right],\n",
        "$$\n",
        "où $M^\\lambda = \\lambda_1 M^{(1)} + \\lambda_2 M^{(2)}$ .\n",
        "\n",
        "L’optimisation est réalisée par descente de gradient avec l’algorithme L-BFGS.\n"
      ],
      "metadata": {
        "id": "7QxLtGYIicV_"
      },
      "id": "7QxLtGYIicV_"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def optimize_lambda(martingale1, martingale2, payoffs, n_iter=500):\n",
        "    \"\"\"\n",
        "    Optimise deux coefficients lambda1 = alpha et lambda2 = 1 - alpha,\n",
        "    avec alpha restreint dans [-0.5, 0] via clamp après optimisation.\n",
        "\n",
        "    Inputs :\n",
        "    - martingale1, martingale2 : tensors (N+1, M)\n",
        "    - payoffs : tensor (N+1, M), non actualisé\n",
        "    - r : float, taux d’intérêt sans risque\n",
        "    - T : float, maturité\n",
        "    - n_iter : int, nombre d’itérations pour L-BFGS\n",
        "\n",
        "    Output :\n",
        "    - lambdas_star : tensor(2,) — coefficients optimisés\n",
        "    \"\"\"\n",
        "    device = payoffs.device\n",
        "    N_plus_1 = payoffs.size(0)\n",
        "\n",
        "    # Discounting\n",
        "    times = torch.linspace(0, T, N_plus_1, device=device)\n",
        "    discount_factors = torch.exp(-r * (T - times)).unsqueeze(1)  # (N+1, 1)\n",
        "    discounted_payoffs = discount_factors * payoffs  # (N+1, M)\n",
        "\n",
        "    # Martingales combinées\n",
        "    martingales_tensor = torch.stack([martingale1, martingale2], dim=-1)  # (N+1, M, 2)\n",
        "\n",
        "    # Paramètre libre alpha ∈ ℝ\n",
        "    alpha = torch.full((1,), 0.5, requires_grad=True, device=device)\n",
        "\n",
        "    optimizer = torch.optim.LBFGS([alpha], max_iter=n_iter)\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        lambdas = torch.cat([alpha, 1 - alpha], dim=0)  # (2,)\n",
        "        combined = (martingales_tensor @ lambdas).squeeze(-1)  # (N+1, M)\n",
        "        gap = discounted_payoffs - combined\n",
        "        sup_gap, _ = torch.max(gap, dim=0)\n",
        "        loss = sup_gap.mean()\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "\n",
        "    alpha_star = alpha.detach().clamp(-1.0, 0.0)\n",
        "    lambdas_star = torch.cat([alpha_star, 1 - alpha_star], dim=0)\n",
        "\n",
        "    return lambdas_star\n"
      ],
      "metadata": {
        "id": "z78jzfseVelA"
      },
      "id": "z78jzfseVelA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upper bound"
      ],
      "metadata": {
        "id": "XsUsxnx5LTcR"
      },
      "id": "XsUsxnx5LTcR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimation de la borne supérieure $\\hat{U}$\n",
        "\n",
        "Cette fonction calcule la borne supérieure $\\hat{U}$ par la méthode duale de Rogers. Elle utilise une combinaison linéaire de deux martingales candidates $M^{(1)}$ et $M^{(2)}$ :\n",
        "$$\n",
        "M^\\lambda = \\lambda_1 M^{(1)} + \\lambda_2 M^{(2)},\n",
        "$$\n",
        " L’estimateur est obtenu par :\n",
        "$$\n",
        "\\hat{U} = \\mathbb{E}\\left[\\max_{0 \\leq n \\leq N} \\left( g(n, S_n) - M_n^\\lambda \\right)\\right].\n",
        "$$\n",
        "La variance empirique est également renvoyée.\n"
      ],
      "metadata": {
        "id": "jFfeCJ6ckTsf"
      },
      "id": "jFfeCJ6ckTsf"
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_upper_bound(S_paths, K, r, delta, sigma, T, lambdas_star):\n",
        "    \"\"\"\n",
        "    Estime la borne supérieure de l'option bermudéenne max-call et sa variance empirique.\n",
        "\n",
        "    Inputs :\n",
        "    - S_paths : tensor (N+1, M, d) chemins simulés\n",
        "    - K, r, delta, sigma, T : paramètres financiers\n",
        "    - lambdas_star : tensor (2,), coefficients constants optimisés\n",
        "\n",
        "    Outputs :\n",
        "    - upper_bound : float, estimateur de la borne supérieure\n",
        "    - variance : float, variance empirique non biaisée\n",
        "    \"\"\"\n",
        "\n",
        "    martingale_leading_call = build_leading_asset_call_martingale(S_paths, K, r, delta, sigma, T)\n",
        "    martingale_exchange_option = build_exchange_option_martingale(S_paths, K, r, delta, sigma, T)\n",
        "    payoffs = compute_all_payoffs(S_paths, K)\n",
        "\n",
        "    combined_martingale = (\n",
        "        lambdas_star[0] * martingale_leading_call +\n",
        "        lambdas_star[1] * martingale_exchange_option\n",
        "    )\n",
        "\n",
        "    gap = payoffs - combined_martingale\n",
        "    sup_gap, _ = torch.max(gap, dim=0)\n",
        "\n",
        "    upper_bound = sup_gap.mean().item()\n",
        "    variance = sup_gap.var(unbiased=True).item()\n",
        "\n",
        "    return upper_bound, variance\n"
      ],
      "metadata": {
        "id": "vPrRLNfhLT2g"
      },
      "id": "vPrRLNfhLT2g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Optimisation des coefficients λ sur des trajectoires indépendantes ---\n",
        "\n",
        "N_train=40\n",
        "M_train=1000\n",
        "N_test = N\n",
        "M_test = 8000\n",
        "\n",
        "# 1. Simulation pour optimiser les lambdas (trajectoires fines, N=40)\n",
        "S_paths_dual = simulate_black_scholes_paths(\n",
        "    x0=x0, r=r, sigma=sigma, delta=delta, T=T,\n",
        "    N=N_train, M=M_train, d=n_assets, device=device\n",
        ")\n",
        "\n",
        "# 2. Construction des martingales candidates\n",
        "payoffs_dual = compute_all_payoffs(S_paths_dual, K)\n",
        "martingale_leading = build_leading_asset_call_martingale(S_paths_dual, K, r, delta, sigma, T)\n",
        "martingale_exchange = build_exchange_option_martingale(S_paths_dual, K, r, delta, sigma, T)\n",
        "\n",
        "# 3. Optimisation des coefficients lambdas\n",
        "lambdas_star = optimize_lambda(\n",
        "    martingale1=martingale_leading,\n",
        "    martingale2=martingale_exchange,\n",
        "    payoffs=payoffs_dual,\n",
        "    n_iter=200\n",
        ")\n",
        "\n",
        "# 4. Simulation pour évaluation finale de la borne supérieure (trajectoires grossières, N=9)\n",
        "S_paths_test = simulate_black_scholes_paths(\n",
        "    x0=x0, r=r, sigma=sigma, delta=delta, T=T,\n",
        "    N=N_test, M=M_test, d=n_assets, device=device\n",
        ")\n",
        "\n",
        "print(f\"Lambdas optimisés : {lambdas_star.cpu().numpy()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEIYO3AELvUr",
        "outputId": "eeafa337-4064-43f7-bfab-388ac6254121",
        "collapsed": true
      },
      "id": "gEIYO3AELvUr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lambdas optimisés : [0. 1.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-698f6ce944d9>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  vol_sqrt_tau = sigma * torch.sqrt(torch.tensor(tau, device=S.device))\n",
            "<ipython-input-51-e8feb786f09d>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  vol_sqrt_tau = sigma * math.sqrt(2) * torch.sqrt(torch.tensor(tau, device=S.device))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intervalle de confiance"
      ],
      "metadata": {
        "id": "3S2ZYDdCGon8"
      },
      "id": "3S2ZYDdCGon8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construction d’un intervalle de confiance\n",
        "\n",
        "Cette fonction construit un intervalle de confiance asymétrique autour de la valeur réelle de l’option, basé sur les estimateurs $\\hat{L}$ et $\\hat{U}$. On utilise :\n",
        "$$\n",
        "\\left[ \\hat{L} - z_{\\alpha/2} \\frac{\\hat{\\sigma}_L}{\\sqrt{K_L}},\\ \\hat{U} + z_{\\alpha/2} \\frac{\\hat{\\sigma}_U}{\\sqrt{K_U}} \\right],\n",
        "$$\n",
        "où $z_{\\alpha/2}$ est le quantile de la loi normale standard pour un niveau de confiance $1 - \\alpha$.\n"
      ],
      "metadata": {
        "id": "O_p9PW7QkgnF"
      },
      "id": "O_p9PW7QkgnF"
    },
    {
      "cell_type": "code",
      "source": [
        "def confidence_interval(L, U, var_L, var_U, K_L, K_U, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Calcule l'intervalle de confiance asymétrique :\n",
        "    [ L - z_alpha/2 * sigma_L/sqrt(K_L), U + z_alpha/2 * sigma_U/sqrt(K_U) ]\n",
        "\n",
        "    Inputs :\n",
        "    - L, U : bornes inférieure et supérieure estimées\n",
        "    - var_L, var_U : variances empiriques de L et U\n",
        "    - K_L, K_U : nombre de trajectoires utilisées pour L et U\n",
        "    - alpha : niveau de risque (e.g. 0.05 pour 95%)\n",
        "\n",
        "    Returns :\n",
        "    - lower_CI, upper_CI : bornes de l'intervalle de confiance\n",
        "    - point_estimate : moyenne (L + U)/2\n",
        "    \"\"\"\n",
        "    z = stats.norm.ppf(1 - alpha / 2)\n",
        "\n",
        "    sigma_L = (var_L ** 0.5) / (K_L ** 0.5)\n",
        "    sigma_U = (var_U ** 0.5) / (K_U ** 0.5)\n",
        "\n",
        "    lower_CI = L - z * sigma_L\n",
        "    upper_CI = U + z * sigma_U\n",
        "\n",
        "    point_estimate = 0.5 * (L + U)\n",
        "    interval = [lower_CI, upper_CI]\n",
        "\n",
        "    return interval, point_estimate"
      ],
      "metadata": {
        "id": "8NnLlPaqGuq2"
      },
      "id": "8NnLlPaqGuq2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simuler des  trajectoires pour l'évaluation de la borne inférieure\n",
        "S_paths_lower = simulate_black_scholes_paths(x0, r, sigma, dividend, T, N=N, M=4096000, d=d, device=device)\n",
        "\n",
        "# Estimation de la borne inférieure\n",
        "lower_bound, var_L = estimate_lower_bound(models, S_paths_lower, K) # Now using the defined S_paths_lower\n",
        "K_L = S_paths_lower.size(1)\n",
        "\n",
        "# Simuler des trajectoires pour l'évaluation de la borne supérieure\n",
        "S_paths_upper = simulate_black_scholes_paths(x0, r, sigma, dividend, T, N=N, M=4096000, d=d, device=device)\n",
        "\n",
        "# Estimation de la borne supérieure\n",
        "upper_bound, var_U = estimate_upper_bound(S_paths_upper, K, r, delta, sigma, T, lambdas_star)\n",
        "K_U = S_paths_upper.size(1)\n",
        "\n",
        "# Calcul de l'intervalle de confiance à 95%\n",
        "alpha = 0.05\n",
        "interval, point_estimate = confidence_interval(\n",
        "    lower_bound, upper_bound,\n",
        "    var_L, var_U,\n",
        "    K_L, K_U,\n",
        "    alpha\n",
        ")\n",
        "\n",
        "# Affichage\n",
        "print(f\"\\nRésultats de l'estimation :\")\n",
        "print(f\"Borne inférieure : {lower_bound:.4f}\")\n",
        "print(f\"Variance borne inf : {var_L:.4f}\")\n",
        "print(f\"borne supérieure : {upper_bound:.4f}\")\n",
        "print(f\"Variance borne sup : {var_U:.4f}\")\n",
        "print(f\"Estimation ponctuelle : {point_estimate:.4f}\")\n",
        "print(f\"Intervalle de confiance à 95% : [{interval[0]:.4f}, {interval[1]:.4f}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr8oVXUWIvqK",
        "outputId": "5ac0a7cf-910f-4867-9862-915ea12f9653"
      },
      "id": "lr8oVXUWIvqK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-698f6ce944d9>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  vol_sqrt_tau = sigma * torch.sqrt(torch.tensor(tau, device=S.device))\n",
            "<ipython-input-51-e8feb786f09d>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  vol_sqrt_tau = sigma * math.sqrt(2) * torch.sqrt(torch.tensor(tau, device=S.device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Résultats de l'estimation :\n",
            "Borne inférieure : 26.5655\n",
            "Variance borne inf : 625.2654\n",
            "borne supérieure : 27.8417\n",
            "Variance borne sup : 237.2293\n",
            "Estimation ponctuelle : 27.2036\n",
            "Intervalle de confiance à 95% : [26.5412, 27.8566]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Réduction de variance par variable de contrôle (borne supérieure)\n",
        "\n",
        "Cette fonction applique la méthode de la variable de contrôle à l’estimateur de borne supérieure $\\hat{U}$. On utilise comme variable auxiliaire le payoff européen au temps $T$ :\n",
        "$$\n",
        "Y = \\left( \\max_{1 \\le i \\le d} S_i(T) - K \\right)^+.\n",
        "$$\n",
        "\n",
        "L’estimateur corrigé est défini par :\n",
        "$$\n",
        "\\hat{U}_{\\text{corr}} = \\hat{U} - \\beta (Y - \\mathbb{E}[Y]),\n",
        "$$\n",
        "où le coefficient $\\beta$ est donné par :\n",
        "$$\n",
        "\\beta = \\frac{\\operatorname{Cov}(\\hat{U}, Y)}{\\operatorname{Var}(Y)}.\n",
        "$$\n"
      ],
      "metadata": {
        "id": "M2GtciLeoJy9"
      },
      "id": "M2GtciLeoJy9"
    },
    {
      "cell_type": "code",
      "source": [
        "def control_variate_upper_bound(S_paths, K, r, delta, sigma, T, lambdas_star):\n",
        "    \"\"\"\n",
        "    Applique la méthode de variable de contrôle à la borne supérieure (dualité de Rogers).\n",
        "\n",
        "    Inputs :\n",
        "    - S_paths : trajectoires simulées (N+1, M, d)\n",
        "    - K : strike\n",
        "    - r : taux sans risque\n",
        "    - delta, sigma : pour cohérence avec martingales\n",
        "    - T : maturité\n",
        "    - lambdas_star : coefficients optimisés (2,)\n",
        "\n",
        "    Returns :\n",
        "    - upper_corr : estimateur corrigé\n",
        "    - var_corr : variance corrigée\n",
        "    - upper_raw : estimateur brut\n",
        "    - var_raw : variance brute\n",
        "    \"\"\"\n",
        "\n",
        "    M = S_paths.shape[1]\n",
        "    device = S_paths.device\n",
        "\n",
        "    # Martingales\n",
        "    mart1 = build_leading_asset_call_martingale(S_paths, K, r, delta, sigma, T)\n",
        "    mart2 = build_exchange_option_martingale(S_paths, K, r, delta, sigma, T)\n",
        "\n",
        "    # Combinaison optimisée\n",
        "    combined = lambdas_star[0] * mart1 + lambdas_star[1] * mart2  # (N+1, M)\n",
        "\n",
        "    # Payoffs Bermudéens\n",
        "    payoffs = compute_all_payoffs(S_paths, K)\n",
        "\n",
        "    # Supremum sur chaque trajectoire\n",
        "    gap = payoffs - combined\n",
        "    sup_gap, _ = torch.max(gap, dim=0)  # (M,)\n",
        "\n",
        "    # Estimateur brut\n",
        "    upper_raw = sup_gap.mean().item()\n",
        "    var_raw = sup_gap.var(unbiased=True).item()\n",
        "\n",
        "    # Variable de contrôle = payoff européen à T\n",
        "    S_T = S_paths[-1]  # (M, d)\n",
        "    euro = torch.clamp(S_T.max(dim=1).values - K, min=0.0)\n",
        "    euro_centered = euro - euro.mean()\n",
        "\n",
        "    # Coefficient beta optimal\n",
        "    cov = torch.mean((sup_gap - upper_raw) * euro_centered)\n",
        "    beta = cov / euro_centered.var(unbiased=True)\n",
        "\n",
        "    # Estimateur corrigé\n",
        "    corrected = sup_gap - beta * euro_centered\n",
        "    upper_corr = corrected.mean().item()\n",
        "    var_corr = corrected.var(unbiased=True).item()\n",
        "\n",
        "    return upper_corr, var_corr, upper_raw, var_raw\n"
      ],
      "metadata": {
        "id": "0oBFuX9aU0V_"
      },
      "id": "0oBFuX9aU0V_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Simulation des trajectoires standards pour test avec variable de contrôle\n",
        "S_paths_cv_sup = simulate_black_scholes_paths(\n",
        "    x0=100, r=r, sigma=sigma, delta=delta, T=T,\n",
        "    N=N, M=8000, d=n_assets, device=device\n",
        ")\n",
        "\n",
        "# 2. Estimation de la borne supérieure avec variable de contrôle\n",
        "upper_bound_cv, var_U_cv, upper_raw, var_U_raw = control_variate_upper_bound(\n",
        "    S_paths=S_paths_cv_sup, K=K, r=r, delta=delta, sigma=sigma, T=T, lambdas_star=lambdas_star\n",
        ")\n",
        "\n",
        "# 3. Affichage et comparaison\n",
        "print(f\"Borne supérieure avec variable de contrôle : {upper_bound_cv:.4f}\")\n",
        "print(f\"Variance (control variate)                : {var_U_cv:.6f}\")\n",
        "print(f\"Borne supérieure brute                     : {upper_raw:.4f}\")\n",
        "print(f\"Variance brute                             : {var_U_raw:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2kY22hzWeWW",
        "outputId": "1660308f-e6ef-4722-e659-db3ee818b1a1"
      },
      "id": "b2kY22hzWeWW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Borne supérieure avec variable de contrôle : 27.7343\n",
            "Variance (control variate)                : 125.152184\n",
            "Borne supérieure brute                     : 27.7343\n",
            "Variance brute                             : 228.659485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-698f6ce944d9>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  vol_sqrt_tau = sigma * torch.sqrt(torch.tensor(tau, device=S.device))\n",
            "<ipython-input-51-e8feb786f09d>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  vol_sqrt_tau = sigma * math.sqrt(2) * torch.sqrt(torch.tensor(tau, device=S.device))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}