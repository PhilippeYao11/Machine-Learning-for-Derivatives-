{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00377947-156b-4216-ba3d-695ece4ff049",
   "metadata": {},
   "source": [
    "#  Deep Learning for the Valuation of Bermudan Options: Primal-Dual Bounds Estimation\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "#### Authors:  \r\n",
    "**Georges ROLLAND / Philippe YAO**\r\n",
    "\r\n",
    "#### Professors:  \r\n",
    "**Gilles Pag√®s / Vincent Lemaire**\r\n",
    "\r\n",
    "---\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b532a5f3-e901-4c2c-b99e-2a1a36f0211f",
   "metadata": {},
   "source": [
    "#  Introduction\n",
    "\n",
    "## Bermudan Options\n",
    "\n",
    "A Bermudan option is a financial derivative that allows its holder to exercise their right at specific discrete dates before the final maturity.  \n",
    "It lies between:\n",
    "- European options (exercise only at maturity),\n",
    "- and American options (exercise at any time before maturity).\n",
    "\n",
    "---\n",
    "\n",
    "## Project Objective\n",
    "\n",
    "The goal is to **compute a reliable estimate of the price** of a Bermudan put option by combining two key approaches from the following papers:\n",
    "\n",
    "- **Deep Optimal Stopping (Becker, Cheridito, Jentzen, 2019)**:  \n",
    "  Using **neural networks** to approximate the optimal stopping strategy, determine a lower bound, and construct a candidate martingale for the upper bound.\n",
    "\n",
    "- **Dual Approach of Rogers (2002)**:  \n",
    "  Building an **upper bound** on the price by optimizing over candidate martingales.\n",
    "\n",
    "---\n",
    "\n",
    "## General Approach\n",
    "\n",
    "- Simulation of trajectories for the underlying asset.\n",
    "- Learning the optimal stopping rules using deep learning techniques.\n",
    "- Estimation of the **lower bound** via Monte Carlo simulation.\n",
    "- Estimation of the **upper bound** by combining different martingales (especially the discounted European put price).\n",
    "- Application of **Richardson extrapolation** to enhance numerical accuracy.\n",
    "- **Construction of a confidence interval** for the estimated price.\n",
    "- Robustness Tests and variance reduction\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bab2a1-3de6-4ef9-90ef-77ba7c90b5d4",
   "metadata": {},
   "source": [
    "### Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "117b6715-0b8f-4a7a-84d0-6d52f5f79cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a3f3a-52b1-4f36-b474-56827e58ea41",
   "metadata": {},
   "source": [
    "### Param√®tres initiaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8deb19fe-6cf4-4899-bd75-7196e13eda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.06    # risk-free interest rate\n",
    "sigma = 0.4 # volatility\n",
    "K = 100     # strike price\n",
    "T = 0.5     # maturity (in years)\n",
    "N = 50      # number of discrete time steps\n",
    "d = 1       # dimension of the process\n",
    "h = T / N   # time step size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45867af1-30b4-4c7e-8fba-17082afe1db5",
   "metadata": {},
   "source": [
    "### 1. Simulation des trajectoires et calcul du payoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbfa59b-2da6-4770-8d62-9c499214c07b",
   "metadata": {},
   "source": [
    "#####  Simulation under the Black-Scholes Model\r\n",
    "\r\n",
    "The underlying asset follows the dynamics:\r\n",
    "\r\n",
    "$$\r\n",
    "dS_t = r S_t \\, dt + \\sigma S_t \\, dW_t\r\n",
    "$$\r\n",
    "\r\n",
    "By discretization, we simulate:\r\n",
    "\r\n",
    "$$\r\n",
    "S_{n+1} = S_n \\exp\\left( \\left(r - \\frac{1}{2} \\sigma^2\\right) \\Delta t + \\sigma \\sqrt{\\Delta t} Z_n \\right)\r\n",
    "$$\r\n",
    "\r\n",
    "where $Z_n \\sim \\mathcal{N}(0,1)$ are independent standard normal random variables\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "##### üí∞ Payoff of the European Put Option\r\n",
    "\r\n",
    "The standard payoff used is:\r\n",
    "\r\n",
    "$$\r\n",
    "g(S) = \\max(K - S, 0)\r\n",
    "$$\r\n",
    "\r\n",
    "---\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "546befdc-8dee-4d8b-ba76-455c4fad3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate paths under the Black-Scholes model\n",
    "def simulate_paths(S0, M, N, r, sigma, T, d=1, seed=42):\n",
    "    \"\"\"\n",
    "    Simulates M paths of a lognormal Brownian motion over N+1 time steps.\n",
    "    \n",
    "    Args:\n",
    "        S0: initial asset price\n",
    "        M: number of simulated paths\n",
    "        N: number of time steps\n",
    "        r: risk-free rate\n",
    "        sigma: volatility\n",
    "        T: maturity (in years)\n",
    "        d: dimension of the process (default=1)\n",
    "        seed: random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        time_grid: np.ndarray of time points\n",
    "        X: np.ndarray of simulated paths (shape (N+1, M, d))\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    dt = T / N\n",
    "    time_grid = np.linspace(0, T, N+1)\n",
    "    \n",
    "    # Initialization\n",
    "    X = np.zeros((N+1, M, d))\n",
    "    X[0] = S0\n",
    "\n",
    "    # Path simulation\n",
    "    for n in range(N):\n",
    "        dW = np.random.normal(0, np.sqrt(dt), size=(M, d))\n",
    "        X[n+1] = X[n] * np.exp((r - 0.5 * sigma**2) * dt + sigma * dW)\n",
    "\n",
    "    return time_grid, X\n",
    "\n",
    "\n",
    "# 2. Payoff function for the European Put Option\n",
    "def payoff(S, K):\n",
    "    \"\"\"\n",
    "    Standard payoff function for a European Put option.\n",
    "    \n",
    "    Args:\n",
    "        S: underlying asset prices\n",
    "        K: strike price\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray of payoffs\n",
    "    \"\"\"\n",
    "    return np.maximum(K - S, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a58c8-adb3-4e83-806e-672036749afb",
   "metadata": {},
   "source": [
    "## Optimal Stopping Problem\n",
    "\n",
    "We consider a discrete-time optimal stopping problem where the goal is to maximize the expected reward:\n",
    "\n",
    "$$\n",
    "\\sup_{\\tau \\in \\mathcal{T}} \\mathbb{E}[g(\\tau, X_\\tau)],\n",
    "$$\n",
    "\n",
    "where $ X = (X_n)_{n=0}^N $ is a Markov process taking values in $ \\mathbb{R}^d $, $ g $ is a measurable reward function, and $ \\mathcal{T} $ is the set of all stopping times.\n",
    "\n",
    "In our context, $ X $ follows a Black-Scholes type dynamic, and $ g(\\tau, X_\\tau) $ represents the discounted payoff of a Bermudan option.\n",
    "\n",
    "To solve this problem:\n",
    "\n",
    "- We approximate the optimal stopping decisions using **neural networks** to learn the continuation functions.\n",
    "- We use two types of estimators:\n",
    "  - A **classical estimator** (lower bound) based on the learned stopping policy.\n",
    "  - A **dual estimator** (upper bound) inspired by the method of Rogers (2002).\n",
    "\n",
    "The objective is to obtain an accurate estimate of the option price along with a rigorous confidence interval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2db13-d7f6-4af9-b689-79f4882f1fdb",
   "metadata": {},
   "source": [
    "### 2. Neural Network for Approximating Stopping Decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8331d-234a-481e-a171-d45fff0bfc8b",
   "metadata": {},
   "source": [
    "To approximate the optimal stopping rule, we construct a family of functions $f_n^\\theta : \\mathbb{R}^d \\to \\{0, 1\\}$ using feedforward neural networks.  \r\n",
    "Each $f_n^\\theta$ determines at each time step $n$ whether it is optimal to stop or to continue.\r\n",
    "\r\n",
    "The neural network approximation $F^\\theta$ has the structure:\r\n",
    "\r\n",
    "$$\r\n",
    "F^\\theta = \\psi \\circ a_I^\\theta \\circ \\varphi_{q_{I-1}} \\circ a_{I-1}^\\theta \\circ \\cdots \\circ \\varphi_{q_1} \\circ a_1^\\theta\r\n",
    "$$\r\n",
    "\r\n",
    "where:\r\n",
    "- $\\psi$ is the standard logistic (Sigmoid) activation function,\r\n",
    "- $a_i^\\theta$ are affine transformations (linear layers),\r\n",
    "- $\\varphi_q$ denotes the componentwise ReLU activation functions.\r\n",
    "\r\n",
    "The network is trained via **stochastic gradient ascent** on Monte Carlo simulations, using standard techniques such as Xavier initialization and batch normalization.\r\n",
    "\r\n",
    "At each step, we recursively approximate the optimal stopping rule, starting from the final time $N$ and moving backward to the initial time.\r\n",
    "e initial time.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "18393d9d-2b8a-43d1-904a-6e0915309649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[10, 10, 10]):\n",
    "        \"\"\"\n",
    "        Neural network architecture to approximate stopping decisions.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: dimension of X_t (d)\n",
    "            hidden_layers: list specifying the number of neurons in each hidden layer\n",
    "        \"\"\"\n",
    "        super(StoppingNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_layers\n",
    "        \n",
    "        for i in range(len(hidden_layers)):\n",
    "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            layers.append(nn.ReLU())  # ReLU activation\n",
    "\n",
    "        layers.append(nn.Linear(hidden_layers[-1], 1))\n",
    "        layers.append(nn.Sigmoid())  # Final Sigmoid activation\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)  # Output (batch_size,)\n",
    "\n",
    "def cost_function_f_theta(model, Xn, gXn, g_tau_n1_Xtau_n1):\n",
    "    \"\"\"\n",
    "    Compute the cost function to maximize (expectation form).\n",
    "    \"\"\"\n",
    "    f_theta_Xn = model(Xn)\n",
    "    loss = torch.mean(gXn * f_theta_Xn + g_tau_n1_Xtau_n1 * (1 - f_theta_Xn))\n",
    "    return loss\n",
    "\n",
    "def train_stopping_network(model, Xn, gXn, g_tau_n1_Xtau_n1, n_epochs=100, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train a single stopping network f_theta^n.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = -cost_function_f_theta(model, Xn, gXn, g_tau_n1_Xtau_n1)  # maximize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_all_stopping_networks(X_paths, g_func, hidden_layers=[10, 10, 10], n_epochs=100, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train all stopping networks by backward induction, with normalization.\n",
    "\n",
    "    Returns:\n",
    "        stopping_networks: list of trained networks\n",
    "        mean: mean used for normalization\n",
    "        std: std used for normalization\n",
    "    \"\"\"\n",
    "    N_plus_1, n_paths, d = X_paths.shape\n",
    "    N = N_plus_1 - 1\n",
    "\n",
    "    # Compute normalization parameters\n",
    "    mean = np.mean(X_paths, axis=(0,1))  # shape (d,)\n",
    "    std = np.std(X_paths, axis=(0,1)) + 1e-8  # to avoid division by zero\n",
    "\n",
    "    # Convert to torch\n",
    "    X_paths_torch = torch.tensor(X_paths, dtype=torch.float32)\n",
    "    mean_torch = torch.tensor(mean, dtype=torch.float32)\n",
    "    std_torch = torch.tensor(std, dtype=torch.float32)\n",
    "\n",
    "    # Precompute immediate payoffs\n",
    "    gX = torch.zeros((N_plus_1, n_paths))\n",
    "    for n in range(N_plus_1):\n",
    "        gX[n] = g_func(n, X_paths_torch[n])\n",
    "\n",
    "    stopping_networks = [None for _ in range(N)]\n",
    "\n",
    "    g_tau_n1_Xtau_n1 = gX[N].clone()  # final payoff\n",
    "\n",
    "    for n in reversed(range(N)):\n",
    "        print(f\"Training stopping network for time step n = {n}\")\n",
    "\n",
    "        Xn = (X_paths_torch[n] - mean_torch) / std_torch  # normalization\n",
    "        gXn = gX[n]\n",
    "\n",
    "        model = StoppingNetwork(input_dim=d, hidden_layers=hidden_layers)\n",
    "        trained_model = train_stopping_network(model, Xn, gXn, g_tau_n1_Xtau_n1, n_epochs=n_epochs, lr=lr)\n",
    "        stopping_networks[n] = trained_model\n",
    "\n",
    "        with torch.no_grad():\n",
    "            stop_probs = trained_model(Xn)\n",
    "            g_tau_n1_Xtau_n1 = gXn * stop_probs + g_tau_n1_Xtau_n1 * (1 - stop_probs)\n",
    "\n",
    "    return stopping_networks, mean, std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9028be6-7f49-44ea-a0ca-9641c9a42fe0",
   "metadata": {},
   "source": [
    "### 3.Lower bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded0e2f-e57d-43f5-95f9-de115f9c24f1",
   "metadata": {},
   "source": [
    "In order to estimate the true price $V_0$ of the Bermudan option, we compute:\r\n",
    "\r\n",
    "- A **lower bound** $L$ using the learned stopping decisions $f^\\theta_n$,\r\n",
    "- An **upper bound** $U$ based on the dual approach optimizing over martingalesr Bound\r\n",
    "\r\n",
    "Once the networks $f_n^\\theta$ are trained, the associated stopping time $\\tau^\\Theta$ provides a **lower bound** for the price:\r\n",
    "\r\n",
    "$$\r\n",
    "L = \\mathbb{E} \\left[ g(\\tau^\\Theta, X_{\\tau^\\Theta}) \\right].\r\n",
    "$$\r\n",
    "\r\n",
    "To compute it:\r\n",
    "- Simulate $K_L$ independent test paths $(y_n^k)_{n=0}^N$,\r\n",
    "- Apply the learned stopping rule to each path,\r\n",
    "- Use the Monte Carlo estimator:\r\n",
    "\r\n",
    "$$\r\n",
    "\\hat{L} = \\frac{1}{K_L} \\sum_{k=1}^{K_L} g(l^k, y_{l^k}^k).\r\n",
    "$$\r\n",
    "\r\n",
    "where $l^k$ denotes the stopping time applied to path $k$.\r\n",
    "\r\n",
    "By the law of large numbers, $\\hat{L}$ converges to $L$ when $K_L \\to \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "13186a11-e50e-4e05-9d35-506e44653435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lower_bound(stopping_networks, X_paths_test, payoff_func, mean, std):\n",
    "    \"\"\"\n",
    "    Computes the lower bound using the learned stopping rules.\n",
    "    \"\"\"\n",
    "    N_plus_1, n_paths, d = X_paths_test.shape\n",
    "    N = N_plus_1 - 1\n",
    "\n",
    "    X_paths_test_torch = torch.tensor(X_paths_test, dtype=torch.float32)\n",
    "    X_paths_normalized = (X_paths_test_torch - mean) / std\n",
    "\n",
    "    gX = np.zeros((N_plus_1, n_paths))\n",
    "    for n in range(N_plus_1):\n",
    "        S_n = X_paths_test[n, :, 0] * std.item() + mean.item()  # ‚ö° D√©normaliser\n",
    "        gX[n] = payoff_func(n, torch.tensor(S_n).unsqueeze(1))  # Keep shape (n_paths, 1)\n",
    "\n",
    "    rewards = np.zeros(n_paths)\n",
    "    stopping_times = np.zeros(n_paths, dtype=int)\n",
    "\n",
    "    for j in range(n_paths):\n",
    "        for n in range(N):\n",
    "            prob_stop = stopping_networks[n](X_paths_normalized[n, j].unsqueeze(0)).detach().numpy()\n",
    "            if prob_stop >= 0.5:\n",
    "                rewards[j] = gX[n, j]\n",
    "                stopping_times[j] = n\n",
    "                break\n",
    "        else:\n",
    "            rewards[j] = gX[N, j]\n",
    "            stopping_times[j] = N\n",
    "\n",
    "    lower_bound = np.mean(rewards)\n",
    "    return lower_bound, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7864dd8b-89f3-474b-9809-38324ac42f06",
   "metadata": {},
   "source": [
    "### Optimization of the Vector $ \\lambda^* $ in the Rogers Method (Upper Bound)\r\n",
    "\r\n",
    "The goal is to estimate the value of an American option by constructing an **upper bound** on its price.  \r\n",
    "The upper bound relies on the following dual representation:\r\n",
    "\r\n",
    "$$\r\n",
    "Y_0^* = \\inf_{M \\in H_0^1} \\mathbb{E}\\left[ \\sup_{0 \\leq t \\leq T} (Z_t - M_t) \\right],\r\n",
    "$$\r\n",
    "\r\n",
    "where:\r\n",
    "- $ H_0^1 $ is the set of martingales $ M $ such that $ M_0 = 0 $ and $ \\sup_{0 \\leq t \\leq T} |M_t| \\in L^1 $,\r\n",
    "- $ Z_t $ is the discounted payoff process (for example, $ Z_t = e^{-rt}(K - S_t)^+ $).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Strategy:\r\n",
    "\r\n",
    "- Build a family of candidate martingales $ (M^1, M^2, \\dots, M^d) $,\r\n",
    "- Consider linear combinations $ M_t^\\lambda = \\sum_{k=1}^d \\lambda_k M_t^k $,\r\n",
    "- Solve the following optimization problem:\r\n",
    "\r\n",
    "$$\r\n",
    "\\lambda^* = \\arg\\min_{\\lambda \\in \\mathbb{R}^d} \\mathbb{E}\\left[\\sup_{0 \\leq t \\leq T} \\left(Z_t - \\sum_{k=1}^d \\lambda_k M_t^k\\right) \\right],\r\n",
    "$$\r\n",
    "\r\n",
    "where $ \\lambda \\cdot M_t = \\sum_{k=1}^d \\lambda_k M_t^k $.\r\n",
    "\r\n",
    "This optimization is a **convex minimization** problem over $ \\lambda $.\r\n",
    "roblem over \\( \\lambda \\).\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9edf1-959e-4289-b2e2-b5ea8d4fa1c8",
   "metadata": {},
   "source": [
    "### 4.First Martingale Construction\n",
    "\n",
    "We construct a martingale based on the trained stopping networks.\n",
    "\n",
    "At each time step $n$, we define noisy increments:\n",
    "\n",
    "$$\n",
    "\\Delta M_n^{\\Theta, k} = f_n^\\theta(z_n^k) g(n, z_n^k) + (1 - f_n^\\theta(z_n^k)) C_n^k - C_{n-1}^k,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $f_n^\\theta(z_n^k)$ is the stopping probability predicted by the neural network at time $n$,\n",
    "- $g(n, z_n^k)$ is the immediate reward,\n",
    "- $C_n^k$ is an estimated continuation value based on $J$ additional Monte Carlo simulations starting from $z_n^k$.\n",
    "\n",
    "The cumulative martingale along the $k$-th path is then given by:\n",
    "\n",
    "$$\n",
    "M_n^k = \\sum_{m=1}^n \\Delta M_m^{\\Theta, k},\n",
    "$$\n",
    "\n",
    "with the convention $M_0^k = 0$.\n",
    "\n",
    "This martingale $M_n^k$ approximates the Snell envelope of the process and can be used to derive a **dual upper bound**:\n",
    "\n",
    "$$\n",
    "\\hat{U} = \\frac{1}{K_U} \\sum_{k=1}^{K_U} \\max_{0 \\leq n \\leq N} \\left( g(n, z_n^k) - M_n^k \\right).\n",
    "$$\n",
    "\n",
    "This martingale captures both immediate rewards and continuation values, refined by the trained networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "2064820b-917e-4267-9900-7d3b76880453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_martingale_deep(X_paths_test, stopping_networks, g_func, r, T):\n",
    "    \"\"\"\n",
    "    Construit la martingale bas√©e sur Deep Optimal Stopping (martingale classique).\n",
    "\n",
    "    Args:\n",
    "        X_paths_test: np.ndarray (N+1, n_paths, d)\n",
    "        stopping_networks: list of trained networks\n",
    "        g_func: function (n, Xn) -> rewards\n",
    "        r: taux sans risque\n",
    "        T: maturit√©\n",
    "\n",
    "    Returns:\n",
    "        martingale: np.ndarray (N+1, n_paths)\n",
    "    \"\"\"\n",
    "    N_plus_1, n_paths, d = X_paths_test.shape\n",
    "    N = N_plus_1 - 1\n",
    "    h = T / N\n",
    "\n",
    "    X_paths_test_torch = torch.tensor(X_paths_test, dtype=torch.float32)\n",
    "\n",
    "    martingale = np.zeros((N+1, n_paths))\n",
    "\n",
    "    # First compute the value at time 0 for normalization\n",
    "    discount_factor_0 = np.exp(-r * 0 * h)\n",
    "    value0 = g_func(0, X_paths_test_torch[0]).numpy() * discount_factor_0\n",
    "\n",
    "    for n in range(N+1):\n",
    "        discount_factor = np.exp(-r * n * h)\n",
    "        if n == N:\n",
    "            values = g_func(N, X_paths_test_torch[N]).numpy() * discount_factor\n",
    "        else:\n",
    "            stop_probs = stopping_networks[n](X_paths_test_torch[n]).detach().numpy()\n",
    "            immediate_rewards = g_func(n, X_paths_test_torch[n]).numpy() * discount_factor\n",
    "            continuation_values = np.zeros(n_paths)\n",
    "\n",
    "            for k in range(n_paths):\n",
    "                continuation_values[k] = immediate_rewards[k] * stop_probs[k]\n",
    "        \n",
    "            values = continuation_values\n",
    "\n",
    "        # Centering immediately\n",
    "        martingale[n] = values - value0\n",
    "\n",
    "    return martingale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6418ca-8c9a-4250-8b02-460c1af810d7",
   "metadata": {},
   "source": [
    "#### 5. Second martingale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "47f99255-15f4-4b60-a9d3-d176beaca9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_martingale_rogers(X_paths_test, r, T, N, K, sigma):\n",
    "    \"\"\"\n",
    "    Constructs the candidate martingale based on the discounted European put price.\n",
    "    \n",
    "    Returns:\n",
    "        martingale: np.ndarray of shape (N+1, n_paths), the centered martingale satisfying M_0 = 0\n",
    "    \"\"\"\n",
    "    times = np.linspace(0, T, N+1)\n",
    "    n_paths = X_paths_test.shape[1]\n",
    "\n",
    "    martingale = np.zeros((N+1, n_paths))\n",
    "\n",
    "    # First compute M0 separately\n",
    "    S0 = X_paths_test[0, :, 0]\n",
    "    price0 = np.exp(-r * (T - times[0])) * european_put_price(times[0], S0, K, T, r, sigma)\n",
    "\n",
    "    for n in range(N+1):\n",
    "        S_n = X_paths_test[n, :, 0]  # underlying asset prices at time step n\n",
    "        price = np.exp(-r * (T - times[n])) * european_put_price(times[n], S_n, K, T, r, sigma)\n",
    "        martingale[n] = price - price0  # Direct centering at each step\n",
    "\n",
    "    return martingale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc3679a-8dea-47e8-ba1b-6aa82af1cf4e",
   "metadata": {},
   "source": [
    "#### Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f8d8ac12-5379-41c7-a675-b9ef63001d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def optimize_lambda(payoffs, martingales):\n",
    "    \"\"\"\n",
    "    Robustly optimizes the linear combination of candidate martingales.\n",
    "\n",
    "    Args:\n",
    "        payoffs: np.ndarray (N+1, n_paths) - actualized payoff paths\n",
    "        martingales: np.ndarray (d, N+1, n_paths) - d candidate martingales\n",
    "\n",
    "    Returns:\n",
    "        lambda_opt: optimal vector of lambdas\n",
    "    \"\"\"\n",
    "    N_plus_1, n_paths = payoffs.shape\n",
    "    d = martingales.shape[0]\n",
    "\n",
    "    def cost(lambda_vect):\n",
    "        combined = np.tensordot(lambda_vect, martingales, axes=1)  # shape (N+1, n_paths)\n",
    "        eta = np.max(payoffs - combined, axis=0)\n",
    "        return np.mean(eta)\n",
    "\n",
    "    # Initialization: small random noise around zero\n",
    "    np.random.seed(0)\n",
    "    initial_lambda = 0.01 * np.random.randn(d)\n",
    "\n",
    "    # Robust optimization settings\n",
    "    result = minimize(\n",
    "        cost,\n",
    "        initial_lambda,\n",
    "        method='BFGS',\n",
    "        options={\n",
    "            'gtol': 0.1,       # relaxed tolerance on gradient (was 1e-6)\n",
    "            'maxiter': 500,    # allow more iterations\n",
    "            'disp': False,      # silent mode\n",
    "            'eps': 1e-5         # numerical step for finite differences\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if not result.success:\n",
    "        print(f\"‚ö†Ô∏è Optimization failed: {result.message}\")\n",
    "        print(\"‚öôÔ∏è Using the initial guess as fallback solution.\")\n",
    "        return initial_lambda\n",
    "\n",
    "    return result.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de45482-0f23-450b-ab85-a708b7449179",
   "metadata": {},
   "source": [
    "#### Upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a36d3c5a-4724-4439-b299-edda08e24c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_upper_bound(X_paths_test, stopping_networks, r, T, N, K, sigma, mean, std):\n",
    "    \"\"\"\n",
    "    Computes the upper bound with martingale construction and optimization.\n",
    "    \"\"\"\n",
    "    h = T / N\n",
    "    times = np.linspace(0, T, N+1)\n",
    "    n_paths = X_paths_test.shape[1]\n",
    "\n",
    "    X_paths_test_torch = torch.tensor(X_paths_test, dtype=torch.float32)\n",
    "    \n",
    "    # Normalize for the networks\n",
    "    X_paths_normalized = (X_paths_test_torch - mean) / std\n",
    "\n",
    "    # Deep martingale\n",
    "    martingale_deep = construct_martingale_deep(X_paths_normalized.numpy(), stopping_networks, payoff_func, r, T)\n",
    "\n",
    "    # Rogers martingale\n",
    "    martingale_rogers = construct_martingale_rogers(X_paths_test, r, T, N, K, sigma)\n",
    "\n",
    "    # Discounted Payoff (‚ö° D√©normalisation pour payoff)\n",
    "    S_test = X_paths_test[:, :, 0] * std.item() + mean.item()\n",
    "    discount_factors = np.exp(-r * times[:, None])\n",
    "    payoffs = np.maximum(K - S_test, 0) * discount_factors  # shape (N+1, n_paths)\n",
    "\n",
    "    # Optimize lambda\n",
    "    martingales = np.stack([martingale_deep, martingale_rogers], axis=0)  # (2, N+1, n_paths)\n",
    "    lambda_star = optimize_lambda(payoffs, martingales)\n",
    "\n",
    "    # Combine martingales\n",
    "    combined_martingale = np.tensordot(lambda_star, martingales, axes=1)  # (N+1, n_paths)\n",
    "\n",
    "    # Eta values\n",
    "    eta = np.max(payoffs - combined_martingale, axis=0)\n",
    "\n",
    "    upper_bound = np.mean(eta)\n",
    "    std_dev = np.std(eta)\n",
    "    \n",
    "    return upper_bound, std_dev, eta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb0b4b-6c1a-4ad9-8d06-3b6aa5eec110",
   "metadata": {},
   "source": [
    "#### Confident interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6d9395e3-b79a-45a5-9477-fe7f0f99c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Richardson extrapolation\n",
    "def richardson_extrapolation(V_coarse, V_fine):\n",
    "    \"\"\"\n",
    "    Richardson extrapolation for improved upper bound estimation.\n",
    "    \"\"\"\n",
    "    return 2 * V_fine - V_coarse\n",
    "\n",
    "# Payoff wrapper for training\n",
    "payoff_func = lambda n, X: payoff(X[:, 0], K)\n",
    "\n",
    "# Function to compute point estimate and global confidence interval\n",
    "def compute_interval_and_estimate(lower_bound, upper_bound,\n",
    "                                  rewards_lower, values_upper,\n",
    "                                  KL, KU):\n",
    "    \"\"\"\n",
    "    Compute point estimate and 95% global confidence interval based on empirical variances.\n",
    "\n",
    "    Args:\n",
    "        lower_bound: float, estimated lower bound (mean)\n",
    "        upper_bound: float, estimated upper bound (mean)\n",
    "        rewards_lower: np.ndarray, realizations of g(l^k, y^k_{l^k})\n",
    "        values_upper: np.ndarray, realizations of max_n (g(n, z_n^k) - M_n^k)\n",
    "        KL: int, number of samples for lower bound\n",
    "        KU: int, number of samples for upper bound\n",
    "\n",
    "    Returns:\n",
    "        dict containing the lower bound, upper bound, point estimate, confidence interval, and gap.\n",
    "    \"\"\"\n",
    "    z_score = 1.96  # 95% confidence level (normal quantile)\n",
    "\n",
    "    # Empirical standard deviations following the article\n",
    "    lower_variance = np.sum((rewards_lower - lower_bound)**2) / (KL - 1)\n",
    "    upper_variance = np.sum((values_upper - upper_bound)**2) / (KU - 1)\n",
    "\n",
    "    # Final standard errors (for the CLT scaling)\n",
    "    lower_se = np.sqrt(lower_variance / KL)\n",
    "    upper_se = np.sqrt(upper_variance / KU)\n",
    "\n",
    "    # Point estimate\n",
    "    point_estimate = (lower_bound + upper_bound) / 2\n",
    "\n",
    "    # Global confidence interval\n",
    "    ci_lower = lower_bound - z_score * lower_se\n",
    "    ci_upper = upper_bound + z_score * upper_se\n",
    "\n",
    "    return {\n",
    "        \"Lower Bound\": lower_bound,\n",
    "        \"Upper Bound\": upper_bound,\n",
    "        \"Lower Std Error\": lower_se,\n",
    "        \"Upper Std Error\": upper_se,\n",
    "        \"Point Estimate\": point_estimate,\n",
    "        \"Confidence Interval 95%\": (ci_lower, ci_upper),\n",
    "        \"Gap\": upper_bound - lower_bound\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d816d-6a60-4e91-bf05-01ab36784f41",
   "metadata": {},
   "source": [
    "##### First numerical results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7cb46-4a42-4f9a-bf2f-a3753e3c1004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing S0 = 80\n",
      "Training stopping network for time step n = 99\n",
      "Training stopping network for time step n = 98\n",
      "Training stopping network for time step n = 97\n",
      "Training stopping network for time step n = 96\n",
      "Training stopping network for time step n = 95\n",
      "Training stopping network for time step n = 94\n",
      "Training stopping network for time step n = 93\n",
      "Training stopping network for time step n = 92\n",
      "Training stopping network for time step n = 91\n",
      "Training stopping network for time step n = 90\n",
      "Training stopping network for time step n = 89\n",
      "Training stopping network for time step n = 88\n",
      "Training stopping network for time step n = 87\n",
      "Training stopping network for time step n = 86\n",
      "Training stopping network for time step n = 85\n",
      "Training stopping network for time step n = 84\n",
      "Training stopping network for time step n = 83\n",
      "Training stopping network for time step n = 82\n",
      "Training stopping network for time step n = 81\n",
      "Training stopping network for time step n = 80\n",
      "Training stopping network for time step n = 79\n",
      "Training stopping network for time step n = 78\n",
      "Training stopping network for time step n = 77\n",
      "Training stopping network for time step n = 76\n",
      "Training stopping network for time step n = 75\n",
      "Training stopping network for time step n = 74\n",
      "Training stopping network for time step n = 73\n",
      "Training stopping network for time step n = 72\n",
      "Training stopping network for time step n = 71\n",
      "Training stopping network for time step n = 70\n",
      "Training stopping network for time step n = 69\n",
      "Training stopping network for time step n = 68\n",
      "Training stopping network for time step n = 67\n",
      "Training stopping network for time step n = 66\n",
      "Training stopping network for time step n = 65\n",
      "Training stopping network for time step n = 64\n",
      "Training stopping network for time step n = 63\n",
      "Training stopping network for time step n = 62\n",
      "Training stopping network for time step n = 61\n",
      "Training stopping network for time step n = 60\n",
      "Training stopping network for time step n = 59\n"
     ]
    }
   ],
   "source": [
    "# True American option prices (from Rogers, 2002)\n",
    "true_american_prices = {\n",
    "    80: 21.6059,\n",
    "    85: 18.0374,\n",
    "    90: 14.9187,\n",
    "    95: 12.2314,\n",
    "    100: 9.9458,\n",
    "    105: 8.0281,\n",
    "    110: 6.4352,\n",
    "    115: 5.1265,\n",
    "    120: 4.0611\n",
    "}\n",
    "\n",
    "# Global parameters\n",
    "M_train = 300      # number of training paths\n",
    "M_test_L = 5000    # number of testing paths for lower bound\n",
    "M_test_U = 5000    # number of testing paths for upper bound\n",
    "N = 50             # number of time steps\n",
    "T = 0.5            # maturity\n",
    "r = 0.06           # risk-free interest rate\n",
    "sigma = 0.4        # volatility\n",
    "K = 100            # strike price\n",
    "d = 1              # dimension of the process\n",
    "\n",
    "# List of initial spot prices\n",
    "S0_list = np.arange(80, 125, 5)\n",
    "results = []\n",
    "\n",
    "# Main loop over initial prices\n",
    "for S0 in S0_list:\n",
    "    print(f\"Processing S0 = {S0}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fine simulation (2N time steps)\n",
    "    times_train_fine, X_train_fine = simulate_paths(S0, M_train, 2*N, r, sigma, T)\n",
    "    stopping_networks_fine = train_all_stopping_networks(X_train_fine, payoff_func, n_epochs=100, lr=0.001)\n",
    "\n",
    "    times_test_fine, X_test_fine = simulate_paths(S0, M_test_U, 2*N, r, sigma, T)\n",
    "    V_fine, mad_fine, std_fine, _ = compute_upper_bound(X_test_fine, stopping_networks_fine, r, T, 2*N, K, sigma)\n",
    "\n",
    "    # Coarse reduction (every two time steps)\n",
    "    X_test_coarse = X_test_fine[::2]\n",
    "    V_coarse, mad_coarse, std_coarse, _ = compute_upper_bound(X_test_coarse, stopping_networks_fine, r, T, N, K, sigma)\n",
    "\n",
    "    # Richardson extrapolation\n",
    "    V_richardson = richardson_extrapolation(V_coarse, V_fine)\n",
    "\n",
    "    # Lower bound estimation\n",
    "    times_train, X_train = simulate_paths(S0, M_train, N, r, sigma, T)\n",
    "    stopping_networks = train_all_stopping_networks(X_train, payoff_func, n_epochs=100, lr=0.001)\n",
    "\n",
    "    times_test, X_test = simulate_paths(S0, M_test_L, N, r, sigma, T)\n",
    "    lower_bound, rewards_lower = compute_lower_bound(stopping_networks, X_test, payoff_func)\n",
    "    lower_std = np.std(rewards_lower)\n",
    "\n",
    "    # European option price (Black-Scholes closed formula)\n",
    "    european_price = european_put_price(0, S0, K, T, r, sigma)\n",
    "\n",
    "    # Confidence interval and point estimate computation\n",
    "    stats = compute_interval_and_estimate(lower_bound, lower_std, V_richardson, std_fine, K_L=M_test_L, K_U=M_test_U)\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        \"S0\": S0,\n",
    "        \"American Put Price\": true_american_prices[S0],\n",
    "        \"European Put Price\": european_price,\n",
    "        \"Lower Bound\": stats[\"Lower Bound\"],\n",
    "        \"Upper Bound\": stats[\"Upper Bound\"],\n",
    "        \"Point Estimate\": stats[\"Point Estimate\"],\n",
    "        \"Confidence Interval 95%\": stats[\"Confidence Interval 95%\"],\n",
    "        \"Gap\": stats[\"Gap\"],\n",
    "        \"Standard Error (Lower Bound)\": lower_std / np.sqrt(M_test_L),\n",
    "        \"Standard Error (Upper Bound)\": std_fine / np.sqrt(M_test_U),\n",
    "        \"Execution Time (s)\": execution_time\n",
    "    }\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "# Final conversion into a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results_display = df_results.round(4)\n",
    "print(df_results_display)\n",
    "\n",
    "# (Optional) Export results to CSV\n",
    "# df_results.to_csv(\"deep_optimal_stopping_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
